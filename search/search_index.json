{"config":{"indexing":"full","lang":["pt"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tutoriais do NPAD \u00b6 Aqui os usu\u00e1rios do N\u00facleo de Processamento de Alto Desempenho (NPAD) encontram tutoriais para compreender o uso e funcionamento do supercomputador da UFRN. Encontrou algum problemas nos tutoriais por favor crie uma issue em github.com/NPAD-UFRN/Tutorials/issues","title":"In\u00edcio"},{"location":"#tutoriais-do-npad","text":"Aqui os usu\u00e1rios do N\u00facleo de Processamento de Alto Desempenho (NPAD) encontram tutoriais para compreender o uso e funcionamento do supercomputador da UFRN. Encontrou algum problemas nos tutoriais por favor crie uma issue em github.com/NPAD-UFRN/Tutorials/issues","title":"Tutoriais do NPAD"},{"location":"advanced/jupyter_tutorial/","text":"Usando o Jupyter no supercomputador \u00b6 O Jupyter \u00e9 uma aplica\u00e7\u00e3o web de c\u00f3digo-fonte aberto que permite voc\u00ea criar e compartilhar documentos (chamados de notebooks) que cont\u00e9m c\u00f3digo, equa\u00e7\u00f5es, visualiza\u00e7\u00f5es e texto. Para usar o jupyter no supercomputador, deve-se criar um script semelhante ao mostrado abaixo: #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=10 #Quantidade de n\u00facleos #SBATCH --time=1-00:00 #Tempo m\u00e1ximo do job no formato DIAS-HORAS:MINUTOS #SBATCH --hint=compute_bound # Execute conda activate forge # antes de submeter este script de job ## Parametros iniciais XDG_RUNTIME_DIR = \"\" ipnport = $( shuf -i8000-9999 -n1 ) ipnip = $( hostname -i ) ## Imprime na sa\u00edda slurm-{jobid}.out echo -e \" Copy/Paste this in your local terminal to ssh tunnel with remote Copie e cole no terminal local da sua m\u00e1quina o comando abaixo ----------------------------------------------------------------- ssh -N -L $ipnport : $ipnip : $ipnport -p4422 $USER @sc2.npad.ufrn.br ----------------------------------------------------------------- \" ## start an ipcluster instance and launch jupyter server ## Inicia servidor Jupyter jupyter-lab --no-browser --port = $ipnport --ip = $ipnip No nosso exemplo, o script foi salvo com nome script_jupyter.sh. Com o script criado, siga as seguintes etapas para acessar o jupyter no supercomputador. Primeiro, ative o conda forge no terminal do supercomputador. Digite: $ conda activate forge Envie o job para fila de execu\u00e7\u00e3o com o comando sbatch: $ sbatch script-jupyter.sh Submitted batch job JOBID JOBID \u00e9 n\u00famero do job. Agora, verifique se o seu job est\u00e1 rodando com o comando squeue: $ squeue -lu $USER Caso o job de c\u00f3digo JOBID esteja com estado R (RUNNING) na coluna STATE, o job j\u00e1 foi iniciado e podemos seguir em diante. Use o comando cat slurm-JOBID.out (substitua JOBID com o n\u00famero do job). $ cat slurm-JOBID.out Aparecer\u00e1 uma mensagem contendo duas informa\u00e7\u00f5es que voc\u00ea deveria copiar. A primeira \u00e9 a seguinte: ssh -N -L PORTA:IP:PORTA -p4422 NOMEDOUSUARIO@sc2.npad.ufrn.br Trata-se de um comando para ser executado no terminal de seu computador. Nesse comando, PORTA \u00e9 um valor n\u00famerico, IP s\u00e3o valores num\u00e9ricos separados por ponto e NOMEDOUSUARIO \u00e9 o nome do seu usu\u00e1rio no supercomputador. Copie e cole e execute ele no terminal do seu computador. O terminal ficar\u00e1 travado. A segunda informa\u00e7\u00e3o \u00e9 a seguinte: http://127.0.0.1:PORTA/lab?token = TOKEN Copie e cole no seu navegador e o jupyter ir\u00e1 aparecer. Quando terminar de usar o jupyter, destrave o terminal do seu computador com Ctrl+C. E tamb\u00e9m cancele o job do supercomputador: $ scancel JOBID","title":"Usando o Jupyter no supercomputador"},{"location":"advanced/jupyter_tutorial/#usando-o-jupyter-no-supercomputador","text":"O Jupyter \u00e9 uma aplica\u00e7\u00e3o web de c\u00f3digo-fonte aberto que permite voc\u00ea criar e compartilhar documentos (chamados de notebooks) que cont\u00e9m c\u00f3digo, equa\u00e7\u00f5es, visualiza\u00e7\u00f5es e texto. Para usar o jupyter no supercomputador, deve-se criar um script semelhante ao mostrado abaixo: #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=10 #Quantidade de n\u00facleos #SBATCH --time=1-00:00 #Tempo m\u00e1ximo do job no formato DIAS-HORAS:MINUTOS #SBATCH --hint=compute_bound # Execute conda activate forge # antes de submeter este script de job ## Parametros iniciais XDG_RUNTIME_DIR = \"\" ipnport = $( shuf -i8000-9999 -n1 ) ipnip = $( hostname -i ) ## Imprime na sa\u00edda slurm-{jobid}.out echo -e \" Copy/Paste this in your local terminal to ssh tunnel with remote Copie e cole no terminal local da sua m\u00e1quina o comando abaixo ----------------------------------------------------------------- ssh -N -L $ipnport : $ipnip : $ipnport -p4422 $USER @sc2.npad.ufrn.br ----------------------------------------------------------------- \" ## start an ipcluster instance and launch jupyter server ## Inicia servidor Jupyter jupyter-lab --no-browser --port = $ipnport --ip = $ipnip No nosso exemplo, o script foi salvo com nome script_jupyter.sh. Com o script criado, siga as seguintes etapas para acessar o jupyter no supercomputador. Primeiro, ative o conda forge no terminal do supercomputador. Digite: $ conda activate forge Envie o job para fila de execu\u00e7\u00e3o com o comando sbatch: $ sbatch script-jupyter.sh Submitted batch job JOBID JOBID \u00e9 n\u00famero do job. Agora, verifique se o seu job est\u00e1 rodando com o comando squeue: $ squeue -lu $USER Caso o job de c\u00f3digo JOBID esteja com estado R (RUNNING) na coluna STATE, o job j\u00e1 foi iniciado e podemos seguir em diante. Use o comando cat slurm-JOBID.out (substitua JOBID com o n\u00famero do job). $ cat slurm-JOBID.out Aparecer\u00e1 uma mensagem contendo duas informa\u00e7\u00f5es que voc\u00ea deveria copiar. A primeira \u00e9 a seguinte: ssh -N -L PORTA:IP:PORTA -p4422 NOMEDOUSUARIO@sc2.npad.ufrn.br Trata-se de um comando para ser executado no terminal de seu computador. Nesse comando, PORTA \u00e9 um valor n\u00famerico, IP s\u00e3o valores num\u00e9ricos separados por ponto e NOMEDOUSUARIO \u00e9 o nome do seu usu\u00e1rio no supercomputador. Copie e cole e execute ele no terminal do seu computador. O terminal ficar\u00e1 travado. A segunda informa\u00e7\u00e3o \u00e9 a seguinte: http://127.0.0.1:PORTA/lab?token = TOKEN Copie e cole no seu navegador e o jupyter ir\u00e1 aparecer. Quando terminar de usar o jupyter, destrave o terminal do seu computador com Ctrl+C. E tamb\u00e9m cancele o job do supercomputador: $ scancel JOBID","title":"Usando o Jupyter no supercomputador"},{"location":"advanced/mpi_tutorial/","text":"Tutorial de MPI \u00b6 Nesse tutorial iremos aprender a conduzir a execu\u00e7\u00e3o de aplica\u00e7\u00f5es utilizando o Intel MPI no supercomputador, uma das melhores implementa\u00e7\u00f5es MPI da ind\u00fastria, de maneira simples e pr\u00e1tica. O MPI \u00e9 um padr\u00e3o de troca de mensagens para uso em computa\u00e7\u00e3o paralela, oferecendo uma infraestrutura para a cria\u00e7\u00e3o de programas que utilizem recursos de CPU e mem\u00f3ria de m\u00faltiplos computadores em um cluster ou n\u00f3s em um supercomputador, al\u00e9m de permitir a passagem coordenada de mensagens entre processos. Definindo um padr\u00e3o industrial para a troca de mensagens, existem diversas implementa\u00e7\u00f5es do MPI. Iremos utilizar o Intel MPI juntamente com o gerenciador de recursos SLURM de acordo com os passos a seguir: Conex\u00e3o ao supercomputador por SSH Cria\u00e7\u00e3o de um programa MPI Compila\u00e7\u00e3o do programa MPI Execu\u00e7\u00e3o do programa no supercomputador Passo 1: Conex\u00e3o ao supercomputador por SSH \u00b6 Para se conectar ao supercomputador \u00e9 necess\u00e1rio utilizar o SSH j\u00e1 dispon\u00edvel por padr\u00e3o na maioria dos sistemas Unix como nas distribui\u00e7\u00f5es Linux e todas as vers\u00f5es do Mac OS X assim como dispon\u00edvel livremente na Internet como download para a plataforma Windows. O cliente SSH mais popular da plataforma Windows \u00e9 o Putty Iremos nesse tutorial utilizar como padr\u00e3o a plataforma Linux, mas sua execu\u00e7\u00e3o no Windows segue os mesmos princ\u00edpios. Para se conectar no n\u00f3 de login do supercomputador basta executar o seguinte comando: $ ssh -p 4422 USUARIO@sc2.npad.ufrn.br Troque USUARIO pelo seu nome de usu\u00e1rio j\u00e1 autorizado. Voc\u00ea deve visualizar uma tela com not\u00edcias sobre o supercomputador e a cria\u00e7\u00e3o dos seus diret\u00f3rios pessoais. Nesse momento voc\u00ea j\u00e1 est\u00e1 conectado ao supercomputador no ambiente Linux da m\u00e1quina em sua \u00e1rea pessoal de arquivos. Todos os comandos padr\u00e3o da distribui\u00e7\u00e3o CentOS assim como alguns editores de texto como o 'Emacs' e 'Vi' est\u00e3o dispon\u00edveis para que voc\u00ea possa manipular seus arquivos e pastas pessoais. Nesse momento voc\u00ea j\u00e1 est\u00e1 conectado ao supercomputador no ambiente Linux da m\u00e1quina em sua \u00e1rea pessoal de arquivos. Todos os comandos padr\u00e3o da distribui\u00e7\u00e3o CentOS assim como alguns editores de texto como o 'Emacs' e 'Vi' est\u00e3o dispon\u00edveis para que voc\u00ea possa manipular seus arquivos e pastas pessoais. Passo 2: Cria\u00e7\u00e3o de um programa MPI \u00b6 O esqueleto de um programa MPI \u00e9 estruturado da seguinte forma: Inclus\u00e3o do cabe\u00e7alho mpi.h e dos demais cabe\u00e7alhos necess\u00e1rios ao programa In\u00edcio do programa indicado pela linha de c\u00f3digo \"int main (int argc, char *argv[])\" C\u00f3digo sequencial opcional, por exemplo: inicializa\u00e7\u00e3o de vari\u00e1veis Inicializa\u00e7\u00e3o do c\u00f3digo paralelo indicado pela linha de c\u00f3digo \"MPI_Init(&argc, &argv);\" Leitura da quantidade de processos criados indicado pela linha de c\u00f3digo \"MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\" Leitura do rank do processo utilizado, indicado pela linha de c\u00f3digo \"MPI_Comm_rank(MPI_COMM_WORLD, &comm_sz);\" C\u00f3digo paralelo Finaliza\u00e7\u00e3o do c\u00f3digo paralelo indicado pela linha de c\u00f3digo \"MPI_Finalize();\" C\u00f3digo sequencial opcional e finaliza\u00e7\u00e3o do programa Um exemplo de programa MPI \u00e9 ilustrado abaixo. Copie esse programa para um arquivo na sua pasta pessoal e salve com o nome \"mpi_hello.c\". A fun\u00e7\u00e3o desse programa \u00e9 imprimir uma mensagem de forma paralela a partir de processos diferentes. Esses processos podem estar no mesmo n\u00f3, utilizando diferentes n\u00facleos, ou podem estar em n\u00f3s diferentes. Essa escolha \u00e9 feita pelo usu\u00e1rio a partir dos par\u00e2metros utilizados no script de submiss\u00e3o que ser\u00e1 mostrado numa se\u00e7\u00e3o (ou passo) posterior. #include <stdlib.h> #include <stdio.h> #include <mpi.h> int main ( int argc , char * argv []) { int rank , comm_sz , len ; char hostname [ MPI_MAX_PROCESSOR_NAME ]; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & comm_sz ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( hostname , & len ); printf ( \"Hello from process %d on node %s! \\n \" , rank , hostname ); if ( rank == 0 ) printf ( \"From process %d: Number of MPI processes is %d \\n \" , rank , comm_sz ); MPI_Finalize (); } Passo 3: Compila\u00e7\u00e3o do programa MPI \u00b6 De posse do arquivo mpi_hello.c , voc\u00ea pode compilar o mesmo com o comando \u201cmpiicc\u201d da seguinte forma: $ mpiicc mpi_hello.c -o mpi_hello $ ls mpi_hello.c mpi_hello A compila\u00e7\u00e3o gerou com sucesso o arquivo bin\u00e1rio \u201cmpi_hello\u201d que ser\u00e1 executado em v\u00e1rios n\u00facleos de processadores do supercomputador no pr\u00f3ximo passo. Passo 4: Execu\u00e7\u00e3o do programa no supercomputador \u00b6 No supercomputador \u00e9 utilizado o gerenciador de recursos SLURM para se executar tarefas em diversos processadores da m\u00e1quina. Tradicionalmente usu\u00e1rios do MPI utilizam o comando \u201cmpirun\u201d ou \u201cmpiexec\u201d, mas no nosso ambiente iremos utilizar o SLURM com o comando \u201csbatch\u201d que recebe um script bash contendo configura\u00e7\u00f5es espec\u00edficas sobre os recursos desejados, assim como que programa ser\u00e1 executado pela sua tarefa. Esse script \u00e9 chamado oficialmente de \u201cJob submission file\u201d. Exemplo de script mais simples \u00b6 Para executar 4 tarefas do programa \u201cmpi_hello\u201d devemos criar um script chamado, por exemplo, de \u201cjobMPI.sh\u201d com o conte\u00fado abaixo: #!/bin/bash srun --time = 0 -0:5 -n4 mpi_hello Para enviar esse \u201cjob\u201d para a fila de execu\u00e7\u00e3o usamos o comando: $ sbatch jobMPI.sh Submitted batch job 2230 Esse comando ir\u00e1 adicionar o \u201cjob\u201d na fila de execu\u00e7\u00e3o e retornar um id (nesse caso 2230) dessa nova tarefa adicionada. Para monitorar a tarefa voc\u00ea pode usar o comando squeue para visualizar todas as tarefas em andamento. Se desejar cancelar uma tarefa em execu\u00e7\u00e3o utilize o comando scancel JOB_ID . Ao finalizar sua execu\u00e7\u00e3o o SLURM cria automaticamente um arquivo chamado \u201cslurm-JOB_ID.out\u201d com a sa\u00edda padr\u00e3o de sua tarefa redirecionada para esse arquivo. O nome do arquivo cont\u00e9m o ID do job enviado (JOB_ID) para facilitar sua rela\u00e7\u00e3o com o mesmo. Isso \u00e9 importante se seu programa gera uma sa\u00edda padr\u00e3o que cont\u00e9m um resultado \u00fatil da sua execu\u00e7\u00e3o. Veja abaixo o resultado da execu\u00e7\u00e3o do simples script que criamos e sua sa\u00edda em arquivos: $ sbatch jobMPI.sh Submitted batch job 2230 $ cat slurm-2230.out Hello from process 3 on node r1i2n16! Hello from process 2 on node r1i2n16! Hello from process 1 on node r1i2n16! Hello from process 0 on node r1i2n16! From process 0 : Number of MPI processes is 4 Repare que todas as 4 tarefas foram executadas no mesmo n\u00f3 (neste caso o r1i2n16), pois o SLURM s\u00f3 foi instru\u00eddo para executar 4 tarefas e nada mais. Exemplo de script mais detalhado \u00b6 Um exemplo de script que solicita que 4 tarefas do programa \u201cmpi_hello\u201d executem em 2 n\u00f3s diferentes \u00e9 mostrado abaixo: #!/bin/bash #SBATCH --job-name=MPI_hello #SBATCH --output=saida%j.out #SBATCH --error=erro%j.err #SBATCH --nodes=2 #SBATCH --ntasks-per-node=2 #SBATCH --cpus-per-task=1 #SBATCH --time=0-0:5 srun mpi_hello Repare que agora o script cont\u00e9m v\u00e1rios par\u00e2metros que foram adicionados. A descri\u00e7\u00e3o desses par\u00e2metros \u00e9 informada abaixo: job-name : Nome que aparecer\u00e1 na fila de jobs output : Arquivo de sa\u00edda padr\u00e3o do programa error : Arquivo de sa\u00edda de erro do programa nodes : Quantidade de n\u00f3s alocados para o programa ntasks-per-node : N\u00famero de processos em um mesmo n\u00f3 (normalmente 1 para jobs OpenMP) - \u00datil para jobs MPI cpus-per-task : N\u00famero de n\u00facleos de CPU alocados para um processo do programa time : Tempo m\u00e1ximo para execu\u00e7\u00e3o do job (nesse exemplo = 5 min). Caso o job ainda n\u00e3o tenha terminado, ap\u00f3s esse tempo ele ser\u00e1 cancelado. Formato: dias-horas:minutos Observa-se, ent\u00e3o, que foram requisitados 2 (valor de --nodes) n\u00f3s para os jobs e que em cada n\u00f3 ser\u00e3o criados 2 (valor de --ntasks-per-node) processos. O comando \u201csrun mpi_hello\u201d agora n\u00e3o deve mais conter o n\u00famero de tarefas que ser\u00e1 executada, uma vez que isso j\u00e1 est\u00e1 definido anteriormente no script. Veja sua execu\u00e7\u00e3o: $ sbatch jobMPI.sh Submitted batch job 2236 [ seuUsuario@service0 ~ ] $ cat slurm-2236.out Hello from process 3 on node r1i1n2! Hello from process 2 on node r1i1n2! Hello from process 1 on node r1i3n5! Hello from process 0 on node r1i3n5! From process 0 : Number of MPI processes is 4 Repare que agora as 4 tarefas foram executadas em dois n\u00f3s distintos (r1i1n2 e r1i3n5), exatamente da forma que instru\u00edmos. Assim finalizamos nosso tutorial Intel MPI. Para mais informa\u00e7\u00f5es sobre o Intel MPI e o SLURM acesse os links abaixo: http://slurm.schedmd.com/ https://software.intel.com/en-us/intel-mpi-library","title":"Tutorial de MPI"},{"location":"advanced/mpi_tutorial/#tutorial-de-mpi","text":"Nesse tutorial iremos aprender a conduzir a execu\u00e7\u00e3o de aplica\u00e7\u00f5es utilizando o Intel MPI no supercomputador, uma das melhores implementa\u00e7\u00f5es MPI da ind\u00fastria, de maneira simples e pr\u00e1tica. O MPI \u00e9 um padr\u00e3o de troca de mensagens para uso em computa\u00e7\u00e3o paralela, oferecendo uma infraestrutura para a cria\u00e7\u00e3o de programas que utilizem recursos de CPU e mem\u00f3ria de m\u00faltiplos computadores em um cluster ou n\u00f3s em um supercomputador, al\u00e9m de permitir a passagem coordenada de mensagens entre processos. Definindo um padr\u00e3o industrial para a troca de mensagens, existem diversas implementa\u00e7\u00f5es do MPI. Iremos utilizar o Intel MPI juntamente com o gerenciador de recursos SLURM de acordo com os passos a seguir: Conex\u00e3o ao supercomputador por SSH Cria\u00e7\u00e3o de um programa MPI Compila\u00e7\u00e3o do programa MPI Execu\u00e7\u00e3o do programa no supercomputador","title":"Tutorial de MPI"},{"location":"advanced/mpi_tutorial/#passo-1-conexao-ao-supercomputador-por-ssh","text":"Para se conectar ao supercomputador \u00e9 necess\u00e1rio utilizar o SSH j\u00e1 dispon\u00edvel por padr\u00e3o na maioria dos sistemas Unix como nas distribui\u00e7\u00f5es Linux e todas as vers\u00f5es do Mac OS X assim como dispon\u00edvel livremente na Internet como download para a plataforma Windows. O cliente SSH mais popular da plataforma Windows \u00e9 o Putty Iremos nesse tutorial utilizar como padr\u00e3o a plataforma Linux, mas sua execu\u00e7\u00e3o no Windows segue os mesmos princ\u00edpios. Para se conectar no n\u00f3 de login do supercomputador basta executar o seguinte comando: $ ssh -p 4422 USUARIO@sc2.npad.ufrn.br Troque USUARIO pelo seu nome de usu\u00e1rio j\u00e1 autorizado. Voc\u00ea deve visualizar uma tela com not\u00edcias sobre o supercomputador e a cria\u00e7\u00e3o dos seus diret\u00f3rios pessoais. Nesse momento voc\u00ea j\u00e1 est\u00e1 conectado ao supercomputador no ambiente Linux da m\u00e1quina em sua \u00e1rea pessoal de arquivos. Todos os comandos padr\u00e3o da distribui\u00e7\u00e3o CentOS assim como alguns editores de texto como o 'Emacs' e 'Vi' est\u00e3o dispon\u00edveis para que voc\u00ea possa manipular seus arquivos e pastas pessoais. Nesse momento voc\u00ea j\u00e1 est\u00e1 conectado ao supercomputador no ambiente Linux da m\u00e1quina em sua \u00e1rea pessoal de arquivos. Todos os comandos padr\u00e3o da distribui\u00e7\u00e3o CentOS assim como alguns editores de texto como o 'Emacs' e 'Vi' est\u00e3o dispon\u00edveis para que voc\u00ea possa manipular seus arquivos e pastas pessoais.","title":"Passo 1: Conex\u00e3o ao supercomputador por SSH"},{"location":"advanced/mpi_tutorial/#passo-2-criacao-de-um-programa-mpi","text":"O esqueleto de um programa MPI \u00e9 estruturado da seguinte forma: Inclus\u00e3o do cabe\u00e7alho mpi.h e dos demais cabe\u00e7alhos necess\u00e1rios ao programa In\u00edcio do programa indicado pela linha de c\u00f3digo \"int main (int argc, char *argv[])\" C\u00f3digo sequencial opcional, por exemplo: inicializa\u00e7\u00e3o de vari\u00e1veis Inicializa\u00e7\u00e3o do c\u00f3digo paralelo indicado pela linha de c\u00f3digo \"MPI_Init(&argc, &argv);\" Leitura da quantidade de processos criados indicado pela linha de c\u00f3digo \"MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\" Leitura do rank do processo utilizado, indicado pela linha de c\u00f3digo \"MPI_Comm_rank(MPI_COMM_WORLD, &comm_sz);\" C\u00f3digo paralelo Finaliza\u00e7\u00e3o do c\u00f3digo paralelo indicado pela linha de c\u00f3digo \"MPI_Finalize();\" C\u00f3digo sequencial opcional e finaliza\u00e7\u00e3o do programa Um exemplo de programa MPI \u00e9 ilustrado abaixo. Copie esse programa para um arquivo na sua pasta pessoal e salve com o nome \"mpi_hello.c\". A fun\u00e7\u00e3o desse programa \u00e9 imprimir uma mensagem de forma paralela a partir de processos diferentes. Esses processos podem estar no mesmo n\u00f3, utilizando diferentes n\u00facleos, ou podem estar em n\u00f3s diferentes. Essa escolha \u00e9 feita pelo usu\u00e1rio a partir dos par\u00e2metros utilizados no script de submiss\u00e3o que ser\u00e1 mostrado numa se\u00e7\u00e3o (ou passo) posterior. #include <stdlib.h> #include <stdio.h> #include <mpi.h> int main ( int argc , char * argv []) { int rank , comm_sz , len ; char hostname [ MPI_MAX_PROCESSOR_NAME ]; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & comm_sz ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( hostname , & len ); printf ( \"Hello from process %d on node %s! \\n \" , rank , hostname ); if ( rank == 0 ) printf ( \"From process %d: Number of MPI processes is %d \\n \" , rank , comm_sz ); MPI_Finalize (); }","title":"Passo 2: Cria\u00e7\u00e3o de um programa MPI"},{"location":"advanced/mpi_tutorial/#passo-3-compilacao-do-programa-mpi","text":"De posse do arquivo mpi_hello.c , voc\u00ea pode compilar o mesmo com o comando \u201cmpiicc\u201d da seguinte forma: $ mpiicc mpi_hello.c -o mpi_hello $ ls mpi_hello.c mpi_hello A compila\u00e7\u00e3o gerou com sucesso o arquivo bin\u00e1rio \u201cmpi_hello\u201d que ser\u00e1 executado em v\u00e1rios n\u00facleos de processadores do supercomputador no pr\u00f3ximo passo.","title":"Passo 3: Compila\u00e7\u00e3o do programa MPI"},{"location":"advanced/mpi_tutorial/#passo-4-execucao-do-programa-no-supercomputador","text":"No supercomputador \u00e9 utilizado o gerenciador de recursos SLURM para se executar tarefas em diversos processadores da m\u00e1quina. Tradicionalmente usu\u00e1rios do MPI utilizam o comando \u201cmpirun\u201d ou \u201cmpiexec\u201d, mas no nosso ambiente iremos utilizar o SLURM com o comando \u201csbatch\u201d que recebe um script bash contendo configura\u00e7\u00f5es espec\u00edficas sobre os recursos desejados, assim como que programa ser\u00e1 executado pela sua tarefa. Esse script \u00e9 chamado oficialmente de \u201cJob submission file\u201d.","title":"Passo 4: Execu\u00e7\u00e3o do programa no supercomputador"},{"location":"advanced/mpi_tutorial/#exemplo-de-script-mais-simples","text":"Para executar 4 tarefas do programa \u201cmpi_hello\u201d devemos criar um script chamado, por exemplo, de \u201cjobMPI.sh\u201d com o conte\u00fado abaixo: #!/bin/bash srun --time = 0 -0:5 -n4 mpi_hello Para enviar esse \u201cjob\u201d para a fila de execu\u00e7\u00e3o usamos o comando: $ sbatch jobMPI.sh Submitted batch job 2230 Esse comando ir\u00e1 adicionar o \u201cjob\u201d na fila de execu\u00e7\u00e3o e retornar um id (nesse caso 2230) dessa nova tarefa adicionada. Para monitorar a tarefa voc\u00ea pode usar o comando squeue para visualizar todas as tarefas em andamento. Se desejar cancelar uma tarefa em execu\u00e7\u00e3o utilize o comando scancel JOB_ID . Ao finalizar sua execu\u00e7\u00e3o o SLURM cria automaticamente um arquivo chamado \u201cslurm-JOB_ID.out\u201d com a sa\u00edda padr\u00e3o de sua tarefa redirecionada para esse arquivo. O nome do arquivo cont\u00e9m o ID do job enviado (JOB_ID) para facilitar sua rela\u00e7\u00e3o com o mesmo. Isso \u00e9 importante se seu programa gera uma sa\u00edda padr\u00e3o que cont\u00e9m um resultado \u00fatil da sua execu\u00e7\u00e3o. Veja abaixo o resultado da execu\u00e7\u00e3o do simples script que criamos e sua sa\u00edda em arquivos: $ sbatch jobMPI.sh Submitted batch job 2230 $ cat slurm-2230.out Hello from process 3 on node r1i2n16! Hello from process 2 on node r1i2n16! Hello from process 1 on node r1i2n16! Hello from process 0 on node r1i2n16! From process 0 : Number of MPI processes is 4 Repare que todas as 4 tarefas foram executadas no mesmo n\u00f3 (neste caso o r1i2n16), pois o SLURM s\u00f3 foi instru\u00eddo para executar 4 tarefas e nada mais.","title":"Exemplo de script mais simples"},{"location":"advanced/mpi_tutorial/#exemplo-de-script-mais-detalhado","text":"Um exemplo de script que solicita que 4 tarefas do programa \u201cmpi_hello\u201d executem em 2 n\u00f3s diferentes \u00e9 mostrado abaixo: #!/bin/bash #SBATCH --job-name=MPI_hello #SBATCH --output=saida%j.out #SBATCH --error=erro%j.err #SBATCH --nodes=2 #SBATCH --ntasks-per-node=2 #SBATCH --cpus-per-task=1 #SBATCH --time=0-0:5 srun mpi_hello Repare que agora o script cont\u00e9m v\u00e1rios par\u00e2metros que foram adicionados. A descri\u00e7\u00e3o desses par\u00e2metros \u00e9 informada abaixo: job-name : Nome que aparecer\u00e1 na fila de jobs output : Arquivo de sa\u00edda padr\u00e3o do programa error : Arquivo de sa\u00edda de erro do programa nodes : Quantidade de n\u00f3s alocados para o programa ntasks-per-node : N\u00famero de processos em um mesmo n\u00f3 (normalmente 1 para jobs OpenMP) - \u00datil para jobs MPI cpus-per-task : N\u00famero de n\u00facleos de CPU alocados para um processo do programa time : Tempo m\u00e1ximo para execu\u00e7\u00e3o do job (nesse exemplo = 5 min). Caso o job ainda n\u00e3o tenha terminado, ap\u00f3s esse tempo ele ser\u00e1 cancelado. Formato: dias-horas:minutos Observa-se, ent\u00e3o, que foram requisitados 2 (valor de --nodes) n\u00f3s para os jobs e que em cada n\u00f3 ser\u00e3o criados 2 (valor de --ntasks-per-node) processos. O comando \u201csrun mpi_hello\u201d agora n\u00e3o deve mais conter o n\u00famero de tarefas que ser\u00e1 executada, uma vez que isso j\u00e1 est\u00e1 definido anteriormente no script. Veja sua execu\u00e7\u00e3o: $ sbatch jobMPI.sh Submitted batch job 2236 [ seuUsuario@service0 ~ ] $ cat slurm-2236.out Hello from process 3 on node r1i1n2! Hello from process 2 on node r1i1n2! Hello from process 1 on node r1i3n5! Hello from process 0 on node r1i3n5! From process 0 : Number of MPI processes is 4 Repare que agora as 4 tarefas foram executadas em dois n\u00f3s distintos (r1i1n2 e r1i3n5), exatamente da forma que instru\u00edmos. Assim finalizamos nosso tutorial Intel MPI. Para mais informa\u00e7\u00f5es sobre o Intel MPI e o SLURM acesse os links abaixo: http://slurm.schedmd.com/ https://software.intel.com/en-us/intel-mpi-library","title":"Exemplo de script mais detalhado"},{"location":"advanced/openmp_tutorial/","text":"Tutorial de OpenMP \u00b6 Veremos neste tutorial como executar um job contendo c\u00f3digos escritos com OpenMP no supercomputador. OpenMP \u00e9 um modelo de programa\u00e7\u00e3o paralela para sistemas de mem\u00f3ria compartilhada. Nesse modelo, o programa cria diversas threads que s\u00e3o coordenadas por um thread mestre. O programador define algumas se\u00e7\u00f5es do c\u00f3digo como paralelas utilizando diretivas de preprocessamento espec\u00edficas. Os n\u00f3s do supercomputador n\u00e3o compartilham mem\u00f3ria. Por isso, n\u00e3o \u00e9 poss\u00edvel executar em mais de um n\u00f3 um job que utiliza somente OpenMP como ferramenta de programa\u00e7\u00e3o parelela. Contudo, um programa escrito com OpenMP pode ser executado em m\u00faltiplos cores em um mesmo n\u00f3. Para que um job seja executado em v\u00e1rios n\u00f3s, deve-se utilizar um modelo de program\u00e7\u00e1o paralela para sistemas de mem\u00f3ria distribu\u00edda (e.g. MPI). Tamb\u00e9m \u00e9 poss\u00edvel utilizar OpenMP e MPI em um mesmo c\u00f3digo. Exemplo: Hello World \u00b6 //Arquivo hello_openmp.c #include <omp.h> #include <stdio.h> int main ( int argc , char * argv []) { int nthreads , thread_id ; #pragma omp parallel private(nthreads, thread_id) { thread_id = omp_get_thread_num (); printf ( \"Thread %d says: Hello World \\n \" , thread_id ); if ( thread_id == 0 ) { nthreads = omp_get_num_threads (); printf ( \"Thread %d reports: the number of threads are %d \\n \" , thread_id , nthreads ); } } return 0 ; } Em seguida, \u00e9 necess\u00e1rio escrever um script para executar o programa. #!/bin/bash #SBATCH --job-name=OMP_hello #SBATCH --time=0-0:5 #SBATCH --cpus-per-task=8 #SBATCH --hint=compute_bound export OMP_NUM_THREADS = 32 ./hello_openmp A descri\u00e7\u00e3o dos valores dos campos do script \u00e9 informada abaixo: job-name: Nome que aparecer\u00e1 na fila de jobs time: Tempo m\u00e1ximo para execu\u00e7\u00e3o do job (neste exemplo = 5 min). Caso o job ainda n\u00e3o tenha terminado, ap\u00f3s esse tempo ele ser\u00e1 cancelado. Formato: dias-horas:minutos. Note a configura\u00e7\u00e3o da vari\u00e1vel de ambiente OMP_NUM_THREADS para 32. Isso controla quantas threads ser\u00e3o utilizadas nos jobs que usam OpenMP. Configurar essa vari\u00e1vel para um n\u00famero maior que 32 provavelmente n\u00e3o ir\u00e1 melhorar o desempenho do programa uma vez que cada n\u00f3 do supercomputador possui 32 cores. Submiss\u00e3o de um job \u00b6 Para enviar arquivos para o supercomputador recomenda-se utilizar o comando rsync. $ rsync -azP -e \"ssh -p 4422\" ~/pastaLocal/ { hello_openmp.c,run.slurm } seuUsuario@sc2.npad.ufrn.brr:~/pastaRemota O acesso ao supercomputador pode ser feito atrav\u00e9s do protocolo SSH (Secure Shell). $ ssh -p4422 nome_do_usuario@sc2.npad.ufrn.brr Em seguida, acesse a pasta onde est\u00e3o o c\u00f3digo fonte em OpenMP e o script de execu\u00e7\u00e3o. $ cd pastaRemota/ Compile o c\u00f3digo fonte com a flag de otimiza\u00e7\u00e3o \"-O_\" desejada. No exemplo abaixo, \u00e9 utilizada a flag -O2 (a flag \u00e9 habilitada pela letra \"O\" e n\u00e3o o n\u00famero \"0\"). $ icc -O2 hello_openmp.c -o hello_openmp -openmp Flags de otimiza\u00e7\u00e3o \u00b6 -O0 Desabilita todas as otimiza\u00e7\u00f5es. -O1 Habilita otimiza\u00e7\u00e3o para melhorar velocidade, entretanto desabilita algumas otimiza\u00e7\u00f5es que aumentam o tamanho do c\u00f3digo e afetam a velocidade. -O2 Habilita otimiza\u00e7\u00e3o para melhorar velocidade. Esse \u00e9 o n\u00edvel de otimiza\u00e7\u00e3o normalente recomend\u00e1vel e padr\u00e3o para o icc. Vetoriza\u00e7\u00e3o \u00e9 habilitada em O2 e n\u00edveis maiores. -O3 Realiza otimiza\u00e7\u00f5es O2 e habilita transforma\u00e7\u00f5es de loop mais agressivos. Essas otimiza\u00e7\u00f5es podem deixar o c\u00f3digo mais lento em alguns casos quando comparados a O2. A op\u00e7\u00e3o O3 \u00e9 recomendada para aplica\u00e7\u00f5es que t\u00eam loops que utilizam fortemente c\u00e1lculos de ponto flutante e processam grandes conjuntos de dados. Quanto maior o n\u00edvel de otimiza\u00e7\u00e3o, mais liberdade o compilador ter\u00e1 para fazer suposi\u00e7\u00f5es sobre o seu c\u00f3digo, podendo produzir resultados indesejados. Submeta o job. $ sbatch run.slurm O job entrar\u00e1 em uma fila para ser executado. Quando isso acontecer, ser\u00e1 poss\u00edvel verificar as sa\u00eddas que foram salvas nos arquivos configurados nas tags \"--output\" e \"--error\". $ cat slurm.out Thread 0 says: Hello Word! Thread 0 reports: the number of threads are 8 Thread 7 says: Hello Word! Thread 4 says: Hello Word! Thread 6 says: Hello Word! Thread 1 says: Hello Word! Thread 3 says: Hello Word! Thread 5 says: Hello Word! Thread 2 says: Hello Word!","title":"Tutorial de OpenMP"},{"location":"advanced/openmp_tutorial/#tutorial-de-openmp","text":"Veremos neste tutorial como executar um job contendo c\u00f3digos escritos com OpenMP no supercomputador. OpenMP \u00e9 um modelo de programa\u00e7\u00e3o paralela para sistemas de mem\u00f3ria compartilhada. Nesse modelo, o programa cria diversas threads que s\u00e3o coordenadas por um thread mestre. O programador define algumas se\u00e7\u00f5es do c\u00f3digo como paralelas utilizando diretivas de preprocessamento espec\u00edficas. Os n\u00f3s do supercomputador n\u00e3o compartilham mem\u00f3ria. Por isso, n\u00e3o \u00e9 poss\u00edvel executar em mais de um n\u00f3 um job que utiliza somente OpenMP como ferramenta de programa\u00e7\u00e3o parelela. Contudo, um programa escrito com OpenMP pode ser executado em m\u00faltiplos cores em um mesmo n\u00f3. Para que um job seja executado em v\u00e1rios n\u00f3s, deve-se utilizar um modelo de program\u00e7\u00e1o paralela para sistemas de mem\u00f3ria distribu\u00edda (e.g. MPI). Tamb\u00e9m \u00e9 poss\u00edvel utilizar OpenMP e MPI em um mesmo c\u00f3digo.","title":"Tutorial de OpenMP"},{"location":"advanced/openmp_tutorial/#exemplo-hello-world","text":"//Arquivo hello_openmp.c #include <omp.h> #include <stdio.h> int main ( int argc , char * argv []) { int nthreads , thread_id ; #pragma omp parallel private(nthreads, thread_id) { thread_id = omp_get_thread_num (); printf ( \"Thread %d says: Hello World \\n \" , thread_id ); if ( thread_id == 0 ) { nthreads = omp_get_num_threads (); printf ( \"Thread %d reports: the number of threads are %d \\n \" , thread_id , nthreads ); } } return 0 ; } Em seguida, \u00e9 necess\u00e1rio escrever um script para executar o programa. #!/bin/bash #SBATCH --job-name=OMP_hello #SBATCH --time=0-0:5 #SBATCH --cpus-per-task=8 #SBATCH --hint=compute_bound export OMP_NUM_THREADS = 32 ./hello_openmp A descri\u00e7\u00e3o dos valores dos campos do script \u00e9 informada abaixo: job-name: Nome que aparecer\u00e1 na fila de jobs time: Tempo m\u00e1ximo para execu\u00e7\u00e3o do job (neste exemplo = 5 min). Caso o job ainda n\u00e3o tenha terminado, ap\u00f3s esse tempo ele ser\u00e1 cancelado. Formato: dias-horas:minutos. Note a configura\u00e7\u00e3o da vari\u00e1vel de ambiente OMP_NUM_THREADS para 32. Isso controla quantas threads ser\u00e3o utilizadas nos jobs que usam OpenMP. Configurar essa vari\u00e1vel para um n\u00famero maior que 32 provavelmente n\u00e3o ir\u00e1 melhorar o desempenho do programa uma vez que cada n\u00f3 do supercomputador possui 32 cores.","title":"Exemplo: Hello World"},{"location":"advanced/openmp_tutorial/#submissao-de-um-job","text":"Para enviar arquivos para o supercomputador recomenda-se utilizar o comando rsync. $ rsync -azP -e \"ssh -p 4422\" ~/pastaLocal/ { hello_openmp.c,run.slurm } seuUsuario@sc2.npad.ufrn.brr:~/pastaRemota O acesso ao supercomputador pode ser feito atrav\u00e9s do protocolo SSH (Secure Shell). $ ssh -p4422 nome_do_usuario@sc2.npad.ufrn.brr Em seguida, acesse a pasta onde est\u00e3o o c\u00f3digo fonte em OpenMP e o script de execu\u00e7\u00e3o. $ cd pastaRemota/ Compile o c\u00f3digo fonte com a flag de otimiza\u00e7\u00e3o \"-O_\" desejada. No exemplo abaixo, \u00e9 utilizada a flag -O2 (a flag \u00e9 habilitada pela letra \"O\" e n\u00e3o o n\u00famero \"0\"). $ icc -O2 hello_openmp.c -o hello_openmp -openmp","title":"Submiss\u00e3o de um job"},{"location":"advanced/openmp_tutorial/#flags-de-otimizacao","text":"-O0 Desabilita todas as otimiza\u00e7\u00f5es. -O1 Habilita otimiza\u00e7\u00e3o para melhorar velocidade, entretanto desabilita algumas otimiza\u00e7\u00f5es que aumentam o tamanho do c\u00f3digo e afetam a velocidade. -O2 Habilita otimiza\u00e7\u00e3o para melhorar velocidade. Esse \u00e9 o n\u00edvel de otimiza\u00e7\u00e3o normalente recomend\u00e1vel e padr\u00e3o para o icc. Vetoriza\u00e7\u00e3o \u00e9 habilitada em O2 e n\u00edveis maiores. -O3 Realiza otimiza\u00e7\u00f5es O2 e habilita transforma\u00e7\u00f5es de loop mais agressivos. Essas otimiza\u00e7\u00f5es podem deixar o c\u00f3digo mais lento em alguns casos quando comparados a O2. A op\u00e7\u00e3o O3 \u00e9 recomendada para aplica\u00e7\u00f5es que t\u00eam loops que utilizam fortemente c\u00e1lculos de ponto flutante e processam grandes conjuntos de dados. Quanto maior o n\u00edvel de otimiza\u00e7\u00e3o, mais liberdade o compilador ter\u00e1 para fazer suposi\u00e7\u00f5es sobre o seu c\u00f3digo, podendo produzir resultados indesejados. Submeta o job. $ sbatch run.slurm O job entrar\u00e1 em uma fila para ser executado. Quando isso acontecer, ser\u00e1 poss\u00edvel verificar as sa\u00eddas que foram salvas nos arquivos configurados nas tags \"--output\" e \"--error\". $ cat slurm.out Thread 0 says: Hello Word! Thread 0 reports: the number of threads are 8 Thread 7 says: Hello Word! Thread 4 says: Hello Word! Thread 6 says: Hello Word! Thread 1 says: Hello Word! Thread 3 says: Hello Word! Thread 5 says: Hello Word! Thread 2 says: Hello Word!","title":"Flags de otimiza\u00e7\u00e3o"},{"location":"advanced/tch-rs_tutorial/","text":"tch-rs-example MNIST \u00b6 Nesse tutorial voc\u00ea ir\u00e1 aprender a usar a parti\u00e7\u00e3o gpu e rust para treinar um modelo de deep learning para resolver o MNIST . tch-rs-example MNIST Instalando depend\u00eancias Criando um aplica\u00e7\u00e3o em rust para treinar uma rede neural Carregamento do conjunto de dados MNIST Configura\u00e7\u00e3o do dispositivo de processamento Criando um modelo deep learning Treinamento Executando a aplica\u00e7\u00e3o com cuda no supercomputador Instalando depend\u00eancias \u00b6 Primeiro instale o rust toolchain na sua pasta HOME, no n\u00f3 de login. $ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh segundo crie um novo projeto rust com cargo chamado pytorch-example. Para esse projeto Adicione as depend\u00eancias: anyhow , uma biblioteca para facilita o tratamento de erro em rust tch O framework Pytorch para cria\u00e7\u00e3o de modelos deep learning escrito em C++, mas com bindings para rust cargo new pytorch-example cd pytorch-example cargo add anyhow cargo add tch Criando um aplica\u00e7\u00e3o em rust para treinar uma rede neural \u00b6 A aplica\u00e7\u00e3o escrita em rust, dever\u00e1 selecionar qual dispositivo de processamento, CPU ou GPU, dever\u00e1 ser usado para a execu\u00e7\u00e3o dos c\u00e1lculos num\u00e9ricos. Carregar o conjunto de dados MNIST para ser computado em tal dispositivo. Criar um modelo deep learning e treinar o modelo. Portanto podemos entender a aplica\u00e7\u00e3o em 4 partes importantes: carregamento do conjunto de dados MNIST, configura\u00e7\u00e3o do dispositivo, criando um modelo deep learning, treinamento. Carregamento do conjunto de dados MNIST \u00b6 Como o MNIST \u00e9 um conjunto de dados muito famoso, o pr\u00f3prio pytorch possui mecanismos de carreg\u00e1-lo, desde que voc\u00ea tenha ele baixado e descompactado. Para baixar o dataset, voc\u00ea pode criar um script similar ao get_inputs.sh . Onde basicamente ele cria o diret\u00f3rio chamado data e baixa os arquivos do dataset e os extrai com gunzip #!/bin/bash # get_inputs.sh mkdir data -p cd data wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz ; gunzip train-images-idx3-ubyte.gz wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz ; gunzip train-labels-idx1-ubyte.gz wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz ; gunzip t10k-images-idx3-ubyte.gz wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz ; gunzip t10k-labels-idx1-ubyte.gz Para carregar o dataset basta usar o a fun\u00e7\u00e3o tch::vision::mnist::load_dir let dataset = match tch :: vision :: mnist :: load_dir ( \"data\" ) { Dataset Ok ( d ) => d , Err ( _ ) => panic! ( \"Dataset Not found, run the get_inputs.sh !!\" ), }; Configura\u00e7\u00e3o do dispositivo de processamento \u00b6 Utilizando um framework pytorch podemos selecionar o dispositivo da seguinte forma, se uma GPU estiver dispon\u00edvel, ent\u00e3o utilize GPU. Caso o contr\u00e1rio utilize CPU . Com o dispositivo podemos criar um conjunto de tensores cujo seus valores variam e que esses valores dever\u00e3o ser capazes de serem armazenados, ou seja criamos uma VarStore . let device = tch :: Device :: cuda_if_available (); let vs = tch :: nn :: VarStore :: new ( device . clone ()); Tamb\u00e9m precisamos transformar ou mover os dados do conjunto de dados para o dispositivo de processamento fn dataset_to_device ( dataset : Dataset , device : & Device ) -> Dataset { let train_labels = dataset . train_labels . to_device ( device . clone ()); let train_images = dataset . train_images . to_device ( device . clone ()); let test_labels = dataset . test_labels . to_device ( device . clone ()); let test_images = dataset . test_images . to_device ( device . clone ()); Dataset { test_images , test_labels , train_images , train_labels , labels : dataset . labels , } } let dataset = dataset_to_device ( dataset , & device ); Criando um modelo deep learning \u00b6 O modelo deep learning utilizado nesse exemplo ser\u00e1 um modelo sequencial simples e com poucas camadas. Em rust o modelo ficou assim: use tch :: { nn , nn :: Module }; const IMAGE_DIM : i64 = 784 ; const HIDDEN_NODES : i64 = 128 ; const LABELS : i64 = 10 ; fn net ( vs : & nn :: Path ) -> impl Module { nn :: seq () . add ( nn :: linear ( vs / \"layer1\" , IMAGE_DIM , HIDDEN_NODES , Default :: default (), )) . add_fn ( | xs | xs . relu ()) . add ( nn :: linear ( vs , HIDDEN_NODES , LABELS , Default :: default ())) } let net = net ( & vs . root ()); Observe que vs (VarStore) \u00e9 passada na fun\u00e7\u00e3o net , de modo que criar uma camada, ou m\u00f3dulo (layer) \u00e9 alocar novos de tensores, para a vari\u00e1vel. Treinamento \u00b6 Para treinar uma rede neural, pode-se utilizar v\u00e1rios algoritmos de otimiza\u00e7\u00e3o, por\u00e9m nesse exemplo foi utilizado o Adam , com uma taxa de aprendizado de 0.001 . O modelo ser\u00e1 treinado durante 200 intera\u00e7\u00f5es. Em rust a implementa\u00e7\u00e3o do treinamento fica da seguinte forma let mut opt = nn :: Adam :: default (). build ( & vs , 1e-3 ) ? ; for epoch in 1 .. 200 { let loss = net . forward ( & dataset . train_images ) . cross_entropy_for_logits ( & dataset . train_labels ); opt . backward_step ( & loss ); let test_accuracy = net . forward ( & dataset . test_images ) . accuracy_for_logits ( & dataset . test_labels ); println! ( \"epoch: {:4} train loss: {:8.5} test acc: {:5.2}% is cuda: {}\" , epoch , f64 :: try_from ( & loss ) ? , 100. * f64 :: try_from ( & test_accuracy ) ? , device . is_cuda (), ); } Voc\u00ea pode ver a implementa\u00e7\u00e3o completa em main.rs Executando a aplica\u00e7\u00e3o com cuda no supercomputador \u00b6 Atualmente o supercomputador no NPAD possui uma parti\u00e7\u00e3o chamada gpu , nessa parti\u00e7\u00e3o encontram-se os n\u00f3s com GPUs bem potentes capazes de executar a aplica\u00e7\u00e3o em 1 segundo. #!/bin/bash #SBATCH --job-name=neural_train #SBATCH --time=0-0:15 #SBATCH --partition=gpu #SBATCH --exclusive # informando ao tch-rs que desejo compilar com cuda na vers\u00e3o 11.7 export TORCH_CUDA_VERSION = cu117 cargo r --release Cargo \u00e9 o gerenciador de pacotes official da linguagem Rust, perceba que ao executar o comando cargo r --release . A aplica\u00e7\u00e3o cargo ir\u00e1 compilar a aplica\u00e7\u00e3o utilizando flags de otimiza\u00e7\u00e3o e ir\u00e1 executar o programa. Caso tenha compilado a aplica\u00e7\u00e3o no n\u00f3 de login, ser\u00e1 necess\u00e1rio remover a pasta target , antes de submeter o script # rm -rf target # caso tenha compilado a aplica\u00e7\u00e3o no n\u00f3 de login. sbatch run_on_superpc.sh o script run_on_superpc.sh pode ser encontrado aqui . Todo o projeto pode ser encontrado no github github.com/samuel-cavalcanti/tch-rs-example","title":"tch-rs-example MNIST"},{"location":"advanced/tch-rs_tutorial/#tch-rs-example-mnist","text":"Nesse tutorial voc\u00ea ir\u00e1 aprender a usar a parti\u00e7\u00e3o gpu e rust para treinar um modelo de deep learning para resolver o MNIST . tch-rs-example MNIST Instalando depend\u00eancias Criando um aplica\u00e7\u00e3o em rust para treinar uma rede neural Carregamento do conjunto de dados MNIST Configura\u00e7\u00e3o do dispositivo de processamento Criando um modelo deep learning Treinamento Executando a aplica\u00e7\u00e3o com cuda no supercomputador","title":"tch-rs-example MNIST"},{"location":"advanced/tch-rs_tutorial/#instalando-dependencias","text":"Primeiro instale o rust toolchain na sua pasta HOME, no n\u00f3 de login. $ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh segundo crie um novo projeto rust com cargo chamado pytorch-example. Para esse projeto Adicione as depend\u00eancias: anyhow , uma biblioteca para facilita o tratamento de erro em rust tch O framework Pytorch para cria\u00e7\u00e3o de modelos deep learning escrito em C++, mas com bindings para rust cargo new pytorch-example cd pytorch-example cargo add anyhow cargo add tch","title":"Instalando depend\u00eancias"},{"location":"advanced/tch-rs_tutorial/#criando-um-aplicacao-em-rust-para-treinar-uma-rede-neural","text":"A aplica\u00e7\u00e3o escrita em rust, dever\u00e1 selecionar qual dispositivo de processamento, CPU ou GPU, dever\u00e1 ser usado para a execu\u00e7\u00e3o dos c\u00e1lculos num\u00e9ricos. Carregar o conjunto de dados MNIST para ser computado em tal dispositivo. Criar um modelo deep learning e treinar o modelo. Portanto podemos entender a aplica\u00e7\u00e3o em 4 partes importantes: carregamento do conjunto de dados MNIST, configura\u00e7\u00e3o do dispositivo, criando um modelo deep learning, treinamento.","title":"Criando um aplica\u00e7\u00e3o em rust para treinar uma rede neural"},{"location":"advanced/tch-rs_tutorial/#carregamento-do-conjunto-de-dados-mnist","text":"Como o MNIST \u00e9 um conjunto de dados muito famoso, o pr\u00f3prio pytorch possui mecanismos de carreg\u00e1-lo, desde que voc\u00ea tenha ele baixado e descompactado. Para baixar o dataset, voc\u00ea pode criar um script similar ao get_inputs.sh . Onde basicamente ele cria o diret\u00f3rio chamado data e baixa os arquivos do dataset e os extrai com gunzip #!/bin/bash # get_inputs.sh mkdir data -p cd data wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz ; gunzip train-images-idx3-ubyte.gz wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz ; gunzip train-labels-idx1-ubyte.gz wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz ; gunzip t10k-images-idx3-ubyte.gz wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz ; gunzip t10k-labels-idx1-ubyte.gz Para carregar o dataset basta usar o a fun\u00e7\u00e3o tch::vision::mnist::load_dir let dataset = match tch :: vision :: mnist :: load_dir ( \"data\" ) { Dataset Ok ( d ) => d , Err ( _ ) => panic! ( \"Dataset Not found, run the get_inputs.sh !!\" ), };","title":"Carregamento do conjunto de dados MNIST"},{"location":"advanced/tch-rs_tutorial/#configuracao-do-dispositivo-de-processamento","text":"Utilizando um framework pytorch podemos selecionar o dispositivo da seguinte forma, se uma GPU estiver dispon\u00edvel, ent\u00e3o utilize GPU. Caso o contr\u00e1rio utilize CPU . Com o dispositivo podemos criar um conjunto de tensores cujo seus valores variam e que esses valores dever\u00e3o ser capazes de serem armazenados, ou seja criamos uma VarStore . let device = tch :: Device :: cuda_if_available (); let vs = tch :: nn :: VarStore :: new ( device . clone ()); Tamb\u00e9m precisamos transformar ou mover os dados do conjunto de dados para o dispositivo de processamento fn dataset_to_device ( dataset : Dataset , device : & Device ) -> Dataset { let train_labels = dataset . train_labels . to_device ( device . clone ()); let train_images = dataset . train_images . to_device ( device . clone ()); let test_labels = dataset . test_labels . to_device ( device . clone ()); let test_images = dataset . test_images . to_device ( device . clone ()); Dataset { test_images , test_labels , train_images , train_labels , labels : dataset . labels , } } let dataset = dataset_to_device ( dataset , & device );","title":"Configura\u00e7\u00e3o do dispositivo de processamento"},{"location":"advanced/tch-rs_tutorial/#criando-um-modelo-deep-learning","text":"O modelo deep learning utilizado nesse exemplo ser\u00e1 um modelo sequencial simples e com poucas camadas. Em rust o modelo ficou assim: use tch :: { nn , nn :: Module }; const IMAGE_DIM : i64 = 784 ; const HIDDEN_NODES : i64 = 128 ; const LABELS : i64 = 10 ; fn net ( vs : & nn :: Path ) -> impl Module { nn :: seq () . add ( nn :: linear ( vs / \"layer1\" , IMAGE_DIM , HIDDEN_NODES , Default :: default (), )) . add_fn ( | xs | xs . relu ()) . add ( nn :: linear ( vs , HIDDEN_NODES , LABELS , Default :: default ())) } let net = net ( & vs . root ()); Observe que vs (VarStore) \u00e9 passada na fun\u00e7\u00e3o net , de modo que criar uma camada, ou m\u00f3dulo (layer) \u00e9 alocar novos de tensores, para a vari\u00e1vel.","title":"Criando um modelo deep learning"},{"location":"advanced/tch-rs_tutorial/#treinamento","text":"Para treinar uma rede neural, pode-se utilizar v\u00e1rios algoritmos de otimiza\u00e7\u00e3o, por\u00e9m nesse exemplo foi utilizado o Adam , com uma taxa de aprendizado de 0.001 . O modelo ser\u00e1 treinado durante 200 intera\u00e7\u00f5es. Em rust a implementa\u00e7\u00e3o do treinamento fica da seguinte forma let mut opt = nn :: Adam :: default (). build ( & vs , 1e-3 ) ? ; for epoch in 1 .. 200 { let loss = net . forward ( & dataset . train_images ) . cross_entropy_for_logits ( & dataset . train_labels ); opt . backward_step ( & loss ); let test_accuracy = net . forward ( & dataset . test_images ) . accuracy_for_logits ( & dataset . test_labels ); println! ( \"epoch: {:4} train loss: {:8.5} test acc: {:5.2}% is cuda: {}\" , epoch , f64 :: try_from ( & loss ) ? , 100. * f64 :: try_from ( & test_accuracy ) ? , device . is_cuda (), ); } Voc\u00ea pode ver a implementa\u00e7\u00e3o completa em main.rs","title":"Treinamento"},{"location":"advanced/tch-rs_tutorial/#executando-a-aplicacao-com-cuda-no-supercomputador","text":"Atualmente o supercomputador no NPAD possui uma parti\u00e7\u00e3o chamada gpu , nessa parti\u00e7\u00e3o encontram-se os n\u00f3s com GPUs bem potentes capazes de executar a aplica\u00e7\u00e3o em 1 segundo. #!/bin/bash #SBATCH --job-name=neural_train #SBATCH --time=0-0:15 #SBATCH --partition=gpu #SBATCH --exclusive # informando ao tch-rs que desejo compilar com cuda na vers\u00e3o 11.7 export TORCH_CUDA_VERSION = cu117 cargo r --release Cargo \u00e9 o gerenciador de pacotes official da linguagem Rust, perceba que ao executar o comando cargo r --release . A aplica\u00e7\u00e3o cargo ir\u00e1 compilar a aplica\u00e7\u00e3o utilizando flags de otimiza\u00e7\u00e3o e ir\u00e1 executar o programa. Caso tenha compilado a aplica\u00e7\u00e3o no n\u00f3 de login, ser\u00e1 necess\u00e1rio remover a pasta target , antes de submeter o script # rm -rf target # caso tenha compilado a aplica\u00e7\u00e3o no n\u00f3 de login. sbatch run_on_superpc.sh o script run_on_superpc.sh pode ser encontrado aqui . Todo o projeto pode ser encontrado no github github.com/samuel-cavalcanti/tch-rs-example","title":"Executando a aplica\u00e7\u00e3o com cuda no supercomputador"},{"location":"beginner/gnome_files/","text":"Copiando arquivos atrav\u00e9s de uma interface gr\u00e1fica Gnome Files (linux) \u00b6 Gnome \u00e9 a interface padr\u00e3o do Ubuntu . Que \u00e9 distribui\u00e7\u00e3o padr\u00e3o para desenvolvimento do Instituto Metr\u00f3pole Digital (IMD). Portanto iremos mostrar como conectar o navegador de arquivos padr\u00e3o do Gnome, chamado Gnome files com os seus arquivos no supercomputador. A vers\u00e3o do Gnome files utilizada \u00e9 a 43.3 Clique em Other Locations \u00b6 Abra o gestor de arquivos Gnome files e clique em Other Locations como apontado na imagem Digite o endere\u00e7o do super pc \u00b6 No canto inferior direito, digite o endere\u00e7o do supercomputador. No caso o endere\u00e7o varia de acordo com o seu nome de usu\u00e1rio: ssh://nomeDoUsuario@sc2.npad.ufrn.br:4422 No entanto caso voc\u00ea tenha configurado o arquivo ~/.ssh/config o endere\u00e7o pode ser escrito da seguinte forma: ssh://super-pc Dica: Adicione nos favoritos \u00b6 Para n\u00e3o precisar ficar refazendo este tutorial, voc\u00ea pode salvar o o endere\u00e7o do supercomputador nos favoritos, clicando com bot\u00e3o direito do mouse e depois em add to Bookmarks","title":"Copiando arquivos atrav\u00e9s de uma interface gr\u00e1fica Gnome Files (linux)"},{"location":"beginner/gnome_files/#copiando-arquivos-atraves-de-uma-interface-grafica-gnome-files-linux","text":"Gnome \u00e9 a interface padr\u00e3o do Ubuntu . Que \u00e9 distribui\u00e7\u00e3o padr\u00e3o para desenvolvimento do Instituto Metr\u00f3pole Digital (IMD). Portanto iremos mostrar como conectar o navegador de arquivos padr\u00e3o do Gnome, chamado Gnome files com os seus arquivos no supercomputador. A vers\u00e3o do Gnome files utilizada \u00e9 a 43.3","title":"Copiando arquivos atrav\u00e9s de uma interface gr\u00e1fica Gnome Files (linux)"},{"location":"beginner/gnome_files/#clique-em-other-locations","text":"Abra o gestor de arquivos Gnome files e clique em Other Locations como apontado na imagem","title":"Clique em Other Locations"},{"location":"beginner/gnome_files/#digite-o-endereco-do-super-pc","text":"No canto inferior direito, digite o endere\u00e7o do supercomputador. No caso o endere\u00e7o varia de acordo com o seu nome de usu\u00e1rio: ssh://nomeDoUsuario@sc2.npad.ufrn.br:4422 No entanto caso voc\u00ea tenha configurado o arquivo ~/.ssh/config o endere\u00e7o pode ser escrito da seguinte forma: ssh://super-pc","title":"Digite o endere\u00e7o do super pc"},{"location":"beginner/gnome_files/#dica-adicione-nos-favoritos","text":"Para n\u00e3o precisar ficar refazendo este tutorial, voc\u00ea pode salvar o o endere\u00e7o do supercomputador nos favoritos, clicando com bot\u00e3o direito do mouse e depois em add to Bookmarks","title":"Dica: Adicione nos favoritos"},{"location":"beginner/mobaxterm_tutorial/","text":"Tutorial do MobaXterm \u00b6 A vers\u00e3o do MobaXterm utilizada neste tutorial \u00e9 a 23.1 Criando uma sess\u00e3o com NPAD \u00b6 Para deixar salvo uma sess\u00e3o com NPAD, voc\u00ea ir em: session -> ssh e preencha o formul\u00e1rio da seguinte maneira: Remote host coloque: sc2.npad.ufrn.br Port: 4422 Marque a caixa Specify username e coloque o seu usu\u00e1rio Clique em Advanced SSH settings e marque a caixa Use private key . Verifique se a chave privada est\u00e1 correta. Se voc\u00ea criou uma chave seguindo o nosso tutorial, o nome da chave \u00e9 id_rsa e est\u00e1 localizada em C:\\Users\\NomeDoSeuUsu\u00e1rio\\AppData\\Roaming\\MobaXterm\\home\\.ssh\\id_rsa .","title":"Tutorial do MobaXterm"},{"location":"beginner/mobaxterm_tutorial/#tutorial-do-mobaxterm","text":"A vers\u00e3o do MobaXterm utilizada neste tutorial \u00e9 a 23.1","title":"Tutorial do MobaXterm"},{"location":"beginner/mobaxterm_tutorial/#criando-uma-sessao-com-npad","text":"Para deixar salvo uma sess\u00e3o com NPAD, voc\u00ea ir em: session -> ssh e preencha o formul\u00e1rio da seguinte maneira: Remote host coloque: sc2.npad.ufrn.br Port: 4422 Marque a caixa Specify username e coloque o seu usu\u00e1rio Clique em Advanced SSH settings e marque a caixa Use private key . Verifique se a chave privada est\u00e1 correta. Se voc\u00ea criou uma chave seguindo o nosso tutorial, o nome da chave \u00e9 id_rsa e est\u00e1 localizada em C:\\Users\\NomeDoSeuUsu\u00e1rio\\AppData\\Roaming\\MobaXterm\\home\\.ssh\\id_rsa .","title":"Criando uma sess\u00e3o com NPAD"},{"location":"beginner/putty_tutorial/","text":"Tutorial do PuTTy \u00b6 Nesses tutoriais foi utilizado o PuTTy na sua vers\u00e3o 0.78 Gerando um par de chaves p\u00fablico/privada tipo RSA \u00b6 Procure nos seus aplicativos e abra o programa puttygen . Em Type of Key to generate selecione RSA . Clique em Generate . Fa\u00e7a movimentos aleat\u00f3rios com o mouse at\u00e9 preencher toda a barra verde. Quando isso acontecer ir\u00e1 aparecer a seguinte tela: Debaixo de Public key for pasting into OpenSSH authorized keys file: Encontra-se a chave que voc\u00ea deve copiar colocar criar uma conta no NPAD. Lembre-se de salvar duas chaves, a publica e a privada. Voc\u00ea deve salvar as chaves clicando em Save public-key e Save private-key . Se voc\u00ea n\u00e3o colocar uma passphrase ele ir\u00e1 lhe alerta isso mas n\u00e3o \u00e9 necess\u00e1rio colocar uma. LEMBRE-SE que se voc\u00ea colocar uma passphrase e esquece-la ou perder o par de chaves voc\u00ea ter\u00e1 que criar um novo par de chaves e adicionar uma nova chave . Acessando o supercomputador atrav\u00e9s do PuTTy \u00b6 Uma vez tendo feito o cadastro com a chave publica gerada anteriormente. \u00c9 necess\u00e1rio configurar o PuTTy para realizar o login no supercomputador. Tr\u00eas configura\u00e7\u00f5es precisam ser feitas. Adicionar a chave privada que voc\u00ea criou com PuTTYgen em Connection -> SSH -> Auth -> Credentials Adicionar o seu nome de usu\u00e1rio cadastrado no NPAD em Connection -> Data Adicionar o hostname: sc2.npad.ufrn.br e port: 4422 em Session Certifique-se que em Session , a caixa SSH em Connection type est\u00e1 marcada. Abaixo de Saved Sessions escreva um nome dessa sess\u00e3o, como por exemplo NPAD. Clique em Save para salvar todas as configura\u00e7\u00f5es feitas at\u00e9 o momento. Dessa forma sempre que quiser acessar o NPAD atrav\u00e9s do PuTTy \u00e9 s\u00f3 selecionar o nome da sess\u00e3o, clicar em Load e depois no bot\u00e3o Open .","title":"Tutorial do PuTTy"},{"location":"beginner/putty_tutorial/#tutorial-do-putty","text":"Nesses tutoriais foi utilizado o PuTTy na sua vers\u00e3o 0.78","title":"Tutorial do PuTTy"},{"location":"beginner/putty_tutorial/#gerando-um-par-de-chaves-publicoprivada-tipo-rsa","text":"Procure nos seus aplicativos e abra o programa puttygen . Em Type of Key to generate selecione RSA . Clique em Generate . Fa\u00e7a movimentos aleat\u00f3rios com o mouse at\u00e9 preencher toda a barra verde. Quando isso acontecer ir\u00e1 aparecer a seguinte tela: Debaixo de Public key for pasting into OpenSSH authorized keys file: Encontra-se a chave que voc\u00ea deve copiar colocar criar uma conta no NPAD. Lembre-se de salvar duas chaves, a publica e a privada. Voc\u00ea deve salvar as chaves clicando em Save public-key e Save private-key . Se voc\u00ea n\u00e3o colocar uma passphrase ele ir\u00e1 lhe alerta isso mas n\u00e3o \u00e9 necess\u00e1rio colocar uma. LEMBRE-SE que se voc\u00ea colocar uma passphrase e esquece-la ou perder o par de chaves voc\u00ea ter\u00e1 que criar um novo par de chaves e adicionar uma nova chave .","title":"Gerando um par de chaves p\u00fablico/privada tipo RSA"},{"location":"beginner/putty_tutorial/#acessando-o-supercomputador-atraves-do-putty","text":"Uma vez tendo feito o cadastro com a chave publica gerada anteriormente. \u00c9 necess\u00e1rio configurar o PuTTy para realizar o login no supercomputador. Tr\u00eas configura\u00e7\u00f5es precisam ser feitas. Adicionar a chave privada que voc\u00ea criou com PuTTYgen em Connection -> SSH -> Auth -> Credentials Adicionar o seu nome de usu\u00e1rio cadastrado no NPAD em Connection -> Data Adicionar o hostname: sc2.npad.ufrn.br e port: 4422 em Session Certifique-se que em Session , a caixa SSH em Connection type est\u00e1 marcada. Abaixo de Saved Sessions escreva um nome dessa sess\u00e3o, como por exemplo NPAD. Clique em Save para salvar todas as configura\u00e7\u00f5es feitas at\u00e9 o momento. Dessa forma sempre que quiser acessar o NPAD atrav\u00e9s do PuTTy \u00e9 s\u00f3 selecionar o nome da sess\u00e3o, clicar em Load e depois no bot\u00e3o Open .","title":"Acessando o supercomputador atrav\u00e9s do PuTTy"},{"location":"beginner/rsync_tutorial/","text":"Tutorial do rsync \u00b6 Outra aplica\u00e7\u00e3o que permite copiar arquivos \u00e9 o rsync . Em alguns sistemas eles n\u00e3o vem instalado por padr\u00e3o, certifique-se de que ele esteja instalado. Copiar um arquivo DO SEU COMPUTADOR para o supercomputador usando rsync \u00b6 Para copiar o arquivo meuArquivo DO SEU COMPUTADOR para o super computador usando o programa rsync . Abra um terminal Linux, use o seguinte comando: # caso tenha configurado o ~/.ssh/config rsync -aP ~/Downloads/meuArquivo super-pc:~/ # caso n\u00e3o tenha configurado rsync -aP ~/Downloads/meuArquivo --rsh = 'ssh -p4422' -aP nomeDoUsuario@sc2.npad.ufrn.br:~/ LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo: meuArquivo a ser\u00e1 copiado na pasta home do supercomputador. Perceba que o arquivo meuArquivo est\u00e1 localizado na pasta Downloads. Copiar um arquivo DO SUPERCOMPUTADOR para o seu computador usando rsync \u00b6 Para copiar o arquivo: meuArquivo DO SUPERCOMPUTADOR para o seu computador na pasta Downloads usando o programa rsync . Abra um terminal Linux, use o seguinte comando: # caso tenha configurado o ~/.ssh/config rsync super-pc:~/meuArquivo ~/Downloads # caso n\u00e3o tenha configurado rsync --rsh = 'ssh -p4422' -aP nomeDoUsuario@sc2.npad.ufrn.br:~/meuArquivo ~/Downloads LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador","title":"Tutorial do rsync"},{"location":"beginner/rsync_tutorial/#tutorial-do-rsync","text":"Outra aplica\u00e7\u00e3o que permite copiar arquivos \u00e9 o rsync . Em alguns sistemas eles n\u00e3o vem instalado por padr\u00e3o, certifique-se de que ele esteja instalado.","title":"Tutorial do rsync"},{"location":"beginner/rsync_tutorial/#copiar-um-arquivo-do-seu-computador-para-o-supercomputador-usando-rsync","text":"Para copiar o arquivo meuArquivo DO SEU COMPUTADOR para o super computador usando o programa rsync . Abra um terminal Linux, use o seguinte comando: # caso tenha configurado o ~/.ssh/config rsync -aP ~/Downloads/meuArquivo super-pc:~/ # caso n\u00e3o tenha configurado rsync -aP ~/Downloads/meuArquivo --rsh = 'ssh -p4422' -aP nomeDoUsuario@sc2.npad.ufrn.br:~/ LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo: meuArquivo a ser\u00e1 copiado na pasta home do supercomputador. Perceba que o arquivo meuArquivo est\u00e1 localizado na pasta Downloads.","title":"Copiar um arquivo DO SEU COMPUTADOR para o supercomputador usando rsync"},{"location":"beginner/rsync_tutorial/#copiar-um-arquivo-do-supercomputador-para-o-seu-computador-usando-rsync","text":"Para copiar o arquivo: meuArquivo DO SUPERCOMPUTADOR para o seu computador na pasta Downloads usando o programa rsync . Abra um terminal Linux, use o seguinte comando: # caso tenha configurado o ~/.ssh/config rsync super-pc:~/meuArquivo ~/Downloads # caso n\u00e3o tenha configurado rsync --rsh = 'ssh -p4422' -aP nomeDoUsuario@sc2.npad.ufrn.br:~/meuArquivo ~/Downloads LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador","title":"Copiar um arquivo DO SUPERCOMPUTADOR para o seu computador usando rsync"},{"location":"beginner/scp_tutorial/","text":"Tutorial do scp \u00b6 scp \u00e9 um aplica\u00e7\u00e3o que copia arquivos entre dois computadores pela internet atrav\u00e9s do ssh . \u00c9 poss\u00edvel utilizar o scp tanto no Linux quando no Windows. Copiar um arquivo DO SEU COMPUTADOR para o supercomputador usando scp \u00b6 Para copiar o arquivo meuArquivo DO SEU COMPUTADOR para o super computador usando o programa scp . Abra um terminal Linux, use o seguinte comando: # caso tenha configurado o ~/.ssh/config scp -r meuArquivo super-pc:~/ # caso n\u00e3o tenha configurado scp -r -P4422 meuArquivo nomeDoUsuario@sc2.npad.ufrn.br:~/ LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador. Copiar um arquivo DO SUPERCOMPUTADOR para o seu computador usando scp \u00b6 Para copiar o arquivo: meuArquivo DO SUPERCOMPUTADOR para o seu computador na pasta Downloads usando o programa scp . Abra um terminal Linux, use o seguinte comando: # caso tenha configurado o ~/.ssh/config scp -r super-pc:~/meuArquivo ~/Downloads # caso n\u00e3o tenha configurado scp -r -P4422 nomeDoUsuario@sc2.npad.ufrn.br:~/meuArquivo ~/Downloads LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador.","title":"Tutorial do scp"},{"location":"beginner/scp_tutorial/#tutorial-do-scp","text":"scp \u00e9 um aplica\u00e7\u00e3o que copia arquivos entre dois computadores pela internet atrav\u00e9s do ssh . \u00c9 poss\u00edvel utilizar o scp tanto no Linux quando no Windows.","title":"Tutorial do scp"},{"location":"beginner/scp_tutorial/#copiar-um-arquivo-do-seu-computador-para-o-supercomputador-usando-scp","text":"Para copiar o arquivo meuArquivo DO SEU COMPUTADOR para o super computador usando o programa scp . Abra um terminal Linux, use o seguinte comando: # caso tenha configurado o ~/.ssh/config scp -r meuArquivo super-pc:~/ # caso n\u00e3o tenha configurado scp -r -P4422 meuArquivo nomeDoUsuario@sc2.npad.ufrn.br:~/ LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador.","title":"Copiar um arquivo DO SEU COMPUTADOR para o supercomputador usando scp"},{"location":"beginner/scp_tutorial/#copiar-um-arquivo-do-supercomputador-para-o-seu-computador-usando-scp","text":"Para copiar o arquivo: meuArquivo DO SUPERCOMPUTADOR para o seu computador na pasta Downloads usando o programa scp . Abra um terminal Linux, use o seguinte comando: # caso tenha configurado o ~/.ssh/config scp -r super-pc:~/meuArquivo ~/Downloads # caso n\u00e3o tenha configurado scp -r -P4422 nomeDoUsuario@sc2.npad.ufrn.br:~/meuArquivo ~/Downloads LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador.","title":"Copiar um arquivo DO SUPERCOMPUTADOR para o seu computador usando scp"},{"location":"beginner/superpc_introduction_part_1/","text":"Introdu\u00e7\u00e3o ao supercomputador - Parte 1 \u00b6 O NPAD oferece como solu\u00e7\u00e3o um acesso a um supercomputador atrav\u00e9s de um terminal remoto. Para acessar o supercomputador \u00e9 necess\u00e1rio utilizar um programa chamado ssh . O ssh \u00e9 uma programa que permite fazer login em uma m\u00e1quina remotamente. Neste tutorial iremos aprender a gerar uma chave ssh, a acessar o supercomputador e a transferir arquivos para o supercomputador. Caso tenha alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato conosco atrav\u00e9s do e-mail atendimento\\ npad.ufrn.br (substituindo \\ por @). Introdu\u00e7\u00e3o ao supercomputador - Parte 1 Instale os pr\u00e9-requisitos Gerando uma chave SSH p\u00fablica Criando uma conta no NPAD Acessando o supercomputador Dica: Crie uma configura\u00e7\u00e3o para ssh Acessando arquivos do supercomputador Atrav\u00e9s de uma interface gr\u00e1fica Atrav\u00e9s do terminal Instale os pr\u00e9-requisitos \u00b6 Como dito anteriormente, acessar o supercomputador requer ssh. O ssh \u00e9 apenas uma das ferramentas do OpenSSH . Ent\u00e3o Caso voc\u00ea esteja utilizando Windows voc\u00ea pode seguir por tr\u00eas caminhos: Instalar OpenSSH e utilizar o Windows PowerShell como terminal (Recomendado) Instalar o MobaXterm Home Edition que j\u00e1 vem com OpenSSH Instalar o PuTTY que possui o seu pr\u00f3prio cliente ssh Se voc\u00ea utiliza Linux procure por openssh nos reposit\u00f3rios oficiais. No caso do ubuntu para instalar o openssh \u00e9: sudo apt install openssh-client Recomendamos utilizar o cliente ssh e terminal oficiais do sistema operacional que estiver utilizando. Gerando uma chave SSH p\u00fablica \u00b6 Para criar uma conta no supercomputador, ou tenha perdido a chave p\u00fablica. Ser\u00e1 necess\u00e1rio inserir uma nova chave p\u00fablica do tipo rsa . Caso esteja utilizando o PuTTy veja esse tutorial: PuTTy Tutoriais: Gerando um par de chaves publico privada tipo RSA . Para as demais op\u00e7\u00f5es \u00e9 necess\u00e1rio voc\u00ea abrir um terminal. Windows + MobaXterm: abra a aplica\u00e7\u00e3o e clique no bot\u00e3o start local terminal Windows com Windows PowerShell: procure o programa Windows PowerShell e abra-o Ubuntu: procure por gnome-terminal ou gnome-console ou aperte Ctrl + Alt + T Para gerar sua chave ssh do tipo rsa , e digite o comando a seguir: ssh-keygen -t rsa Ir\u00e1 ser realizado uma sequ\u00eancia de perguntas, apenas pressione enter em todas elas. Para visualizar sua chave p\u00fablica, digite o comando a seguir: cat .ssh/id_rsa.pub Voc\u00ea precisar\u00e1 copiar e colar essa chave p\u00fablica na hora de criar uma conta ou adicionar outra chave. Com uma chave p\u00fablica voc\u00ea est\u00e1 pronto para criar uma conta no NPAD. Perceba que voc\u00ea criou uma chave privada em .ssh/id_rsa e uma chave p\u00fablica .ssh/id_rsa.pub . Criando uma conta no NPAD \u00b6 Para utilizar o supercomputador \u00e9 necess\u00e1rio criar uma conta na nossa P\u00e1gina de Cadastro . Para realizar o cadastro, verifique qual o seu enquadramento na nossa Pol\u00edtica de Acesso para saber qual o Tipo de Usu\u00e1rio da sua conta. Na P\u00e1gina de Primeiros Passos voc\u00ea obter\u00e1 informa\u00e7\u00f5es sobre o primeiro acesso. Depois de fazer o cadastro no site do NPAD, voc\u00ea receber\u00e1 um e-mail confirmando sua inscri\u00e7\u00e3o. Ap\u00f3s receber o e-mail, voc\u00ea poder\u00e1 acessar o supercomputador do computador que gerou o par de chaves ssh observando as orienta\u00e7\u00f5es presentes neste tutorial. Acessando o supercomputador \u00b6 Uma vez que tenha cadastro no NPAD, voc\u00ea pode acessar o supercomputador de duas formas: Usando a aplica\u00e7\u00e3o PuTTy . Caso deseja usar o PuTTy veja o tutorial do PuTTy . atrav\u00e9s de um terminal como: Windows PowerShell , MobaXterm , usando o comando ssh Caso deseja usar o ssh, ent\u00e3o dentro do terminal digite o comando: ssh -p4422 nomeDoUsuario@sc2.npad.ufrn.br substituindo o termo nomeDoUsuario pelo nome de usu\u00e1rio criado. Caso tenha feito tudo corretamente ser\u00e1 apresentada a tela inicial do supercomputador: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 NPAD \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 N\u00facleo de Processamento de Alto Desempenho - Universidade Federal do Rio Grande do \u2502 \u2502 Norte \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ALERTA \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 O uso deste sistema \u00e9 restrito apenas a usu\u00e1rios autorizados. Voc\u00ea deve possuir \u2502 \u2502 permiss\u00f5es expl\u00edcitas para acessar ou configurar este servidor. Todas as atividades \u2502 \u2502 realizadas neste dispositivo est\u00e3o sujeitas a monitoramento e grava\u00e7\u00e3o e poder\u00e3o ser \u2502 \u2502 devidamente reportadas em caso de uso ilegal. Obrigado. \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 FAQ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 D\u00favidas? Veja nosso FAQ em http://npad.ufrn.br/faq.php \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 USO \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 Uso atual dos n\u00f3s do sistema \u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502 \u2503 Parti\u00e7\u00e3o \u2503 Idle \u2503 Mixed \u2503 Alloc \u2503 Maint/Down \u2503 \u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502 \u2502 cluster \u2502 10 \u2502 9 \u2502 31 \u2502 1 \u2502 \u2502 \u2502 \u2502 service \u2502 3 \u2502 0 \u2502 1 \u2502 0 \u2502 \u2502 \u2502 \u2502 test \u2502 13 \u2502 9 \u2502 32 \u2502 1 \u2502 \u2502 \u2502 \u2502 intel-512 \u2502 0 \u2502 0 \u2502 6 \u2502 0 \u2502 \u2502 \u2502 \u2502 intel-256 \u2502 3 \u2502 3 \u2502 0 \u2502 1 \u2502 \u2502 \u2502 \u2502 gpu \u2502 2 \u2502 0 \u2502 0 \u2502 0 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 QUOTA \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 Voc\u00ea utilizou 81 .5 % de sua quota de disco. \u2502 \u2502 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 NOT\u00cdCIAS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 | ........... | \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f [ nomeDoUsuario@service0 ~ ] $ Dica: Crie uma configura\u00e7\u00e3o para ssh \u00b6 Se voc\u00ea criar ou adicionar a seguinte configura\u00e7\u00e3o no arquivo ~/.ssh/config : Host super-pc HostName sc2.npad.ufrn.br Port 4422 User nomeDoUsuario trocando o nomeDoUsuario pelo nome do seu usu\u00e1rio, voc\u00ea poder\u00e1 acessar o supercomputador usando o comando: ssh super-pc Caso voc\u00ea esteja usando MobaXterm, voc\u00ea pode criar uma nova sess\u00e3o para facilitar o acesso ao supercomputador: MobaXterm tutoriais: criando uma sess\u00e3o com NPAD Acessando arquivos do supercomputador \u00b6 O OpenSSH al\u00e9m de permitir fazer login em uma m\u00e1quina remotamente, tamb\u00e9m permite a transfer\u00eancia de arquivos por aplica\u00e7\u00f5es de linha de comando quanto por software de interface gr\u00e1fica de terceiros. Atrav\u00e9s de uma interface gr\u00e1fica \u00b6 Em muitos casos \u00e9 simplesmente mais pr\u00e1tico acessar, copiar e mover arquivos do seu computador para o supercomputador atrav\u00e9s de uma interface gr\u00e1fica. Para isso voc\u00ea pode utilizar o WinSCP no caso do Windows ou configurar o pr\u00f3prio navegador de arquivos do ubuntu: Gnome Files para essa tarefa. Foi feito dois tutoriais: Copiando Arquivos atrav\u00e9s de uma Interface gr\u00e1fica WinSCP (Windows) Copiando Arquivos atrav\u00e9s de uma Interface gr\u00e1fica Gnome Files (linux) Atrav\u00e9s do terminal \u00b6 \u00c9 poss\u00edvel transferir arquivos atrav\u00e9s das aplica\u00e7\u00f5es de linhas de comando como: scp e rsync . Sendo o rsync apenas para linux e scp funciona no Windows apenas se voc\u00ea instalar o OpenSSH . Para usar como usar o scp veja o tutorial: scp e para o rsync veja o tutorial: rsync .","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte&nbsp;1"},{"location":"beginner/superpc_introduction_part_1/#introducao-ao-supercomputador-parte1","text":"O NPAD oferece como solu\u00e7\u00e3o um acesso a um supercomputador atrav\u00e9s de um terminal remoto. Para acessar o supercomputador \u00e9 necess\u00e1rio utilizar um programa chamado ssh . O ssh \u00e9 uma programa que permite fazer login em uma m\u00e1quina remotamente. Neste tutorial iremos aprender a gerar uma chave ssh, a acessar o supercomputador e a transferir arquivos para o supercomputador. Caso tenha alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato conosco atrav\u00e9s do e-mail atendimento\\ npad.ufrn.br (substituindo \\ por @). Introdu\u00e7\u00e3o ao supercomputador - Parte 1 Instale os pr\u00e9-requisitos Gerando uma chave SSH p\u00fablica Criando uma conta no NPAD Acessando o supercomputador Dica: Crie uma configura\u00e7\u00e3o para ssh Acessando arquivos do supercomputador Atrav\u00e9s de uma interface gr\u00e1fica Atrav\u00e9s do terminal","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte&nbsp;1"},{"location":"beginner/superpc_introduction_part_1/#instale-os-pre-requisitos","text":"Como dito anteriormente, acessar o supercomputador requer ssh. O ssh \u00e9 apenas uma das ferramentas do OpenSSH . Ent\u00e3o Caso voc\u00ea esteja utilizando Windows voc\u00ea pode seguir por tr\u00eas caminhos: Instalar OpenSSH e utilizar o Windows PowerShell como terminal (Recomendado) Instalar o MobaXterm Home Edition que j\u00e1 vem com OpenSSH Instalar o PuTTY que possui o seu pr\u00f3prio cliente ssh Se voc\u00ea utiliza Linux procure por openssh nos reposit\u00f3rios oficiais. No caso do ubuntu para instalar o openssh \u00e9: sudo apt install openssh-client Recomendamos utilizar o cliente ssh e terminal oficiais do sistema operacional que estiver utilizando.","title":"Instale os pr\u00e9-requisitos"},{"location":"beginner/superpc_introduction_part_1/#gerando-uma-chave-ssh-publica","text":"Para criar uma conta no supercomputador, ou tenha perdido a chave p\u00fablica. Ser\u00e1 necess\u00e1rio inserir uma nova chave p\u00fablica do tipo rsa . Caso esteja utilizando o PuTTy veja esse tutorial: PuTTy Tutoriais: Gerando um par de chaves publico privada tipo RSA . Para as demais op\u00e7\u00f5es \u00e9 necess\u00e1rio voc\u00ea abrir um terminal. Windows + MobaXterm: abra a aplica\u00e7\u00e3o e clique no bot\u00e3o start local terminal Windows com Windows PowerShell: procure o programa Windows PowerShell e abra-o Ubuntu: procure por gnome-terminal ou gnome-console ou aperte Ctrl + Alt + T Para gerar sua chave ssh do tipo rsa , e digite o comando a seguir: ssh-keygen -t rsa Ir\u00e1 ser realizado uma sequ\u00eancia de perguntas, apenas pressione enter em todas elas. Para visualizar sua chave p\u00fablica, digite o comando a seguir: cat .ssh/id_rsa.pub Voc\u00ea precisar\u00e1 copiar e colar essa chave p\u00fablica na hora de criar uma conta ou adicionar outra chave. Com uma chave p\u00fablica voc\u00ea est\u00e1 pronto para criar uma conta no NPAD. Perceba que voc\u00ea criou uma chave privada em .ssh/id_rsa e uma chave p\u00fablica .ssh/id_rsa.pub .","title":"Gerando uma chave SSH p\u00fablica"},{"location":"beginner/superpc_introduction_part_1/#criando-uma-conta-no-npad","text":"Para utilizar o supercomputador \u00e9 necess\u00e1rio criar uma conta na nossa P\u00e1gina de Cadastro . Para realizar o cadastro, verifique qual o seu enquadramento na nossa Pol\u00edtica de Acesso para saber qual o Tipo de Usu\u00e1rio da sua conta. Na P\u00e1gina de Primeiros Passos voc\u00ea obter\u00e1 informa\u00e7\u00f5es sobre o primeiro acesso. Depois de fazer o cadastro no site do NPAD, voc\u00ea receber\u00e1 um e-mail confirmando sua inscri\u00e7\u00e3o. Ap\u00f3s receber o e-mail, voc\u00ea poder\u00e1 acessar o supercomputador do computador que gerou o par de chaves ssh observando as orienta\u00e7\u00f5es presentes neste tutorial.","title":"Criando uma conta no NPAD"},{"location":"beginner/superpc_introduction_part_1/#acessando-o-supercomputador","text":"Uma vez que tenha cadastro no NPAD, voc\u00ea pode acessar o supercomputador de duas formas: Usando a aplica\u00e7\u00e3o PuTTy . Caso deseja usar o PuTTy veja o tutorial do PuTTy . atrav\u00e9s de um terminal como: Windows PowerShell , MobaXterm , usando o comando ssh Caso deseja usar o ssh, ent\u00e3o dentro do terminal digite o comando: ssh -p4422 nomeDoUsuario@sc2.npad.ufrn.br substituindo o termo nomeDoUsuario pelo nome de usu\u00e1rio criado. Caso tenha feito tudo corretamente ser\u00e1 apresentada a tela inicial do supercomputador: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 NPAD \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 N\u00facleo de Processamento de Alto Desempenho - Universidade Federal do Rio Grande do \u2502 \u2502 Norte \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ALERTA \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 O uso deste sistema \u00e9 restrito apenas a usu\u00e1rios autorizados. Voc\u00ea deve possuir \u2502 \u2502 permiss\u00f5es expl\u00edcitas para acessar ou configurar este servidor. Todas as atividades \u2502 \u2502 realizadas neste dispositivo est\u00e3o sujeitas a monitoramento e grava\u00e7\u00e3o e poder\u00e3o ser \u2502 \u2502 devidamente reportadas em caso de uso ilegal. Obrigado. \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 FAQ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 D\u00favidas? Veja nosso FAQ em http://npad.ufrn.br/faq.php \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 USO \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 Uso atual dos n\u00f3s do sistema \u2502 \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2502 \u2502 \u2503 Parti\u00e7\u00e3o \u2503 Idle \u2503 Mixed \u2503 Alloc \u2503 Maint/Down \u2503 \u2502 \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 \u2502 \u2502 cluster \u2502 10 \u2502 9 \u2502 31 \u2502 1 \u2502 \u2502 \u2502 \u2502 service \u2502 3 \u2502 0 \u2502 1 \u2502 0 \u2502 \u2502 \u2502 \u2502 test \u2502 13 \u2502 9 \u2502 32 \u2502 1 \u2502 \u2502 \u2502 \u2502 intel-512 \u2502 0 \u2502 0 \u2502 6 \u2502 0 \u2502 \u2502 \u2502 \u2502 intel-256 \u2502 3 \u2502 3 \u2502 0 \u2502 1 \u2502 \u2502 \u2502 \u2502 gpu \u2502 2 \u2502 0 \u2502 0 \u2502 0 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 QUOTA \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 Voc\u00ea utilizou 81 .5 % de sua quota de disco. \u2502 \u2502 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 NOT\u00cdCIAS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 | ........... | \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f [ nomeDoUsuario@service0 ~ ] $","title":"Acessando o supercomputador"},{"location":"beginner/superpc_introduction_part_1/#dica-crie-uma-configuracao-para-ssh","text":"Se voc\u00ea criar ou adicionar a seguinte configura\u00e7\u00e3o no arquivo ~/.ssh/config : Host super-pc HostName sc2.npad.ufrn.br Port 4422 User nomeDoUsuario trocando o nomeDoUsuario pelo nome do seu usu\u00e1rio, voc\u00ea poder\u00e1 acessar o supercomputador usando o comando: ssh super-pc Caso voc\u00ea esteja usando MobaXterm, voc\u00ea pode criar uma nova sess\u00e3o para facilitar o acesso ao supercomputador: MobaXterm tutoriais: criando uma sess\u00e3o com NPAD","title":"Dica: Crie uma configura\u00e7\u00e3o para ssh"},{"location":"beginner/superpc_introduction_part_1/#acessando-arquivos-do-supercomputador","text":"O OpenSSH al\u00e9m de permitir fazer login em uma m\u00e1quina remotamente, tamb\u00e9m permite a transfer\u00eancia de arquivos por aplica\u00e7\u00f5es de linha de comando quanto por software de interface gr\u00e1fica de terceiros.","title":"Acessando arquivos do supercomputador"},{"location":"beginner/superpc_introduction_part_1/#atraves-de-uma-interface-grafica","text":"Em muitos casos \u00e9 simplesmente mais pr\u00e1tico acessar, copiar e mover arquivos do seu computador para o supercomputador atrav\u00e9s de uma interface gr\u00e1fica. Para isso voc\u00ea pode utilizar o WinSCP no caso do Windows ou configurar o pr\u00f3prio navegador de arquivos do ubuntu: Gnome Files para essa tarefa. Foi feito dois tutoriais: Copiando Arquivos atrav\u00e9s de uma Interface gr\u00e1fica WinSCP (Windows) Copiando Arquivos atrav\u00e9s de uma Interface gr\u00e1fica Gnome Files (linux)","title":"Atrav\u00e9s de uma interface gr\u00e1fica"},{"location":"beginner/superpc_introduction_part_1/#atraves-do-terminal","text":"\u00c9 poss\u00edvel transferir arquivos atrav\u00e9s das aplica\u00e7\u00f5es de linhas de comando como: scp e rsync . Sendo o rsync apenas para linux e scp funciona no Windows apenas se voc\u00ea instalar o OpenSSH . Para usar como usar o scp veja o tutorial: scp e para o rsync veja o tutorial: rsync .","title":"Atrav\u00e9s do terminal"},{"location":"beginner/superpc_introduction_part_2/","text":"Introdu\u00e7\u00e3o ao supercomputador - Parte 2 \u00b6 Nesse tutorial iremos aprender a criar um script e executar um programa utilizando o supercomputador. Caso surja alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato conosco atrav\u00e9s do e-mail atendimento\\ npad.ufrn.br (substituindo \\ por @ ). Introdu\u00e7\u00e3o ao supercomputador - Parte 2 Criando um script para um programa j\u00e1 existente no supercomputador Executando um programa j\u00e1 existente no supercomputador Execu\u00e7\u00e3o de programas no n\u00f3 de login Execu\u00e7\u00e3o de programas no n\u00f3 de computa\u00e7\u00e3o Verifica\u00e7\u00e3o da execu\u00e7\u00e3o Criando um script para executar um programa criado a partir do c\u00f3digo-fonte Executando um programa criado a partir do c\u00f3digo-fonte Criando um script para um programa j\u00e1 existente no supercomputador \u00b6 Para criar um script para executar programas no supercomputador \u00e9 necess\u00e1rio informar o nome do programa que ir\u00e1 ser executado e o tempo m\u00e1ximo de sua execu\u00e7\u00e3o (saiba como escolher o tempo de execu\u00e7\u00e3o aqui ). O tempo m\u00e1ximo depende do tipo de n\u00f3 escolhido. Acesse o supercomputador e siga as orienta\u00e7\u00f5es a seguir. Exemplo: cria\u00e7\u00e3o de um script para executar o programa factor. No terminal, entre com o comando abaixo no terminal do Linux, seguido de enter. nano nomeScript # Substitua nomeScript pelo nome que voc\u00ea deseja dar ao seu script Ap\u00f3s abrir o programa nano , execute os comandos abaixo no terminal do Linux: #!/bin/bash #SBATCH --time=0-0:5 # Especifica o tempo m\u00e1ximo de execu\u00e7\u00e3o do job, dado no padr\u00e3o dias-horas:minutos factor 120 # Especifica o programa a ser executado (no caso, factor) e o par\u00e2metro de entrada (120) Em sequ\u00eancia aperte ctrl+o, depois enter e, ent\u00e3o, aperte ctrl+x para sair do nano . Vale ressaltar que tamb\u00e9m \u00e9 poss\u00edvel abrir o arquivo correspondente ao script e fazer altera\u00e7\u00f5es diretamente nele, via interface gr\u00e1fica . Executando um programa j\u00e1 existente no supercomputador \u00b6 Para executar um programa no supercomputador, voc\u00ea precisar\u00e1 criar um script, conforme descrito anteriormente, e escolher em qual tipo de n\u00f3 o programa vai ser executado: Execu\u00e7\u00e3o de programas no n\u00f3 de login \u00b6 O n\u00f3 de login \u00e9 mais indicado para execu\u00e7\u00e3o de softwares de teste, ou seja, que requerem um tempo de execu\u00e7\u00e3o menor. O tempo limite para execu\u00e7\u00e3o neste n\u00f3 \u00e9 de 30 minutos, considerando que o usu\u00e1rio est\u00e1 utilizando apenas um n\u00facleo e 100% de sua capacidade. Um usu\u00e1rio que decide utilizar esse n\u00f3 para execu\u00e7\u00e3o de um processo que ocupa 100% de 10 CPUs, por exemplo, ser\u00e1 terminado automaticamente ap\u00f3s 3 minutos de uso, como pode ser visto na tabela abaixo ( Figura 1 ), junto com demais exemplos. Isso ocorre mesmo que o script tenha tempo maior configurado. Desse modo, recomenda-se utilizar o n\u00f3 de login apenas para fins de teste. Figura 1 - Tabela com exemplos de tempo de execu\u00e7\u00e3o de programas no n\u00f3 de login, para v\u00e1rias configura\u00e7\u00f5es Para executar um programa no supercomputador no n\u00f3 de login, use o comando a seguir no terminal do Linux: ./meuPrimeiroScript Onde meuPrimeiroScript \u00e9 o script com os dados de execu\u00e7\u00e3o. Execu\u00e7\u00e3o de programas no n\u00f3 de computa\u00e7\u00e3o \u00b6 O N\u00f3 de Computa\u00e7\u00e3o, por sua vez, \u00e9 o mais indicado para uso de softwares gerais, que necessitem de um tempo de execu\u00e7\u00e3o elevado. \u00c9 nele que voc\u00ea ir\u00e1 utilizar a maioria dos programas dispon\u00edveis no supercomputador, podendo deix\u00e1-los em execu\u00e7\u00e3o por longos per\u00edodos. A seguir, voc\u00ea ir\u00e1 aprender como criar scripts e executar programas no n\u00f3 de computa\u00e7\u00e3o. Para executar um programa no supercomputador no n\u00f3 de computa\u00e7\u00e3o, use o comando a seguir no terminal do Linux: sbatch meuPrimeiroScript Onde meuPrimeiroScript \u00e9 o script com os dados de execu\u00e7\u00e3o. Verifica\u00e7\u00e3o da execu\u00e7\u00e3o \u00b6 Caso o job tenha sido submetido corretamente, aparecer\u00e1 a mensagem: Submitted batch job JobID Onde JobID ser\u00e1 substitu\u00eddo pelo n\u00famero que identificar\u00e1 o job. A sa\u00edda do programa n\u00e3o aparecer\u00e1 na tela. Ela ser\u00e1 escrita em um arquivo de nome slurm-JobID.out , onde JobID ser\u00e1 o mesmo valor do id do job submetido. Para ver a sa\u00edda do programa, use o comando: cat slurm-JobID.out Substituindo JobID pelo id do job que voc\u00ea deseja ver a sa\u00edda. [ nomeDoUsuario@service0 ~ ] $ sbatch meuPrimeiroScript Submitted batch job 14518 [ nomeDoUsuario@service0 ~ ] $ cat slurm-14518.out 120 : 2 2 2 3 5 A execu\u00e7\u00e3o de um programa no supercomputador est\u00e1 sujeita \u00e0 disponibilidade de n\u00f3s, desse modo, o resultado pode demorar. Para acompanhar o andamento da fila, bem como a prioridade do job submetido por voc\u00ea na mesma, a se\u00e7\u00e3o de Comandos pode ser bastante \u00fatil. Criando um script para executar um programa criado a partir do c\u00f3digo-fonte \u00b6 Caso o programa tenha sido criado a partir do c\u00f3dido-fonte deve-se acrescentar ao nome do programa a pasta onde este se encontra. Exemplo: #!/bin/bash #SBATCH --time=0-0:5 #especifica o tempo m\u00e1ximo de execu\u00e7\u00e3o do job, dado no padr\u00e3o dias-horas:minutos ./helloWorld #o ponto e a barra indicam o caminho at\u00e9 a pasta atual. Saiba como escolher o tempo de execu\u00e7\u00e3o aqui . Executando um programa criado a partir do c\u00f3digo-fonte \u00b6 Para executar um programa a partir do c\u00f3digo-fonte o usu\u00e1rio dever\u00e1, em sua pasta no supercomputador, gerar o programa execut\u00e1vel. Para criar o execut\u00e1vel do programa digite: gcc helloWorld.c -o helloWorld Onde helloWorld.c \u00e9 o c\u00f3digo-fonte e helloWorld \u00e9 o c\u00f3digo execut\u00e1vel. Para executar o programa gerado, voc\u00ea dever\u00e1 executar o script gerado anteriormente utilizando o comando sbatch jobHelloWorld ou sbatch -p test jobHelloWorld de acordo com o tipo de n\u00f3 desejado. Caso o job tenha sido submetido corretamente, aparecer\u00e1 a mensagem: Submitted batch job JobID Onde JobID ser\u00e1 substitu\u00eddo pelo n\u00famero que identificar\u00e1 o job. A sa\u00edda do programa n\u00e3o aparecer\u00e1 na tela. Ela ser\u00e1 escrita em um arquivo de nome slurm-JobID.out , onde JobID ser\u00e1 o mesmo valor do id do job submetido. Para ver a sa\u00edda do programa, use o comando: cat slurm-JobID.out Substituindo JobID pelo id do job que voc\u00ea deseja ver a sa\u00edda. [ nomeDoUsuario@service0 ~ ] $ gcc helloWorld.c -o helloWorld [ nomeDoUsuario@service0 ~ ] $ sbatch jobHelloWorld Submitted batch job 14520 [ nomeDoUsuario@service0 ~ ] $ cat slurm-14520.out Hello World! Para executar programas em paralelo no supercomputador, leia os tutorias de OpenMP e MPI .","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte&nbsp;2"},{"location":"beginner/superpc_introduction_part_2/#introducao-ao-supercomputador-parte2","text":"Nesse tutorial iremos aprender a criar um script e executar um programa utilizando o supercomputador. Caso surja alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato conosco atrav\u00e9s do e-mail atendimento\\ npad.ufrn.br (substituindo \\ por @ ). Introdu\u00e7\u00e3o ao supercomputador - Parte 2 Criando um script para um programa j\u00e1 existente no supercomputador Executando um programa j\u00e1 existente no supercomputador Execu\u00e7\u00e3o de programas no n\u00f3 de login Execu\u00e7\u00e3o de programas no n\u00f3 de computa\u00e7\u00e3o Verifica\u00e7\u00e3o da execu\u00e7\u00e3o Criando um script para executar um programa criado a partir do c\u00f3digo-fonte Executando um programa criado a partir do c\u00f3digo-fonte","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte&nbsp;2"},{"location":"beginner/superpc_introduction_part_2/#criando-um-script-para-um-programa-ja-existente-no-supercomputador","text":"Para criar um script para executar programas no supercomputador \u00e9 necess\u00e1rio informar o nome do programa que ir\u00e1 ser executado e o tempo m\u00e1ximo de sua execu\u00e7\u00e3o (saiba como escolher o tempo de execu\u00e7\u00e3o aqui ). O tempo m\u00e1ximo depende do tipo de n\u00f3 escolhido. Acesse o supercomputador e siga as orienta\u00e7\u00f5es a seguir. Exemplo: cria\u00e7\u00e3o de um script para executar o programa factor. No terminal, entre com o comando abaixo no terminal do Linux, seguido de enter. nano nomeScript # Substitua nomeScript pelo nome que voc\u00ea deseja dar ao seu script Ap\u00f3s abrir o programa nano , execute os comandos abaixo no terminal do Linux: #!/bin/bash #SBATCH --time=0-0:5 # Especifica o tempo m\u00e1ximo de execu\u00e7\u00e3o do job, dado no padr\u00e3o dias-horas:minutos factor 120 # Especifica o programa a ser executado (no caso, factor) e o par\u00e2metro de entrada (120) Em sequ\u00eancia aperte ctrl+o, depois enter e, ent\u00e3o, aperte ctrl+x para sair do nano . Vale ressaltar que tamb\u00e9m \u00e9 poss\u00edvel abrir o arquivo correspondente ao script e fazer altera\u00e7\u00f5es diretamente nele, via interface gr\u00e1fica .","title":"Criando um script para um programa j\u00e1 existente no supercomputador"},{"location":"beginner/superpc_introduction_part_2/#executando-um-programa-ja-existente-no-supercomputador","text":"Para executar um programa no supercomputador, voc\u00ea precisar\u00e1 criar um script, conforme descrito anteriormente, e escolher em qual tipo de n\u00f3 o programa vai ser executado:","title":"Executando um programa j\u00e1 existente no supercomputador"},{"location":"beginner/superpc_introduction_part_2/#execucao-de-programas-no-no-de-login","text":"O n\u00f3 de login \u00e9 mais indicado para execu\u00e7\u00e3o de softwares de teste, ou seja, que requerem um tempo de execu\u00e7\u00e3o menor. O tempo limite para execu\u00e7\u00e3o neste n\u00f3 \u00e9 de 30 minutos, considerando que o usu\u00e1rio est\u00e1 utilizando apenas um n\u00facleo e 100% de sua capacidade. Um usu\u00e1rio que decide utilizar esse n\u00f3 para execu\u00e7\u00e3o de um processo que ocupa 100% de 10 CPUs, por exemplo, ser\u00e1 terminado automaticamente ap\u00f3s 3 minutos de uso, como pode ser visto na tabela abaixo ( Figura 1 ), junto com demais exemplos. Isso ocorre mesmo que o script tenha tempo maior configurado. Desse modo, recomenda-se utilizar o n\u00f3 de login apenas para fins de teste. Figura 1 - Tabela com exemplos de tempo de execu\u00e7\u00e3o de programas no n\u00f3 de login, para v\u00e1rias configura\u00e7\u00f5es Para executar um programa no supercomputador no n\u00f3 de login, use o comando a seguir no terminal do Linux: ./meuPrimeiroScript Onde meuPrimeiroScript \u00e9 o script com os dados de execu\u00e7\u00e3o.","title":"Execu\u00e7\u00e3o de programas no n\u00f3 de login"},{"location":"beginner/superpc_introduction_part_2/#execucao-de-programas-no-no-de-computacao","text":"O N\u00f3 de Computa\u00e7\u00e3o, por sua vez, \u00e9 o mais indicado para uso de softwares gerais, que necessitem de um tempo de execu\u00e7\u00e3o elevado. \u00c9 nele que voc\u00ea ir\u00e1 utilizar a maioria dos programas dispon\u00edveis no supercomputador, podendo deix\u00e1-los em execu\u00e7\u00e3o por longos per\u00edodos. A seguir, voc\u00ea ir\u00e1 aprender como criar scripts e executar programas no n\u00f3 de computa\u00e7\u00e3o. Para executar um programa no supercomputador no n\u00f3 de computa\u00e7\u00e3o, use o comando a seguir no terminal do Linux: sbatch meuPrimeiroScript Onde meuPrimeiroScript \u00e9 o script com os dados de execu\u00e7\u00e3o.","title":"Execu\u00e7\u00e3o de programas no n\u00f3 de computa\u00e7\u00e3o"},{"location":"beginner/superpc_introduction_part_2/#verificacao-da-execucao","text":"Caso o job tenha sido submetido corretamente, aparecer\u00e1 a mensagem: Submitted batch job JobID Onde JobID ser\u00e1 substitu\u00eddo pelo n\u00famero que identificar\u00e1 o job. A sa\u00edda do programa n\u00e3o aparecer\u00e1 na tela. Ela ser\u00e1 escrita em um arquivo de nome slurm-JobID.out , onde JobID ser\u00e1 o mesmo valor do id do job submetido. Para ver a sa\u00edda do programa, use o comando: cat slurm-JobID.out Substituindo JobID pelo id do job que voc\u00ea deseja ver a sa\u00edda. [ nomeDoUsuario@service0 ~ ] $ sbatch meuPrimeiroScript Submitted batch job 14518 [ nomeDoUsuario@service0 ~ ] $ cat slurm-14518.out 120 : 2 2 2 3 5 A execu\u00e7\u00e3o de um programa no supercomputador est\u00e1 sujeita \u00e0 disponibilidade de n\u00f3s, desse modo, o resultado pode demorar. Para acompanhar o andamento da fila, bem como a prioridade do job submetido por voc\u00ea na mesma, a se\u00e7\u00e3o de Comandos pode ser bastante \u00fatil.","title":"Verifica\u00e7\u00e3o da execu\u00e7\u00e3o"},{"location":"beginner/superpc_introduction_part_2/#criando-um-script-para-executar-um-programa-criado-a-partir-do-codigo-fonte","text":"Caso o programa tenha sido criado a partir do c\u00f3dido-fonte deve-se acrescentar ao nome do programa a pasta onde este se encontra. Exemplo: #!/bin/bash #SBATCH --time=0-0:5 #especifica o tempo m\u00e1ximo de execu\u00e7\u00e3o do job, dado no padr\u00e3o dias-horas:minutos ./helloWorld #o ponto e a barra indicam o caminho at\u00e9 a pasta atual. Saiba como escolher o tempo de execu\u00e7\u00e3o aqui .","title":"Criando um script para executar um programa criado a partir do c\u00f3digo-fonte"},{"location":"beginner/superpc_introduction_part_2/#executando-um-programa-criado-a-partir-do-codigo-fonte","text":"Para executar um programa a partir do c\u00f3digo-fonte o usu\u00e1rio dever\u00e1, em sua pasta no supercomputador, gerar o programa execut\u00e1vel. Para criar o execut\u00e1vel do programa digite: gcc helloWorld.c -o helloWorld Onde helloWorld.c \u00e9 o c\u00f3digo-fonte e helloWorld \u00e9 o c\u00f3digo execut\u00e1vel. Para executar o programa gerado, voc\u00ea dever\u00e1 executar o script gerado anteriormente utilizando o comando sbatch jobHelloWorld ou sbatch -p test jobHelloWorld de acordo com o tipo de n\u00f3 desejado. Caso o job tenha sido submetido corretamente, aparecer\u00e1 a mensagem: Submitted batch job JobID Onde JobID ser\u00e1 substitu\u00eddo pelo n\u00famero que identificar\u00e1 o job. A sa\u00edda do programa n\u00e3o aparecer\u00e1 na tela. Ela ser\u00e1 escrita em um arquivo de nome slurm-JobID.out , onde JobID ser\u00e1 o mesmo valor do id do job submetido. Para ver a sa\u00edda do programa, use o comando: cat slurm-JobID.out Substituindo JobID pelo id do job que voc\u00ea deseja ver a sa\u00edda. [ nomeDoUsuario@service0 ~ ] $ gcc helloWorld.c -o helloWorld [ nomeDoUsuario@service0 ~ ] $ sbatch jobHelloWorld Submitted batch job 14520 [ nomeDoUsuario@service0 ~ ] $ cat slurm-14520.out Hello World! Para executar programas em paralelo no supercomputador, leia os tutorias de OpenMP e MPI .","title":"Executando um programa criado a partir do c\u00f3digo-fonte"},{"location":"beginner/winscp_tutorial/","text":"Copiando arquivos atrav\u00e9s de uma interface gr\u00e1fica WinSCP (Windows) \u00b6 Ao contr\u00e1rio do linux, o windows explorer n\u00e3o possui suporte ao scp , tendo que fazer uso de um programa de terceiros chamado WinSCP . Para instalar o WinSCP v\u00e1 no site oficial e clique em Download . Ap\u00f3s a finaliza\u00e7\u00e3o do download, clique no execut\u00e1vel e instale a aplica\u00e7\u00e3o. Ao executar a aplica\u00e7\u00e3o ir\u00e1 aparecer a seguinte interface: HostName coloque: sc2.npad.ufrn.br Port number coloque 4422 User name coloque o seu nome de usu\u00e1rio IMPORTANTE: deixe o campo Password vazio Depois clique em Advanced... Em Advanced, v\u00e1 em Authentication , um submenu de SSH . Procure por Private key file e clique nos tr\u00eas pontinhos ( ... ). Agora voc\u00ea precisa procurar a chave privada que foi criada anteriormente, no meu caso o nome dela \u00e9 id_rsa . PuTTy \u00b6 Caso voc\u00ea esteja utilizando o Putty v\u00e1 onde voc\u00ea salvou a chave privada pelo PuttyGen . OpenSSH + Windows PowerShell \u00b6 Caso voc\u00ea tenha esteja usando OpenSSH e Windows PowerShell e n\u00e3o tenha mudado o nome da chave, o caminho at\u00e9 a chave privada \u00e9 C:\\Users\\NomeDoSeuUsu\u00e1rio\\.ssh\\id_rsa . Para visualiza-l\u00e1 ter\u00e1 que selecionar a op\u00e7\u00e3o: todos os tipos de arquivo . MobaXterm \u00b6 No caso em que voc\u00ea esteja usando o MobaXterm, saiba que o local padr\u00e3o em que o MobaXterm salva as chaves \u00e9 C:\\Users\\NomeDoSeuUsu\u00e1rio\\AppData\\Roaming\\MobaXterm\\home\\.ssh . Para visualizar a chave ter\u00e1 que selecionar a op\u00e7\u00e3o: todos os tipos de arquivo . Se tudo der certo o WinSCP ir\u00e1 pedir para converter a Chave em formato OpenSSH para um formado PuTTY caso precise. Concorde, converta, salve e sa\u00edda das configura\u00e7\u00f5es avan\u00e7adas atrav\u00e9s do bot\u00e3o OK . Quando voc\u00ea pressionar o bot\u00e3o Login . Voc\u00ea ter\u00e1 acesso ao seu sistema de arquivos do supercomputador. O WinSCP ir\u00e1 parecer como uma dessas duas telas dependendo do layout escolhido durante a instala\u00e7\u00e3o. Informa\u00e7\u00f5es extras sobre a aplica\u00e7\u00e3o utilizada \u00b6 WinSCP : winscp docs","title":"Copiando arquivos atrav\u00e9s de uma interface gr\u00e1fica WinSCP (Windows)"},{"location":"beginner/winscp_tutorial/#copiando-arquivos-atraves-de-uma-interface-grafica-winscp-windows","text":"Ao contr\u00e1rio do linux, o windows explorer n\u00e3o possui suporte ao scp , tendo que fazer uso de um programa de terceiros chamado WinSCP . Para instalar o WinSCP v\u00e1 no site oficial e clique em Download . Ap\u00f3s a finaliza\u00e7\u00e3o do download, clique no execut\u00e1vel e instale a aplica\u00e7\u00e3o. Ao executar a aplica\u00e7\u00e3o ir\u00e1 aparecer a seguinte interface: HostName coloque: sc2.npad.ufrn.br Port number coloque 4422 User name coloque o seu nome de usu\u00e1rio IMPORTANTE: deixe o campo Password vazio Depois clique em Advanced... Em Advanced, v\u00e1 em Authentication , um submenu de SSH . Procure por Private key file e clique nos tr\u00eas pontinhos ( ... ). Agora voc\u00ea precisa procurar a chave privada que foi criada anteriormente, no meu caso o nome dela \u00e9 id_rsa .","title":"Copiando arquivos atrav\u00e9s de uma interface gr\u00e1fica WinSCP (Windows)"},{"location":"beginner/winscp_tutorial/#putty","text":"Caso voc\u00ea esteja utilizando o Putty v\u00e1 onde voc\u00ea salvou a chave privada pelo PuttyGen .","title":"PuTTy"},{"location":"beginner/winscp_tutorial/#openssh-windows-powershell","text":"Caso voc\u00ea tenha esteja usando OpenSSH e Windows PowerShell e n\u00e3o tenha mudado o nome da chave, o caminho at\u00e9 a chave privada \u00e9 C:\\Users\\NomeDoSeuUsu\u00e1rio\\.ssh\\id_rsa . Para visualiza-l\u00e1 ter\u00e1 que selecionar a op\u00e7\u00e3o: todos os tipos de arquivo .","title":"OpenSSH + Windows PowerShell"},{"location":"beginner/winscp_tutorial/#mobaxterm","text":"No caso em que voc\u00ea esteja usando o MobaXterm, saiba que o local padr\u00e3o em que o MobaXterm salva as chaves \u00e9 C:\\Users\\NomeDoSeuUsu\u00e1rio\\AppData\\Roaming\\MobaXterm\\home\\.ssh . Para visualizar a chave ter\u00e1 que selecionar a op\u00e7\u00e3o: todos os tipos de arquivo . Se tudo der certo o WinSCP ir\u00e1 pedir para converter a Chave em formato OpenSSH para um formado PuTTY caso precise. Concorde, converta, salve e sa\u00edda das configura\u00e7\u00f5es avan\u00e7adas atrav\u00e9s do bot\u00e3o OK . Quando voc\u00ea pressionar o bot\u00e3o Login . Voc\u00ea ter\u00e1 acesso ao seu sistema de arquivos do supercomputador. O WinSCP ir\u00e1 parecer como uma dessas duas telas dependendo do layout escolhido durante a instala\u00e7\u00e3o.","title":"MobaXterm"},{"location":"beginner/winscp_tutorial/#informacoes-extras-sobre-a-aplicacao-utilizada","text":"WinSCP : winscp docs","title":"Informa\u00e7\u00f5es extras sobre a aplica\u00e7\u00e3o utilizada"},{"location":"intermediate/install_apps/","text":"Instala\u00e7\u00e3o de programas no supercomputador \u00b6 No supercomputador, h\u00e1 diversos programas instalados para todos os usu\u00e1rios. Caso deseje instalar um novo software no supercomputador, voc\u00ea pode solicitar que a equipe do NPAD fa\u00e7a a instala\u00e7\u00e3o, atrav\u00e9s da nossa P\u00e1gina de Solicita\u00e7\u00e3o de Software . Entretanto, esse pedido de software ser\u00e1 analisado e apenas softwares a serem utilizados por diversos pesquisadores ser\u00e3o aprovados. Os software aprovados entrar\u00e3o na fila de tarefas da equipe do NPAD e ser\u00e3o instalados assim que poss\u00edvel. Caso sua necessidade de instala\u00e7\u00e3o seja urgente ou o software que voc\u00ea deseja instalar n\u00e3o seja largamente utilizado por pesquisadores, voc\u00ea poder\u00e1 instalar o programa desejado na sua pasta pessoal seguindo uma das duas formas de instala\u00e7\u00e3o. Instala\u00e7\u00e3o manual ou atrav\u00e9s do gerenciador de pacotes Instala\u00e7\u00e3o de programas no supercomputador Instala\u00e7\u00e3o manual (recomendado) Exemplo: instala\u00e7\u00e3o manual do programa htop Instalando programas utilizando o yumdownloader Exemplo: Instala\u00e7\u00e3o do htop atrav\u00e9s do yumdownloader Instala\u00e7\u00e3o manual (recomendado) \u00b6 A instala\u00e7\u00e3o manual se resume em baixar, compilar o c\u00f3digo fonte e instalar na pasta home. A instala\u00e7\u00e3o manual \u00e9 a recomendada pois dessa forma voc\u00ea est\u00e1 livre para realizar as modifica\u00e7\u00f5es que quiser, escolher a vers\u00e3o que quiser sem a necessidade de interven\u00e7\u00e3o da equipe NPAD. No entanto, exige um conhecimento avan\u00e7ado sobre Linux que muitos clientes n\u00e3o possuem. Exemplo: instala\u00e7\u00e3o manual do programa htop \u00b6 Baixando o htop . No meu caso estou na minha pasta home, ao executar o comando. wget https://github.com/htop-dev/htop/archive/refs/tags/3.2.2.tar.gz ao finalizar o download terei baixado o arquivo: 3.2.2.tar.gz que representa o c\u00f3digo do htop na sua vers\u00e3o 3.2.2 . Posso visualizar o arquivos com o comando ls . # comando ls na minha pasta home ls 3 .2.2.tar.gz etc libvips pascal-parsec pkgs scratch O arquivo est\u00e1 compactado no formato .tar.gz . Ent\u00e3o vou descompactar o arquivo. tar -xf 3 .2.2.tar.gz Listando o diret\u00f3rio novamente verei que agora tenho uma pasta chamada htop-3.2.2 . ls 3 .2.2.tar.gz etc htop-3.2.2 libvips pascal-parsec pkgs scratch Entro do diret\u00f3rio do htop-3.2.2 e inicio o processo de configura\u00e7\u00e3o e compila\u00e7\u00e3o do pacote seguindo a documenta\u00e7\u00e3o do projeto https://github.com/htop-dev/htop#compile-from-source cd htop-3.2.2 ; Segundo a documenta\u00e7\u00e3o preciso executar 3 comandos autogen.sh , . /configure e make . Sendo o \u00faltimo comando a compila\u00e7\u00e3o. Segundo a documenta\u00e7\u00e3o o comando ./configure , aceita o par\u00e2metro --prefix referente ao local de instala\u00e7\u00e3o do pacote. Como n\u00e3o se pode instalar nenhum programa globalmente no supercomputador, ent\u00e3o deve-se passar um outro diret\u00f3rio. Segundo a freedesktop.org \u00e9 recomendado instalar aplica\u00e7\u00f5es de usu\u00e1rio na pasta oculta: ~/.local ou $HOME/.local , caso n\u00e3o exista essa pasta, crie. # configurando e compilando o htop ./autogen.sh && ./configure --prefix = $HOME /.local && make # instalando o htop make install Agora posso executar htop da seguinte forma ~/.local/bin/htop Como voc\u00ea instalou o htop na pasta padr\u00e3o do freedesktop , ent\u00e3o se voc\u00ea relogar ou executar o comando source ~/.bashrc ir\u00e1 perceber que o htop est\u00e1 dispon\u00edvel nas suas vari\u00e1veis de ambiente ou seja, pode executar o htop da seguinte forma: htop --version htop 3 .2.2 perceba que a vers\u00e3o que vem instalada por padr\u00e3o \u00e9 a 3.0.5 /bin/htop --version htop 3 .0.5 Instalando programas utilizando o yumdownloader \u00b6 Ao acessar sua pasta no supercomputador, execute o seguinte comando para baixar o seu programa. yumdownloader <nome do programa> O yumdownloader ir\u00e1 procurar em diferentes reposit\u00f3rios uma vers\u00e3o empacotada .rpm do programa que voc\u00ea pesquisou. Caso voc\u00ea encontre o pacote \u00e9 poss\u00edvel instalar o programa na sua pasta home com os programas: rpm2cpio e cpio , atrav\u00e9s do seguinte comando: rpm2cpio <nome do arquivo baixado>.rpm | cpio -idv Se tudo der certo o programa ser\u00e1 instalado no caminho relativo: ./usr . Exemplo: Instala\u00e7\u00e3o do htop atrav\u00e9s do yumdownloader \u00b6 Procure o htop nos reposit\u00f3rios com yumdownloader yumdownloader htop Caso yumdownloader encontre o pacote ele ir\u00e1 baix\u00e1-lo para voc\u00ea ls etc htop-3.2.1-1.el8.x86_64.rpm libvips pascal-parsec pkgs scratch no caso do htop ele encontrou uma vers\u00e3o mais antiga da aplica\u00e7\u00e3o a vers\u00e3o 3.2.1 . Para instalar o htop no diret\u00f3rio: ./usr , basta usar o comando: rpm2cpio htop-3.2.1-1.el8.x86_64.rpm | cpio -idv Perceba que ele criou uma pasta usr no local onde voc\u00ea est\u00e1 e instalar o htop nessa pasta ls etc htop-3.2.1-1.el8.x86_64.rpm libvips pascal-parsec pkgs scratch usr Como eu utilizei o comando rpm2cpio na minha pasta home. Ent\u00e3o posso posso executar htop da seguinte forma ~/usr/bin/htop","title":"Instala\u00e7\u00e3o de programas no supercomputador"},{"location":"intermediate/install_apps/#instalacao-de-programas-no-supercomputador","text":"No supercomputador, h\u00e1 diversos programas instalados para todos os usu\u00e1rios. Caso deseje instalar um novo software no supercomputador, voc\u00ea pode solicitar que a equipe do NPAD fa\u00e7a a instala\u00e7\u00e3o, atrav\u00e9s da nossa P\u00e1gina de Solicita\u00e7\u00e3o de Software . Entretanto, esse pedido de software ser\u00e1 analisado e apenas softwares a serem utilizados por diversos pesquisadores ser\u00e3o aprovados. Os software aprovados entrar\u00e3o na fila de tarefas da equipe do NPAD e ser\u00e3o instalados assim que poss\u00edvel. Caso sua necessidade de instala\u00e7\u00e3o seja urgente ou o software que voc\u00ea deseja instalar n\u00e3o seja largamente utilizado por pesquisadores, voc\u00ea poder\u00e1 instalar o programa desejado na sua pasta pessoal seguindo uma das duas formas de instala\u00e7\u00e3o. Instala\u00e7\u00e3o manual ou atrav\u00e9s do gerenciador de pacotes Instala\u00e7\u00e3o de programas no supercomputador Instala\u00e7\u00e3o manual (recomendado) Exemplo: instala\u00e7\u00e3o manual do programa htop Instalando programas utilizando o yumdownloader Exemplo: Instala\u00e7\u00e3o do htop atrav\u00e9s do yumdownloader","title":"Instala\u00e7\u00e3o de programas no supercomputador"},{"location":"intermediate/install_apps/#instalacao-manual-recomendado","text":"A instala\u00e7\u00e3o manual se resume em baixar, compilar o c\u00f3digo fonte e instalar na pasta home. A instala\u00e7\u00e3o manual \u00e9 a recomendada pois dessa forma voc\u00ea est\u00e1 livre para realizar as modifica\u00e7\u00f5es que quiser, escolher a vers\u00e3o que quiser sem a necessidade de interven\u00e7\u00e3o da equipe NPAD. No entanto, exige um conhecimento avan\u00e7ado sobre Linux que muitos clientes n\u00e3o possuem.","title":"Instala\u00e7\u00e3o manual (recomendado)"},{"location":"intermediate/install_apps/#exemplo-instalacao-manual-do-programa-htop","text":"Baixando o htop . No meu caso estou na minha pasta home, ao executar o comando. wget https://github.com/htop-dev/htop/archive/refs/tags/3.2.2.tar.gz ao finalizar o download terei baixado o arquivo: 3.2.2.tar.gz que representa o c\u00f3digo do htop na sua vers\u00e3o 3.2.2 . Posso visualizar o arquivos com o comando ls . # comando ls na minha pasta home ls 3 .2.2.tar.gz etc libvips pascal-parsec pkgs scratch O arquivo est\u00e1 compactado no formato .tar.gz . Ent\u00e3o vou descompactar o arquivo. tar -xf 3 .2.2.tar.gz Listando o diret\u00f3rio novamente verei que agora tenho uma pasta chamada htop-3.2.2 . ls 3 .2.2.tar.gz etc htop-3.2.2 libvips pascal-parsec pkgs scratch Entro do diret\u00f3rio do htop-3.2.2 e inicio o processo de configura\u00e7\u00e3o e compila\u00e7\u00e3o do pacote seguindo a documenta\u00e7\u00e3o do projeto https://github.com/htop-dev/htop#compile-from-source cd htop-3.2.2 ; Segundo a documenta\u00e7\u00e3o preciso executar 3 comandos autogen.sh , . /configure e make . Sendo o \u00faltimo comando a compila\u00e7\u00e3o. Segundo a documenta\u00e7\u00e3o o comando ./configure , aceita o par\u00e2metro --prefix referente ao local de instala\u00e7\u00e3o do pacote. Como n\u00e3o se pode instalar nenhum programa globalmente no supercomputador, ent\u00e3o deve-se passar um outro diret\u00f3rio. Segundo a freedesktop.org \u00e9 recomendado instalar aplica\u00e7\u00f5es de usu\u00e1rio na pasta oculta: ~/.local ou $HOME/.local , caso n\u00e3o exista essa pasta, crie. # configurando e compilando o htop ./autogen.sh && ./configure --prefix = $HOME /.local && make # instalando o htop make install Agora posso executar htop da seguinte forma ~/.local/bin/htop Como voc\u00ea instalou o htop na pasta padr\u00e3o do freedesktop , ent\u00e3o se voc\u00ea relogar ou executar o comando source ~/.bashrc ir\u00e1 perceber que o htop est\u00e1 dispon\u00edvel nas suas vari\u00e1veis de ambiente ou seja, pode executar o htop da seguinte forma: htop --version htop 3 .2.2 perceba que a vers\u00e3o que vem instalada por padr\u00e3o \u00e9 a 3.0.5 /bin/htop --version htop 3 .0.5","title":"Exemplo: instala\u00e7\u00e3o manual do programa htop"},{"location":"intermediate/install_apps/#instalando-programas-utilizando-o-yumdownloader","text":"Ao acessar sua pasta no supercomputador, execute o seguinte comando para baixar o seu programa. yumdownloader <nome do programa> O yumdownloader ir\u00e1 procurar em diferentes reposit\u00f3rios uma vers\u00e3o empacotada .rpm do programa que voc\u00ea pesquisou. Caso voc\u00ea encontre o pacote \u00e9 poss\u00edvel instalar o programa na sua pasta home com os programas: rpm2cpio e cpio , atrav\u00e9s do seguinte comando: rpm2cpio <nome do arquivo baixado>.rpm | cpio -idv Se tudo der certo o programa ser\u00e1 instalado no caminho relativo: ./usr .","title":"Instalando programas utilizando o yumdownloader"},{"location":"intermediate/install_apps/#exemplo-instalacao-do-htop-atraves-do-yumdownloader","text":"Procure o htop nos reposit\u00f3rios com yumdownloader yumdownloader htop Caso yumdownloader encontre o pacote ele ir\u00e1 baix\u00e1-lo para voc\u00ea ls etc htop-3.2.1-1.el8.x86_64.rpm libvips pascal-parsec pkgs scratch no caso do htop ele encontrou uma vers\u00e3o mais antiga da aplica\u00e7\u00e3o a vers\u00e3o 3.2.1 . Para instalar o htop no diret\u00f3rio: ./usr , basta usar o comando: rpm2cpio htop-3.2.1-1.el8.x86_64.rpm | cpio -idv Perceba que ele criou uma pasta usr no local onde voc\u00ea est\u00e1 e instalar o htop nessa pasta ls etc htop-3.2.1-1.el8.x86_64.rpm libvips pascal-parsec pkgs scratch usr Como eu utilizei o comando rpm2cpio na minha pasta home. Ent\u00e3o posso posso executar htop da seguinte forma ~/usr/bin/htop","title":"Exemplo: Instala\u00e7\u00e3o do htop atrav\u00e9s do yumdownloader"},{"location":"intermediate/slurm_commands/","text":"Comandos do supercomputador \u00b6 O Slurm \u00e9 o gerenciador de recursos usado no supercomputador. Ele que auxilia o usu\u00e1rio na utiliza\u00e7\u00e3o do supercomputador, disponibilizando ferramentas e comandos que ajudam na submiss\u00e3o, execu\u00e7\u00e3o e gerenciamento de jobs. Aqui ent\u00e3o vamos listar alguns desses comandos, muito \u00fateis para o usu\u00e1rio saber o que se passa com seu job rec\u00e9m-submetido. Comandos \u00b6 Comando Descri\u00e7\u00e3o sinfo Visualize as informa\u00e7\u00f5es das parti\u00e7\u00f5es e n\u00f3s do supercomputador. sprio Visualize as informa\u00e7\u00f5es dos fatores que comp\u00f5em a prioridade na fila de cada job. squeue Visualize as informa\u00e7\u00f5es gerais dos Job's que est\u00e3o na fila ou executando. sbatch Submeta um Job para o supercomputador. scancel Cancele um Job que est\u00e1 na fila ou em execu\u00e7\u00e3o. sinfo \u00b6 sinfo exibe as informa\u00e7\u00f5es das parti\u00e7\u00f5es e n\u00f3s do supercomputador. Exemplo: sinfo \u00b6 scavalcanti@headnode0 ~ ] $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST cluster* up 20 -00:00:0 1 down* r1i2n14 cluster* up 20 -00:00:0 18 alloc r1i0n [ 1 -2,4-7,11-12,14-16 ] ,r1i1n [ 7 ,9,15 ] ,r1i2n [ 1 -2,11 ] ,r1i3n1 cluster* up 20 -00:00:0 13 mix r1i0n [ 0 ,3,9 ] ,r1i1n [ 4 -6,10,14 ] ,r1i2n [ 3 -4,10,13 ] ,r1i3n2 cluster* up 20 -00:00:0 19 idle r1i0n [ 10 ,17 ] ,r1i1n [ 0 -3,11-13,16 ] ,r1i2n [ 0 ,5-7,9,12,15-16 ] ,r1i3n0 service up 20 -00:00:0 3 mix service [ 1 ,3-4 ] service up 20 -00:00:0 1 alloc service2 test up 30 :00 1 down* r1i2n14 test up 30 :00 19 alloc r1i0n [ 1 -2,4-7,11-12,14-16 ] ,r1i1n [ 7 ,9,15 ] ,r1i2n [ 1 -2,11 ] ,r1i3n1,service2 test up 30 :00 16 mix r1i0n [ 0 ,3,9 ] ,r1i1n [ 4 -6,10,14 ] ,r1i2n [ 3 -4,10,13 ] ,r1i3n2,service [ 1 ,3-4 ] test up 30 :00 19 idle r1i0n [ 10 ,17 ] ,r1i1n [ 0 -3,11-13,16 ] ,r1i2n [ 0 ,5-7,9,12,15-16 ] ,r1i3n0 intel-512 up 20 -00:00:0 6 mix r1i3n [ 11 -16 ] intel-256 up 20 -00:00:0 6 mix r1i3n [ 3 -6,9-10 ] intel-256 up 20 -00:00:0 1 idle r1i3n7 gpu up 2 -00:00:00 2 idle gpunode [ 0 -1 ] A lista abaixo mostra o que representa cada campo de sa\u00edda do comando sinfo. PARTITION : Parti\u00e7\u00f5es do supercomputador cluster : Parti\u00e7\u00e3o padr\u00e3o indicada pelo asterisco, composta por 64 n\u00f3s computacionais em l\u00e2mina. \\ r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16] e r1i3n[0-7, 9-16] test : Parti\u00e7\u00e3o para testes r\u00e1pidos, composta por 72 (todos) n\u00f3s computacionais. Jobs rodados na parti\u00e7\u00e3o teste tem o valor de prioridade aumentado em 15000. \\ r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16], r1i3n[0-7, 9-16], service[1-4, 8-11] service : Parti\u00e7\u00e3o composta por 4 n\u00f3s computacionais. \\ service[1-4] knl : Parti\u00e7\u00e3o composta por 2 n\u00f3s computacionais com grande n\u00famero de n\u00facleos. \\ service[8,9] gpu : Parti\u00e7\u00e3o composta por 2 n\u00f3s computacionais com 8 GPUs cada. \\ service[10,11] full : Parti\u00e7\u00e3o composta por todos os 72 n\u00f3s computacionais. \\ r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16], r1i3n[0-7, 9-16], service[1-4, 8-11] Mais detalhes sobre o hardware podem ser vistos em Hardware AVAIL : Disponibilidade de cada parti\u00e7\u00e3o ( Available ) TIMELIMIT : Tempo limite de execu\u00e7\u00e3o do job na parti\u00e7\u00e3o correspondente. Todas as parti\u00e7\u00f5es possuem o tempo limite de 20 dias para execu\u00e7\u00e3o do job. Exceto a parti\u00e7\u00e3o teste, cujo o tempo limite \u00e9 de 30 minutos com ganho na prioridade para execu\u00e7\u00e3o.\\ Formato do timelimit no comando sinfo : Dias - horas : minutos : segundos, exemplo: 20-13:22:21 significa 20 dias, 13 horas, 22 minutos e 21 segundos NODES : N\u00famero de n\u00f3s de cada parti\u00e7\u00e3o STATE : Campo mais relevante dado como resposta do comando sinfo. Seu resultado pode ter significados diferentes, dependendo da sa\u00edda fornecida, conforme \u00e9 poss\u00edvel verificar abaixo. alloc : Indica que um conjunto de n\u00f3s est\u00e1 em uso idle : Indica que um conjunto de n\u00f3s est\u00e1 ocioso down/drain : Indica que um conjunto de n\u00f3s se encontra indispon\u00edvel maint/resv : Indica que um conjunto de n\u00f3s est\u00e1 reservados mix : Indica que um conjunto de n\u00f3s est\u00e1 sendo compartilhados por mais de um job NODELIST : Representa as listas de n\u00f3s correspondentes a cada par parti\u00e7\u00e3o/estado sprio \u00b6 Sprio \u00e9 usado para exibir componentes da prioridade da fila de jobs. Quanto maior for seu valor, maior ser\u00e1 a sua prioridade. S\u00e3o mostradas somente as informa\u00e7\u00f5es dos jobs que ainda n\u00e3o est\u00e3o em execu\u00e7\u00e3o. Caso seja de seu interesse entender como se d\u00e1 o c\u00e1lculo da prioridade da fila de jobs, clique aqui . Exemplo: sprio \u00b6 sprio -l JOBID PARTITION USER PRIORITY SITE AGE ASSOC FAIRSHARE JOBSIZE PARTITION QOS NICE 164854 intel-512 dnpinhei 25031 0 1000 0 31 0 0 24000 0 165887 intel-512 dnpinhei 25031 0 1000 0 31 0 0 24000 0 167884 cluster dnpinhei 25031 0 1000 0 31 0 0 24000 0 168322 intel-512 bpesilva 1486 0 1000 0 487 0 0 0 0 168323 intel-512 bpesilva 1486 0 1000 0 487 0 0 0 0 168333 intel-512 bpesilva 1486 0 1000 0 487 0 0 0 0 168917 intel-512 bpesilva 1226 0 740 0 487 0 0 0 0 170024 cluster jxdlneto 1028 0 510 0 518 0 0 0 0 170025 cluster jxdlneto 1028 0 510 0 518 0 0 0 0 170026 cluster jxdlneto 1027 0 510 0 518 0 0 0 0 170027 cluster jxdlneto 1027 0 509 0 518 0 0 0 0 170028 cluster jxdlneto 1027 0 509 0 518 0 0 0 0 170030 cluster jxdlneto 1026 0 509 0 518 0 0 0 0 170031 cluster jxdlneto 1024 0 506 0 518 0 0 0 0 170032 cluster jxdlneto 1024 0 506 0 518 0 0 0 0 170033 cluster jxdlneto 1023 0 506 0 518 0 0 0 0 170034 cluster crcosta 1840 0 506 0 1334 0 0 0 0 170035 cluster crcosta 1838 0 504 0 1334 0 0 0 0 170036 cluster crcosta 1838 0 504 0 1334 0 0 0 0 170037 cluster scavalca 2984 0 504 0 2480 0 0 0 0 170514 cluster thsrodri 2423 0 446 0 1978 0 0 0 0 170515 cluster thsrodri 2423 0 446 0 1978 0 0 0 0 170516 cluster thsrodri 2423 0 446 0 1978 0 0 0 0 170517 cluster thsrodri 2423 0 446 0 1978 0 0 0 0 A lista abaixo mostra o que representa cada campo de sa\u00edda do comando sprio. JOBID : Identificador (ID) do job em espera USER : Usu\u00e1rio que submeteu o job PRIORITY : Representa a prioridade do job na fila. Quanto maior seu valor, maior sua prioridade AGE : \u00c9 utilizado para o c\u00e1lculo da prioridade. Se o job acabou de ser submetido, ent\u00e3o o AGE vale 0; caso o mesmo esteja em espera h\u00e1 24 horas, ent\u00e3o o AGE tem valor 500; a partir de 48 horas, esse campo passa a valer 1000. Quaisquer valores intermedi\u00e1rios representam fra\u00e7\u00f5es de tempo, obedecendo o padr\u00e3o supracitado. FAIRSHARE \u00c9 utilizado para o c\u00e1lculo da prioridade, juntamente com o valor do campo AGE. Representa uma esp\u00e9cie de peso, compensa\u00e7\u00e3o, no caso do usu\u00e1rio consumir uma quantidade menor ou maior do que sua fatia esperada. JOBSIZE : Campo irrelevante: n\u00e3o \u00e9 utilizado no c\u00e1lculo da prioridade PARTITION : Somente ter\u00e1 um valor caso a fila utilizada seja a test QOS : Representa simplesmente um b\u00f4nus na prioridade, caso o usu\u00e1rio seja o administrador NICE : Campo irrelevante: n\u00e3o \u00e9 utilizado no c\u00e1lculo da prioridade squeue \u00b6 Squeue exibe as informa\u00e7\u00f5es gerais dos Job's que est\u00e3o na fila ou executando. Este comando possu\u00ed varia\u00e7\u00f5es como:\\ squeue --start : Exibe, al\u00e9m da fila, o tempo esperado para execu\u00e7\u00e3o dos jobs.\\ watch squeue : Atualiza a exibi\u00e7\u00e3o da fila a cada 2 segundos. Exemplo: squeue \u00b6 squeue \u2009 JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 167884 cluster tupi10 dnpinhei PD 0 :00 21 ( Resources ) 170642 cluster MIN_06 rbdpasso PD 0 :00 2 ( Priority ) 170639 cluster MIN_03 rbdpasso PD 0 :00 2 ( Priority ) 170640 cluster MIN_04 rbdpasso PD 0 :00 2 ( Priority ) 170641 cluster MIN_05 rbdpasso PD 0 :00 2 ( Priority ) 170643 cluster MIN_07 rbdpasso PD 0 :00 2 ( Priority ) 170037 cluster PascalAn scavalca PD 0 :00 1 ( Priority ) 170518 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170519 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170517 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170516 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170515 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170514 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170034 cluster SLP_20 crcosta PD 0 :00 1 ( Priority ) 170035 cluster SLP_21 crcosta PD 0 :00 1 ( Priority ) 170036 cluster SLP_22 crcosta PD 0 :00 1 ( Priority ) 170659 cluster SLP_31 crcosta PD 0 :00 1 ( Priority ) 170660 cluster SLP_32 crcosta PD 0 :00 1 ( Priority ) 170661 cluster SLP_33 crcosta PD 0 :00 1 ( Priority ) 170662 cluster SLP_41 crcosta PD 0 :00 1 ( Priority ) 170663 cluster SLP_42 crcosta PD 0 :00 1 ( Priority ) 170664 cluster SLP_43 crcosta PD 0 :00 1 ( Priority ) 170025 cluster dinam_hw jxdlneto PD 0 :00 1 ( Priority ) 170026 cluster dinam_iw jxdlneto PD 0 :00 1 ( Priority ) 170024 cluster dinam_hw jxdlneto PD 0 :00 1 ( Priority ) 170027 cluster dinam_iw jxdlneto PD 0 :00 1 ( Priority ) 170028 cluster dinam_iw jxdlneto PD 0 :00 1 ( Priority ) 170030 cluster PBEqe_ge jxdlneto PD 0 :00 1 ( Priority ) 170032 cluster PBEd35qe jxdlneto PD 0 :00 1 ( Priority ) 170033 cluster PBEd36qe jxdlneto PD 0 :00 1 ( Priority ) 170031 cluster PBEd2qe_ jxdlneto PD 0 :00 1 ( Priority ) 170657 cluster complex_ ldflacer PD 0 :00 1 ( Priority ) 170658 cluster Citopt bpesilva PD 0 :00 1 ( Priority ) 168024 cluster fwi-deli cdssanta R 2 -02:21:08 4 r1i0n [ 14 -16 ] ,r1i1n9 A lista abaixo mostra o que representa cada campo de sa\u00edda do comando squeue. JOBID : Identificador (ID) do job PARTITION : Fila do supercomputador que est\u00e1 sendo utilizada NAME : Nome do job. USER : Usu\u00e1rio que est\u00e1 executando aquele job ST : O estado em que se encontra o job. Abaixo seguem os dois estados poss\u00edveis em que um job pode se encontrar. PD ( Pending ): Significa que o job est\u00e1 pendente, devido a uma ou mais raz\u00f5es. Neste caso, mais detalhes s\u00e3o mostrados no campo NODELIST(REASON) R ( Running ): Significa que o job est\u00e1 em execu\u00e7\u00e3o normal TIME : O tempo total em que o job est\u00e1 em execu\u00e7\u00e3o NODES : O n\u00famero de n\u00f3s em que o job est\u00e1 sendo executado NODELIST(REASON) : Se o job estiver em execu\u00e7\u00e3o, esse campo lista os nomes dos n\u00f3s em que o job est\u00e1 sendo executado. Se o job estiver pendente, este campo mostra o motivo pelo qual o trabalho est\u00e1 pendente. Neste caso, os motivos podem ser: Resources : Significa que os recursos de computa\u00e7\u00e3o necess\u00e1rios para aquele job n\u00e3o est\u00e3o dispon\u00edveis no momento Priority : Indica que o job est\u00e1 aguardando sua vez na fila,seguindo a fila de prioridades Dependency : Indica que o job est\u00e1 aguardando a conclus\u00e3o de outro trabalho antes de ser executado. Depend\u00eancias s\u00e3o solicitadas quando um trabalho \u00e9 submetido PartitionTimeLimit : Significa que o job solicitou mais tempo de execu\u00e7\u00e3o do que a fila permite AssocGrpCpuLimit : Indica que o grupo do usu\u00e1rio est\u00e1 executando pr\u00f3ximo ao seu n\u00famero m\u00e1ximo de n\u00facleos de CPU permitidos AssocGrpCPURunMinsLimit : Indica que executar aquele job colocaria o grupo do usu\u00e1rio al\u00e9m do n\u00famero m\u00e1ximo de minutos de CPU alocados para os jobs atualmente em execu\u00e7\u00e3o. AssocGrpMemLimit : O grupo do usu\u00e1rio alocou sua quantidade m\u00e1xima de RAM JobArrayTaskLimit : Indica que aquele job est\u00e1 envolvido em um vetor (grupo) de tarefas limitado a executar em um n\u00famero definido de n\u00facleos de CPU de uma vez sbatch \u00b6 Sbatch envia um script para o supercomputador, esse script \u00e9 passado atrav\u00e9s do nome do arquivo que for especificado, o sbach ir\u00e1 ler o script a partir da entrada padr\u00e3o. (Voc\u00ea pode encontrar como criar um script na Parte 2 do tutorial referente \u00e0 introdu\u00e7\u00e3o ao supercomputador). Exemplo \u00b6 squeue -u scavalcanti JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) sbatch run_pascalanalyzer.sh Submitted batch job 170926 squeue -u scavalcanti JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 170926 cluster PascalAn scavalca PD 0 :00 1 ( Priority ) no exemplo acima, foi utilizado o comando squeue como o par\u00e2metro -u para mostrar as informa\u00e7\u00f5es gerais dos jobs do usu\u00e1rio scavalcanti, tanto os que est\u00e3o executando. quanto os que est\u00e3o na fila. Sendo que n\u00e3o havia nenhum job a ser mostrado. Posteriormente, o comando sbatch foi usado para submeter um job. Novamente, o comando squeue foi usado, mostrando que o job foi devidamente submetido e aguarda na fila para ser executado. scancel \u00b6 Scancel \u00e9 utilizado para cancelar um job depois que ele foi submetido, podendo ele estar na fila de espera ou em execu\u00e7\u00e3o. O job ser\u00e1 interrompido de imediato, ent\u00e3o seja sempre cuidadoso ao usar esse comando, lembrando sempre que ao executar outro job voc\u00ea estar\u00e1 no final da fila. Entretando, voc\u00ea n\u00e3o precisa se preocupar que nenhum usu\u00e1rio ter\u00e1 acesso ao job de outro, sendo assim, n\u00e3o h\u00e1 riscos de algu\u00e9m cancelar job's que n\u00e3o s\u00e3o seus. Exemplo: scancel \u00b6 squeue -u scavalcanti JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 170926 cluster PascalAn scavalca PD 0 :00 1 ( Priority ) scancel 170926 squeue -u scavalcanti JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) No exemplo acima, foi utilizado o comando squeue como o par\u00e2metro -u para mostrar as informa\u00e7\u00f5es gerais dos jobs do usu\u00e1rio scavalcanti, tanto os que est\u00e3o executando. quanto os que est\u00e3o na fila. Sendo mostrado um \u00fanico job em espera. Posteriormente, o comando scancel foi usado para cancelar o \u00fanico job existente. Novamente, o comando squeue foi usado, mostrando que o job foi devidamente deletado.","title":"Comandos do supercomputador"},{"location":"intermediate/slurm_commands/#comandos-do-supercomputador","text":"O Slurm \u00e9 o gerenciador de recursos usado no supercomputador. Ele que auxilia o usu\u00e1rio na utiliza\u00e7\u00e3o do supercomputador, disponibilizando ferramentas e comandos que ajudam na submiss\u00e3o, execu\u00e7\u00e3o e gerenciamento de jobs. Aqui ent\u00e3o vamos listar alguns desses comandos, muito \u00fateis para o usu\u00e1rio saber o que se passa com seu job rec\u00e9m-submetido.","title":"Comandos do supercomputador"},{"location":"intermediate/slurm_commands/#comandos","text":"Comando Descri\u00e7\u00e3o sinfo Visualize as informa\u00e7\u00f5es das parti\u00e7\u00f5es e n\u00f3s do supercomputador. sprio Visualize as informa\u00e7\u00f5es dos fatores que comp\u00f5em a prioridade na fila de cada job. squeue Visualize as informa\u00e7\u00f5es gerais dos Job's que est\u00e3o na fila ou executando. sbatch Submeta um Job para o supercomputador. scancel Cancele um Job que est\u00e1 na fila ou em execu\u00e7\u00e3o.","title":"Comandos"},{"location":"intermediate/slurm_commands/#sinfo","text":"sinfo exibe as informa\u00e7\u00f5es das parti\u00e7\u00f5es e n\u00f3s do supercomputador.","title":"sinfo"},{"location":"intermediate/slurm_commands/#exemplo-sinfo","text":"scavalcanti@headnode0 ~ ] $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST cluster* up 20 -00:00:0 1 down* r1i2n14 cluster* up 20 -00:00:0 18 alloc r1i0n [ 1 -2,4-7,11-12,14-16 ] ,r1i1n [ 7 ,9,15 ] ,r1i2n [ 1 -2,11 ] ,r1i3n1 cluster* up 20 -00:00:0 13 mix r1i0n [ 0 ,3,9 ] ,r1i1n [ 4 -6,10,14 ] ,r1i2n [ 3 -4,10,13 ] ,r1i3n2 cluster* up 20 -00:00:0 19 idle r1i0n [ 10 ,17 ] ,r1i1n [ 0 -3,11-13,16 ] ,r1i2n [ 0 ,5-7,9,12,15-16 ] ,r1i3n0 service up 20 -00:00:0 3 mix service [ 1 ,3-4 ] service up 20 -00:00:0 1 alloc service2 test up 30 :00 1 down* r1i2n14 test up 30 :00 19 alloc r1i0n [ 1 -2,4-7,11-12,14-16 ] ,r1i1n [ 7 ,9,15 ] ,r1i2n [ 1 -2,11 ] ,r1i3n1,service2 test up 30 :00 16 mix r1i0n [ 0 ,3,9 ] ,r1i1n [ 4 -6,10,14 ] ,r1i2n [ 3 -4,10,13 ] ,r1i3n2,service [ 1 ,3-4 ] test up 30 :00 19 idle r1i0n [ 10 ,17 ] ,r1i1n [ 0 -3,11-13,16 ] ,r1i2n [ 0 ,5-7,9,12,15-16 ] ,r1i3n0 intel-512 up 20 -00:00:0 6 mix r1i3n [ 11 -16 ] intel-256 up 20 -00:00:0 6 mix r1i3n [ 3 -6,9-10 ] intel-256 up 20 -00:00:0 1 idle r1i3n7 gpu up 2 -00:00:00 2 idle gpunode [ 0 -1 ] A lista abaixo mostra o que representa cada campo de sa\u00edda do comando sinfo. PARTITION : Parti\u00e7\u00f5es do supercomputador cluster : Parti\u00e7\u00e3o padr\u00e3o indicada pelo asterisco, composta por 64 n\u00f3s computacionais em l\u00e2mina. \\ r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16] e r1i3n[0-7, 9-16] test : Parti\u00e7\u00e3o para testes r\u00e1pidos, composta por 72 (todos) n\u00f3s computacionais. Jobs rodados na parti\u00e7\u00e3o teste tem o valor de prioridade aumentado em 15000. \\ r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16], r1i3n[0-7, 9-16], service[1-4, 8-11] service : Parti\u00e7\u00e3o composta por 4 n\u00f3s computacionais. \\ service[1-4] knl : Parti\u00e7\u00e3o composta por 2 n\u00f3s computacionais com grande n\u00famero de n\u00facleos. \\ service[8,9] gpu : Parti\u00e7\u00e3o composta por 2 n\u00f3s computacionais com 8 GPUs cada. \\ service[10,11] full : Parti\u00e7\u00e3o composta por todos os 72 n\u00f3s computacionais. \\ r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16], r1i3n[0-7, 9-16], service[1-4, 8-11] Mais detalhes sobre o hardware podem ser vistos em Hardware AVAIL : Disponibilidade de cada parti\u00e7\u00e3o ( Available ) TIMELIMIT : Tempo limite de execu\u00e7\u00e3o do job na parti\u00e7\u00e3o correspondente. Todas as parti\u00e7\u00f5es possuem o tempo limite de 20 dias para execu\u00e7\u00e3o do job. Exceto a parti\u00e7\u00e3o teste, cujo o tempo limite \u00e9 de 30 minutos com ganho na prioridade para execu\u00e7\u00e3o.\\ Formato do timelimit no comando sinfo : Dias - horas : minutos : segundos, exemplo: 20-13:22:21 significa 20 dias, 13 horas, 22 minutos e 21 segundos NODES : N\u00famero de n\u00f3s de cada parti\u00e7\u00e3o STATE : Campo mais relevante dado como resposta do comando sinfo. Seu resultado pode ter significados diferentes, dependendo da sa\u00edda fornecida, conforme \u00e9 poss\u00edvel verificar abaixo. alloc : Indica que um conjunto de n\u00f3s est\u00e1 em uso idle : Indica que um conjunto de n\u00f3s est\u00e1 ocioso down/drain : Indica que um conjunto de n\u00f3s se encontra indispon\u00edvel maint/resv : Indica que um conjunto de n\u00f3s est\u00e1 reservados mix : Indica que um conjunto de n\u00f3s est\u00e1 sendo compartilhados por mais de um job NODELIST : Representa as listas de n\u00f3s correspondentes a cada par parti\u00e7\u00e3o/estado","title":"Exemplo: sinfo"},{"location":"intermediate/slurm_commands/#sprio","text":"Sprio \u00e9 usado para exibir componentes da prioridade da fila de jobs. Quanto maior for seu valor, maior ser\u00e1 a sua prioridade. S\u00e3o mostradas somente as informa\u00e7\u00f5es dos jobs que ainda n\u00e3o est\u00e3o em execu\u00e7\u00e3o. Caso seja de seu interesse entender como se d\u00e1 o c\u00e1lculo da prioridade da fila de jobs, clique aqui .","title":"sprio"},{"location":"intermediate/slurm_commands/#exemplo-sprio","text":"sprio -l JOBID PARTITION USER PRIORITY SITE AGE ASSOC FAIRSHARE JOBSIZE PARTITION QOS NICE 164854 intel-512 dnpinhei 25031 0 1000 0 31 0 0 24000 0 165887 intel-512 dnpinhei 25031 0 1000 0 31 0 0 24000 0 167884 cluster dnpinhei 25031 0 1000 0 31 0 0 24000 0 168322 intel-512 bpesilva 1486 0 1000 0 487 0 0 0 0 168323 intel-512 bpesilva 1486 0 1000 0 487 0 0 0 0 168333 intel-512 bpesilva 1486 0 1000 0 487 0 0 0 0 168917 intel-512 bpesilva 1226 0 740 0 487 0 0 0 0 170024 cluster jxdlneto 1028 0 510 0 518 0 0 0 0 170025 cluster jxdlneto 1028 0 510 0 518 0 0 0 0 170026 cluster jxdlneto 1027 0 510 0 518 0 0 0 0 170027 cluster jxdlneto 1027 0 509 0 518 0 0 0 0 170028 cluster jxdlneto 1027 0 509 0 518 0 0 0 0 170030 cluster jxdlneto 1026 0 509 0 518 0 0 0 0 170031 cluster jxdlneto 1024 0 506 0 518 0 0 0 0 170032 cluster jxdlneto 1024 0 506 0 518 0 0 0 0 170033 cluster jxdlneto 1023 0 506 0 518 0 0 0 0 170034 cluster crcosta 1840 0 506 0 1334 0 0 0 0 170035 cluster crcosta 1838 0 504 0 1334 0 0 0 0 170036 cluster crcosta 1838 0 504 0 1334 0 0 0 0 170037 cluster scavalca 2984 0 504 0 2480 0 0 0 0 170514 cluster thsrodri 2423 0 446 0 1978 0 0 0 0 170515 cluster thsrodri 2423 0 446 0 1978 0 0 0 0 170516 cluster thsrodri 2423 0 446 0 1978 0 0 0 0 170517 cluster thsrodri 2423 0 446 0 1978 0 0 0 0 A lista abaixo mostra o que representa cada campo de sa\u00edda do comando sprio. JOBID : Identificador (ID) do job em espera USER : Usu\u00e1rio que submeteu o job PRIORITY : Representa a prioridade do job na fila. Quanto maior seu valor, maior sua prioridade AGE : \u00c9 utilizado para o c\u00e1lculo da prioridade. Se o job acabou de ser submetido, ent\u00e3o o AGE vale 0; caso o mesmo esteja em espera h\u00e1 24 horas, ent\u00e3o o AGE tem valor 500; a partir de 48 horas, esse campo passa a valer 1000. Quaisquer valores intermedi\u00e1rios representam fra\u00e7\u00f5es de tempo, obedecendo o padr\u00e3o supracitado. FAIRSHARE \u00c9 utilizado para o c\u00e1lculo da prioridade, juntamente com o valor do campo AGE. Representa uma esp\u00e9cie de peso, compensa\u00e7\u00e3o, no caso do usu\u00e1rio consumir uma quantidade menor ou maior do que sua fatia esperada. JOBSIZE : Campo irrelevante: n\u00e3o \u00e9 utilizado no c\u00e1lculo da prioridade PARTITION : Somente ter\u00e1 um valor caso a fila utilizada seja a test QOS : Representa simplesmente um b\u00f4nus na prioridade, caso o usu\u00e1rio seja o administrador NICE : Campo irrelevante: n\u00e3o \u00e9 utilizado no c\u00e1lculo da prioridade","title":"Exemplo: sprio"},{"location":"intermediate/slurm_commands/#squeue","text":"Squeue exibe as informa\u00e7\u00f5es gerais dos Job's que est\u00e3o na fila ou executando. Este comando possu\u00ed varia\u00e7\u00f5es como:\\ squeue --start : Exibe, al\u00e9m da fila, o tempo esperado para execu\u00e7\u00e3o dos jobs.\\ watch squeue : Atualiza a exibi\u00e7\u00e3o da fila a cada 2 segundos.","title":"squeue"},{"location":"intermediate/slurm_commands/#exemplo-squeue","text":"squeue \u2009 JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 167884 cluster tupi10 dnpinhei PD 0 :00 21 ( Resources ) 170642 cluster MIN_06 rbdpasso PD 0 :00 2 ( Priority ) 170639 cluster MIN_03 rbdpasso PD 0 :00 2 ( Priority ) 170640 cluster MIN_04 rbdpasso PD 0 :00 2 ( Priority ) 170641 cluster MIN_05 rbdpasso PD 0 :00 2 ( Priority ) 170643 cluster MIN_07 rbdpasso PD 0 :00 2 ( Priority ) 170037 cluster PascalAn scavalca PD 0 :00 1 ( Priority ) 170518 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170519 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170517 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170516 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170515 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170514 cluster cout1 thsrodri PD 0 :00 1 ( Priority ) 170034 cluster SLP_20 crcosta PD 0 :00 1 ( Priority ) 170035 cluster SLP_21 crcosta PD 0 :00 1 ( Priority ) 170036 cluster SLP_22 crcosta PD 0 :00 1 ( Priority ) 170659 cluster SLP_31 crcosta PD 0 :00 1 ( Priority ) 170660 cluster SLP_32 crcosta PD 0 :00 1 ( Priority ) 170661 cluster SLP_33 crcosta PD 0 :00 1 ( Priority ) 170662 cluster SLP_41 crcosta PD 0 :00 1 ( Priority ) 170663 cluster SLP_42 crcosta PD 0 :00 1 ( Priority ) 170664 cluster SLP_43 crcosta PD 0 :00 1 ( Priority ) 170025 cluster dinam_hw jxdlneto PD 0 :00 1 ( Priority ) 170026 cluster dinam_iw jxdlneto PD 0 :00 1 ( Priority ) 170024 cluster dinam_hw jxdlneto PD 0 :00 1 ( Priority ) 170027 cluster dinam_iw jxdlneto PD 0 :00 1 ( Priority ) 170028 cluster dinam_iw jxdlneto PD 0 :00 1 ( Priority ) 170030 cluster PBEqe_ge jxdlneto PD 0 :00 1 ( Priority ) 170032 cluster PBEd35qe jxdlneto PD 0 :00 1 ( Priority ) 170033 cluster PBEd36qe jxdlneto PD 0 :00 1 ( Priority ) 170031 cluster PBEd2qe_ jxdlneto PD 0 :00 1 ( Priority ) 170657 cluster complex_ ldflacer PD 0 :00 1 ( Priority ) 170658 cluster Citopt bpesilva PD 0 :00 1 ( Priority ) 168024 cluster fwi-deli cdssanta R 2 -02:21:08 4 r1i0n [ 14 -16 ] ,r1i1n9 A lista abaixo mostra o que representa cada campo de sa\u00edda do comando squeue. JOBID : Identificador (ID) do job PARTITION : Fila do supercomputador que est\u00e1 sendo utilizada NAME : Nome do job. USER : Usu\u00e1rio que est\u00e1 executando aquele job ST : O estado em que se encontra o job. Abaixo seguem os dois estados poss\u00edveis em que um job pode se encontrar. PD ( Pending ): Significa que o job est\u00e1 pendente, devido a uma ou mais raz\u00f5es. Neste caso, mais detalhes s\u00e3o mostrados no campo NODELIST(REASON) R ( Running ): Significa que o job est\u00e1 em execu\u00e7\u00e3o normal TIME : O tempo total em que o job est\u00e1 em execu\u00e7\u00e3o NODES : O n\u00famero de n\u00f3s em que o job est\u00e1 sendo executado NODELIST(REASON) : Se o job estiver em execu\u00e7\u00e3o, esse campo lista os nomes dos n\u00f3s em que o job est\u00e1 sendo executado. Se o job estiver pendente, este campo mostra o motivo pelo qual o trabalho est\u00e1 pendente. Neste caso, os motivos podem ser: Resources : Significa que os recursos de computa\u00e7\u00e3o necess\u00e1rios para aquele job n\u00e3o est\u00e3o dispon\u00edveis no momento Priority : Indica que o job est\u00e1 aguardando sua vez na fila,seguindo a fila de prioridades Dependency : Indica que o job est\u00e1 aguardando a conclus\u00e3o de outro trabalho antes de ser executado. Depend\u00eancias s\u00e3o solicitadas quando um trabalho \u00e9 submetido PartitionTimeLimit : Significa que o job solicitou mais tempo de execu\u00e7\u00e3o do que a fila permite AssocGrpCpuLimit : Indica que o grupo do usu\u00e1rio est\u00e1 executando pr\u00f3ximo ao seu n\u00famero m\u00e1ximo de n\u00facleos de CPU permitidos AssocGrpCPURunMinsLimit : Indica que executar aquele job colocaria o grupo do usu\u00e1rio al\u00e9m do n\u00famero m\u00e1ximo de minutos de CPU alocados para os jobs atualmente em execu\u00e7\u00e3o. AssocGrpMemLimit : O grupo do usu\u00e1rio alocou sua quantidade m\u00e1xima de RAM JobArrayTaskLimit : Indica que aquele job est\u00e1 envolvido em um vetor (grupo) de tarefas limitado a executar em um n\u00famero definido de n\u00facleos de CPU de uma vez","title":"Exemplo: squeue"},{"location":"intermediate/slurm_commands/#sbatch","text":"Sbatch envia um script para o supercomputador, esse script \u00e9 passado atrav\u00e9s do nome do arquivo que for especificado, o sbach ir\u00e1 ler o script a partir da entrada padr\u00e3o. (Voc\u00ea pode encontrar como criar um script na Parte 2 do tutorial referente \u00e0 introdu\u00e7\u00e3o ao supercomputador).","title":"sbatch"},{"location":"intermediate/slurm_commands/#exemplo","text":"squeue -u scavalcanti JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) sbatch run_pascalanalyzer.sh Submitted batch job 170926 squeue -u scavalcanti JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 170926 cluster PascalAn scavalca PD 0 :00 1 ( Priority ) no exemplo acima, foi utilizado o comando squeue como o par\u00e2metro -u para mostrar as informa\u00e7\u00f5es gerais dos jobs do usu\u00e1rio scavalcanti, tanto os que est\u00e3o executando. quanto os que est\u00e3o na fila. Sendo que n\u00e3o havia nenhum job a ser mostrado. Posteriormente, o comando sbatch foi usado para submeter um job. Novamente, o comando squeue foi usado, mostrando que o job foi devidamente submetido e aguarda na fila para ser executado.","title":"Exemplo"},{"location":"intermediate/slurm_commands/#scancel","text":"Scancel \u00e9 utilizado para cancelar um job depois que ele foi submetido, podendo ele estar na fila de espera ou em execu\u00e7\u00e3o. O job ser\u00e1 interrompido de imediato, ent\u00e3o seja sempre cuidadoso ao usar esse comando, lembrando sempre que ao executar outro job voc\u00ea estar\u00e1 no final da fila. Entretando, voc\u00ea n\u00e3o precisa se preocupar que nenhum usu\u00e1rio ter\u00e1 acesso ao job de outro, sendo assim, n\u00e3o h\u00e1 riscos de algu\u00e9m cancelar job's que n\u00e3o s\u00e3o seus.","title":"scancel"},{"location":"intermediate/slurm_commands/#exemplo-scancel","text":"squeue -u scavalcanti JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 170926 cluster PascalAn scavalca PD 0 :00 1 ( Priority ) scancel 170926 squeue -u scavalcanti JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) No exemplo acima, foi utilizado o comando squeue como o par\u00e2metro -u para mostrar as informa\u00e7\u00f5es gerais dos jobs do usu\u00e1rio scavalcanti, tanto os que est\u00e3o executando. quanto os que est\u00e3o na fila. Sendo mostrado um \u00fanico job em espera. Posteriormente, o comando scancel foi usado para cancelar o \u00fanico job existente. Novamente, o comando squeue foi usado, mostrando que o job foi devidamente deletado.","title":"Exemplo: scancel"},{"location":"intermediate/superpc_introduction_part_3/","text":"Introdu\u00e7\u00e3o ao supercomputador - Parte 3 \u00b6 Nesse tutorial iremos aprender a fun\u00e7\u00e3o de algumas op\u00e7\u00f5es que podem ser inseridas nos scripts enviados para o supercomputador. Caso surja alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato atrav\u00e9s do e-mail atendimento\\ npad.ufrn.br (substituindo \\ por @ ). Introdu\u00e7\u00e3o ao supercomputador - Parte 3 Script para multithreading Script para utiliza\u00e7\u00e3o de v\u00e1rios n\u00f3s Compartilhamento dos n\u00f3s Escolha da qualidade de servi\u00e7o e Limite no uso do supercomputador QOS 1 QOS 2 preempt Receber e-mails sobre in\u00edcio e fim da execu\u00e7\u00e3o Definir a quantidade de mem\u00f3ria a ser utilizada Carregando softwares dispon\u00edveis Utilizando a sua pasta home Utilizando o scratch global Backfill e escolha do tempo de execu\u00e7\u00e3o Script para multithreading \u00b6 Por padr\u00e3o, quando n\u00e3o \u00e9 definido o n\u00famero de n\u00facleos a ser utilizado, o supercomputador executar\u00e1 o job em apenas um um n\u00facleo do n\u00f3. Para que seu programa execute em mais de um n\u00facleo, \u00e9 necess\u00e1rio definir no script #SBATCH --cpus-per-task , da seguinte forma: #!/bin/bash #SBATCH --job-name=multithreading_example #SBATCH --time=0-0:5 #SBATCH --cpus-per-task=32 #SBATCH --hint=compute_bound export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ./hello_threads 32 #32 threads A op\u00e7\u00e3o #SBATCH --hint=compute_bound muda a configura\u00e7\u00e3o para ser um thread por core. A op\u00e7\u00e3o #SBATCH --cpus-per-task=32 est\u00e1 definindo 32 cores ou n\u00facleos para esse job. A vari\u00e1vel de ambiente SLURM_CPUS_PER_TASK fornece a voc\u00ea o n\u00famero de cores que seu job ter\u00e1 durante a execu\u00e7\u00e3o do programa. Script para utiliza\u00e7\u00e3o de v\u00e1rios n\u00f3s \u00b6 Novamente, por padr\u00e3o, quando n\u00e3o \u00e9 definido o n\u00famero de n\u00f3s a ser utilizado, o supercomputador executar\u00e1 o job em apenas um n\u00f3 e em um n\u00facleo desse n\u00f3 . Para que seu programa execute em mais de um n\u00f3, \u00e9 necess\u00e1rio definir no script da seguinte forma: #!/bin/bash #SBATCH --nodes=2 #n\u00famero de n\u00f3s #SBATCH --ntasks=4 #n\u00famero total de tarefas #SBATCH --ntasks-per-node=2 #n\u00famero de tarefas por n\u00f3 srun prog1 #programa a ser executado. #srun: executa jobs em paralelo Onde #SBATCH --nodes indica a quantidade de n\u00f3s a ser utilizada, podendo tamb\u00e9m ser definido com #SBATCH -N . Tamb\u00e9m se pode definir o n\u00famero de tarefas por n\u00f3 e a quantidade de cpus por tarefas, essas configura\u00e7\u00f5es est\u00e3o relacionadas a paraleliza\u00e7\u00e3o com MPI . Para isso \u00e9 necess\u00e1rio primeiramente definir o n\u00famero de tarefas com a op\u00e7\u00e3o #SBATCH --ntasks ou #SBATCH -n . Com #SBATCH --ntasks-per-node \u00e9 definido as tarefas por n\u00f3. J\u00e1 #SBATCH --cpus-per-task define a quantidade de cpu por tarefas. Vale salientar que as duas \u00faltimas op\u00e7\u00f5es citadas n\u00e3o s\u00e3o dependentes. Com isso, pode ser que a n\u00f3 que esteja utilizando seja compartilhado com outros jobs. Compartilhamento dos n\u00f3s \u00b6 Outro padr\u00e3o do supercomputador \u00e9, ao submeter um job o n\u00f3 n\u00e3o ser\u00e1 reservado exclusivamente para aquele job, podendo ser alocado mais jobs dependendo da disponibilidade dos recursos naquele n\u00f3. Caso seu programa necessite de um n\u00f3 por completo, utilize a op\u00e7\u00e3o #SBATCH --exclusive . Por exemplo, para um programa que ser\u00e1 executado em paralelo, o desempenho do programa ser\u00e1 melhor se o programa puder utilizar os recursos por completo. Escolha da qualidade de servi\u00e7o e Limite no uso do supercomputador \u00b6 O usu\u00e1rio poder\u00e1 enviar m\u00faltiplos jobs para o supercomputador. Por\u00e9m, para permitir que mais pesquisadores compartilhem esse recurso com menos tempo de espera foram impostos limites no uso do supercomputador a partir da qualidade do servi\u00e7o em ingl\u00eas Quality of Service (QOS) utilizado. Comando a ser acrescentado no script de execu\u00e7\u00e3o: #SBATCH --qos=qosN #Subistitua N pelo tipo de QOS desejado QOS 1 \u00b6 O QOS 1 \u00e9 o QOS padr\u00e3o, com ele o usu\u00e1rio poder\u00e1 enviar at\u00e9 100 jobs para o supercomputador, sendo somente 4 jobs em execu\u00e7\u00e3o ou 256 n\u00facleos f\u00edsicos (4 n\u00f3s completos) em utiliza\u00e7\u00e3o, o que ocorrer primeiro. O job tem limite de tempo de at\u00e9 2 dias Ex.: #!/bin/bash #SBATCH --time=0-0:5 ./prog1 QOS 2 \u00b6 O QOS 2 \u00e9 mais indicado para mais jobs que utilizam um n\u00f3 inteiro ou jobs que utilizam alguns poucos n\u00f3s. Com ele, o usu\u00e1rio poder\u00e1 colocar 100 jobs na fila, mas apenas 1 job em execu\u00e7\u00e3o com at\u00e9 256 n\u00facleos f\u00edsicos (4 n\u00f3s completos) em utiliza\u00e7\u00e3o, o que ocorrer primeiro. Tendo o job o limite de tempo de at\u00e9 7 dias Ex.: #!/bin/bash #SBATCH --time=0-0:5 #SBATCH --qos=qos2 ./prog1 preempt \u00b6 Para trabalhos que necessitem rodar v\u00e1rios jobs simultaneamente , ou job que precisa de mais de at\u00e9 20 dias para ser executado. Recomenda-se utilizar preempt, nele o usu\u00e1rio pode deixar rodando at\u00e9 100 jobs simult\u00e2neos. No entanto, por falta de recurso o seus jobs poder\u00e3o ser cancelados a qualquer momento. Ex: #!/bin/bash #SBATCH --time=0-0:5 #SBATCH --qos=preempt ./prog1 Receber e-mails sobre in\u00edcio e fim da execu\u00e7\u00e3o \u00b6 Caso deseje receber notifica\u00e7\u00f5es por e-mail sobre in\u00edcio e fim de execu\u00e7\u00e3o, utiliza-se a op\u00e7\u00e3o --mail-type onde se pode definir que tipo de notifica\u00e7\u00e3o deseja receber. Se for definido ALL , as notifica\u00e7\u00f5es recebidas ser\u00e3o sobre BEGIN , END , FAIL , REQUEUE e STAGE_OUT . Caso deseje apenas um evento espec\u00edfico utilize uma das op\u00e7\u00f5es, sendo as op\u00e7\u00f5es dispon\u00edveis: NONE , BEGIN , END , FAIL , REQUEUE , STAGE_OUT , TIME_LITMIT , TIME_LIMIT_90 (alcan\u00e7ou 90% do tempo limite), TIME_LIMIT_50 e ARRAY_TASKS (enviar e-mail para cada array task). Tamb\u00e9m \u00e9 necess\u00e1rio definir o e-mail que ir\u00e1 receber as notifica\u00e7\u00f5es com #SBATCH --mail-user . #!/bin/bash #SBATCH --mail-user=meuemail@mail.com #SBATCH --mail-type=ALL ./prog1 Definir a quantidade de mem\u00f3ria a ser utilizada \u00b6 O supercomputador est\u00e1 configurado para atribuir, no m\u00ednimo, 4GB de mem\u00f3ria por n\u00facleo. Caso queira modificar a quantidade de mem\u00f3ria padr\u00e3o, voc\u00ea pode utilizar a op\u00e7\u00e3o #SBATCH --mem-per-cpu , como demonstrado no exemplo a seguir: #!/bin/bash #SBATCH --mem-per-cpu=1000 #SBATCH --cpus-per-task=3 ./prog1 O script do exemplo utiliza 3 cpus e para cada cpu \u00e9 reservado aproximadamente 1GB de mem\u00f3ria. Sendo que a multiplica\u00e7\u00e3o entre a quantidade de mem\u00f3ria por cpu e o n\u00famero de cpus usada n\u00e3o pode ultrapassar de 4GB, pois esse \u00e9 o limite de cada n\u00f3. O limite de cada n\u00f3 varia de acordo com a sua parti\u00e7\u00e3o. Um N\u00f3 pertencente a parti\u00e7\u00e3o cluster ou service , s\u00f3 pode utilizar ao total 4GB. Um n\u00f3 na parti\u00e7\u00e3o intel-256 e gpu pode usar at\u00e9 8 GB e intel-512 pode usar at\u00e9 16GB. Lembrando que a parti\u00e7\u00e3o test \u00e9 cluster mais service . Tamb\u00e9m h\u00e1 a op\u00e7\u00e3o #SBATCH --mem que j\u00e1 especifica a quantidade de mem\u00f3ria para o job por completo. #!/bin/bash #SBATCH --mem=3000 #SBATCH --cpus-per-task=3 ./prog1 Carregando softwares dispon\u00edveis \u00b6 A grande maioria dos softwares cient\u00edficos instalados no supercomputador s\u00e3o executados por m\u00f3dulo e, portanto, ser\u00e1 preciso carregar seus m\u00f3dulos. Para verificar quais m\u00f3dulos est\u00e3o dispon\u00edveis no supercomputador, digite no terminal do Linux o comando a seguir: module avail Depois de verificar os m\u00f3dulos dispon\u00edveis, voc\u00ea poder\u00e1 carregar aqueles dos quais voc\u00ea precisa utilizando um comando semelhante ao mostrado a seguir: module load libraries/zlib/1.2.11-intel-16.0.1 # Substitua libraries/zlib/1.2.11-intel-16.0.1 pelo m\u00f3dulo desejado, inserindo seu caminho completo fornecido pelo comando module avail Para verificar quais m\u00f3dulos voc\u00ea tem atualmente carregados, utilize o comando a seguir: module list Para limpar todos os m\u00f3dulos que voc\u00ea tem atualmente carregados, utilize o comando a seguir: module clear Os m\u00f3dulos podem ser adicionados ao script criado usando o editor de textos dispon\u00edvel de sua prefer\u00eancia, caso queira editar via linha comando, pode utilizar o editor Nano , basta voc\u00ea efetuar o devido carregamento dos m\u00f3dulos necess\u00e1rios, atrav\u00e9s dos comandos supracitados. Tome o cuidado apenas de, no script, carregar os m\u00f3dulos antes de fazer a execu\u00e7\u00e3o do programa propriamente dito. #!/bin/bash #SBATCH --time=0-0:5 module load nome-do-software #nome do software que aparecer\u00e1 ap\u00f3s usar o comando module avail nome-do-software prog1 Utilizando a sua pasta home \u00b6 Ao logar no supercomputador, voc\u00ea estar\u00e1 na pasta home. Essa pasta possui uma comunica\u00e7\u00e3o via rede com o supercomputador e j\u00e1 est\u00e1 configurada para, ao submeter os scripts para o supercomputador, ler os arquivos de entrada necess\u00e1rios para executar o job dessa pasta, assim como arquivo log de sa\u00edda que ser\u00e1 salvo nesse mesmo local. Ou seja, n\u00e3o \u00e9 necess\u00e1rio utilizar comandos extras ao utilizar essa pasta. Apenas as configura\u00e7\u00f5es b\u00e1sicas do script, como o tempo estimado de execu\u00e7\u00e3o do programa, quantidade de cpus, a fila em que deseja alocar o job e afins. Uma vez com o script pronto, \u00e9 s\u00f3 enviar o script com o comando sbatch . [ usuario@service0 ~ ] $ sbatch meu-script.sh Utilizando o scratch global \u00b6 O NPAD utiliza o BeeGFS para scratch global, ou seja, armazenamento tempor\u00e1rio de arquivos, compartilhado com todos os n\u00f3s. Como usu\u00e1rio do sistema, tudo que voc\u00ea precisa fazer \u00e9 copiar os arquivos relevantes pra pasta ~/scratch. Voc\u00ea pode inclusive submeter jobs de l\u00e1. Essa pasta na verdade \u00e9 um symlink que aponta pra /scratch/global/usuario. Num job script, voc\u00ea tamb\u00e9m pode usar a vari\u00e1vel de ambiente $SCRATCH_GLOBAL para pegar a localiza\u00e7\u00e3o dessa pasta. Acessando a pasta /scratch/global destinada ao seu usu\u00e1rio: [ usuario@service0 ~ ] $ cd ~/scratch ou [ usuario@service0 ~ ] $ cd $SCRATCH_GLOBAL Exemplo de utiliza\u00e7\u00e3o: #!/bin/bash #SBATCH --time=0-0:5 #move os arquivos com os par\u00e2metros de entrada para a pasta /scratch/global mv entrada.in $SCRATCH_GLOBAL cd $SCRATCH_GLOBAL #execu\u00e7\u00e3o normal do seu programa ./prog #move os arquivos de sa\u00edda para o diret\u00f3rio /home/usuario mv saida.out /home/usuario/ O scratch global deve ser considerada como uma pasta tempor\u00e1ria, e os arquivos l\u00e1 podem ser exclu\u00eddos se a equipe do NPAD determinar que eles n\u00e3o est\u00e3o sendo usados h\u00e1 muito tempo. Portanto, n\u00e3o mantenha arquivos importantes nela. A boa pr\u00e1tica \u00e9 usar a scratch global para a gera\u00e7\u00e3o de arquivos tempor\u00e1rios e depois copiar os resultados importantes para fora dela. Backfill e escolha do tempo de execu\u00e7\u00e3o \u00b6 O Supercomputador por padr\u00e3o aloca os jobs de acordo com suas prioridades, ou seja, jobs com alta prioridade executam antes. Por\u00e9m a op\u00e7\u00e3o backfill \u00e9 utilizada para uma melhor utiliza\u00e7\u00e3o do sistema. Com isso \u00e9 poss\u00edvel que jobs com prioridades menores sejam alocados e executados antes de jobs com alta prioridade. Isso acontece apenas no caso em que o tempo de in\u00edcio esperado do jobs de alta prioridade for maior que o tempo limite de execu\u00e7\u00e3o do job de baixa prioridade. O tempo de in\u00edcio esperado depende da finaliza\u00e7\u00e3o dos jobs em execu\u00e7\u00e3o e o tempo limite de execu\u00e7\u00e3o depende da op\u00e7\u00e3o --time no script do job. Logo, uma escolha razo\u00e1vel do tempo estimado de execu\u00e7\u00e3o pode fazer com que o job comece a executar mais r\u00e1pido. Ex.: #!/bin/bash #SBATCH --time=0-0:5 #Formato padr\u00e3o: dias-horas:minutos Para executar programas em paralelo no supercomputador, leia os tutorias de OpenMP e MPI .","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte&nbsp;3"},{"location":"intermediate/superpc_introduction_part_3/#introducao-ao-supercomputador-parte3","text":"Nesse tutorial iremos aprender a fun\u00e7\u00e3o de algumas op\u00e7\u00f5es que podem ser inseridas nos scripts enviados para o supercomputador. Caso surja alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato atrav\u00e9s do e-mail atendimento\\ npad.ufrn.br (substituindo \\ por @ ). Introdu\u00e7\u00e3o ao supercomputador - Parte 3 Script para multithreading Script para utiliza\u00e7\u00e3o de v\u00e1rios n\u00f3s Compartilhamento dos n\u00f3s Escolha da qualidade de servi\u00e7o e Limite no uso do supercomputador QOS 1 QOS 2 preempt Receber e-mails sobre in\u00edcio e fim da execu\u00e7\u00e3o Definir a quantidade de mem\u00f3ria a ser utilizada Carregando softwares dispon\u00edveis Utilizando a sua pasta home Utilizando o scratch global Backfill e escolha do tempo de execu\u00e7\u00e3o","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte&nbsp;3"},{"location":"intermediate/superpc_introduction_part_3/#script-para-multithreading","text":"Por padr\u00e3o, quando n\u00e3o \u00e9 definido o n\u00famero de n\u00facleos a ser utilizado, o supercomputador executar\u00e1 o job em apenas um um n\u00facleo do n\u00f3. Para que seu programa execute em mais de um n\u00facleo, \u00e9 necess\u00e1rio definir no script #SBATCH --cpus-per-task , da seguinte forma: #!/bin/bash #SBATCH --job-name=multithreading_example #SBATCH --time=0-0:5 #SBATCH --cpus-per-task=32 #SBATCH --hint=compute_bound export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ./hello_threads 32 #32 threads A op\u00e7\u00e3o #SBATCH --hint=compute_bound muda a configura\u00e7\u00e3o para ser um thread por core. A op\u00e7\u00e3o #SBATCH --cpus-per-task=32 est\u00e1 definindo 32 cores ou n\u00facleos para esse job. A vari\u00e1vel de ambiente SLURM_CPUS_PER_TASK fornece a voc\u00ea o n\u00famero de cores que seu job ter\u00e1 durante a execu\u00e7\u00e3o do programa.","title":"Script para multithreading"},{"location":"intermediate/superpc_introduction_part_3/#script-para-utilizacao-de-varios-nos","text":"Novamente, por padr\u00e3o, quando n\u00e3o \u00e9 definido o n\u00famero de n\u00f3s a ser utilizado, o supercomputador executar\u00e1 o job em apenas um n\u00f3 e em um n\u00facleo desse n\u00f3 . Para que seu programa execute em mais de um n\u00f3, \u00e9 necess\u00e1rio definir no script da seguinte forma: #!/bin/bash #SBATCH --nodes=2 #n\u00famero de n\u00f3s #SBATCH --ntasks=4 #n\u00famero total de tarefas #SBATCH --ntasks-per-node=2 #n\u00famero de tarefas por n\u00f3 srun prog1 #programa a ser executado. #srun: executa jobs em paralelo Onde #SBATCH --nodes indica a quantidade de n\u00f3s a ser utilizada, podendo tamb\u00e9m ser definido com #SBATCH -N . Tamb\u00e9m se pode definir o n\u00famero de tarefas por n\u00f3 e a quantidade de cpus por tarefas, essas configura\u00e7\u00f5es est\u00e3o relacionadas a paraleliza\u00e7\u00e3o com MPI . Para isso \u00e9 necess\u00e1rio primeiramente definir o n\u00famero de tarefas com a op\u00e7\u00e3o #SBATCH --ntasks ou #SBATCH -n . Com #SBATCH --ntasks-per-node \u00e9 definido as tarefas por n\u00f3. J\u00e1 #SBATCH --cpus-per-task define a quantidade de cpu por tarefas. Vale salientar que as duas \u00faltimas op\u00e7\u00f5es citadas n\u00e3o s\u00e3o dependentes. Com isso, pode ser que a n\u00f3 que esteja utilizando seja compartilhado com outros jobs.","title":"Script para utiliza\u00e7\u00e3o de v\u00e1rios n\u00f3s"},{"location":"intermediate/superpc_introduction_part_3/#compartilhamento-dos-nos","text":"Outro padr\u00e3o do supercomputador \u00e9, ao submeter um job o n\u00f3 n\u00e3o ser\u00e1 reservado exclusivamente para aquele job, podendo ser alocado mais jobs dependendo da disponibilidade dos recursos naquele n\u00f3. Caso seu programa necessite de um n\u00f3 por completo, utilize a op\u00e7\u00e3o #SBATCH --exclusive . Por exemplo, para um programa que ser\u00e1 executado em paralelo, o desempenho do programa ser\u00e1 melhor se o programa puder utilizar os recursos por completo.","title":"Compartilhamento dos n\u00f3s"},{"location":"intermediate/superpc_introduction_part_3/#escolha-da-qualidade-de-servico-e-limite-no-uso-do-supercomputador","text":"O usu\u00e1rio poder\u00e1 enviar m\u00faltiplos jobs para o supercomputador. Por\u00e9m, para permitir que mais pesquisadores compartilhem esse recurso com menos tempo de espera foram impostos limites no uso do supercomputador a partir da qualidade do servi\u00e7o em ingl\u00eas Quality of Service (QOS) utilizado. Comando a ser acrescentado no script de execu\u00e7\u00e3o: #SBATCH --qos=qosN #Subistitua N pelo tipo de QOS desejado","title":"Escolha da qualidade de servi\u00e7o e Limite no uso do supercomputador"},{"location":"intermediate/superpc_introduction_part_3/#qos-1","text":"O QOS 1 \u00e9 o QOS padr\u00e3o, com ele o usu\u00e1rio poder\u00e1 enviar at\u00e9 100 jobs para o supercomputador, sendo somente 4 jobs em execu\u00e7\u00e3o ou 256 n\u00facleos f\u00edsicos (4 n\u00f3s completos) em utiliza\u00e7\u00e3o, o que ocorrer primeiro. O job tem limite de tempo de at\u00e9 2 dias Ex.: #!/bin/bash #SBATCH --time=0-0:5 ./prog1","title":"QOS 1"},{"location":"intermediate/superpc_introduction_part_3/#qos-2","text":"O QOS 2 \u00e9 mais indicado para mais jobs que utilizam um n\u00f3 inteiro ou jobs que utilizam alguns poucos n\u00f3s. Com ele, o usu\u00e1rio poder\u00e1 colocar 100 jobs na fila, mas apenas 1 job em execu\u00e7\u00e3o com at\u00e9 256 n\u00facleos f\u00edsicos (4 n\u00f3s completos) em utiliza\u00e7\u00e3o, o que ocorrer primeiro. Tendo o job o limite de tempo de at\u00e9 7 dias Ex.: #!/bin/bash #SBATCH --time=0-0:5 #SBATCH --qos=qos2 ./prog1","title":"QOS 2"},{"location":"intermediate/superpc_introduction_part_3/#preempt","text":"Para trabalhos que necessitem rodar v\u00e1rios jobs simultaneamente , ou job que precisa de mais de at\u00e9 20 dias para ser executado. Recomenda-se utilizar preempt, nele o usu\u00e1rio pode deixar rodando at\u00e9 100 jobs simult\u00e2neos. No entanto, por falta de recurso o seus jobs poder\u00e3o ser cancelados a qualquer momento. Ex: #!/bin/bash #SBATCH --time=0-0:5 #SBATCH --qos=preempt ./prog1","title":"preempt"},{"location":"intermediate/superpc_introduction_part_3/#receber-e-mails-sobre-inicio-e-fim-da-execucao","text":"Caso deseje receber notifica\u00e7\u00f5es por e-mail sobre in\u00edcio e fim de execu\u00e7\u00e3o, utiliza-se a op\u00e7\u00e3o --mail-type onde se pode definir que tipo de notifica\u00e7\u00e3o deseja receber. Se for definido ALL , as notifica\u00e7\u00f5es recebidas ser\u00e3o sobre BEGIN , END , FAIL , REQUEUE e STAGE_OUT . Caso deseje apenas um evento espec\u00edfico utilize uma das op\u00e7\u00f5es, sendo as op\u00e7\u00f5es dispon\u00edveis: NONE , BEGIN , END , FAIL , REQUEUE , STAGE_OUT , TIME_LITMIT , TIME_LIMIT_90 (alcan\u00e7ou 90% do tempo limite), TIME_LIMIT_50 e ARRAY_TASKS (enviar e-mail para cada array task). Tamb\u00e9m \u00e9 necess\u00e1rio definir o e-mail que ir\u00e1 receber as notifica\u00e7\u00f5es com #SBATCH --mail-user . #!/bin/bash #SBATCH --mail-user=meuemail@mail.com #SBATCH --mail-type=ALL ./prog1","title":"Receber e-mails sobre in\u00edcio e fim da execu\u00e7\u00e3o"},{"location":"intermediate/superpc_introduction_part_3/#definir-a-quantidade-de-memoria-a-ser-utilizada","text":"O supercomputador est\u00e1 configurado para atribuir, no m\u00ednimo, 4GB de mem\u00f3ria por n\u00facleo. Caso queira modificar a quantidade de mem\u00f3ria padr\u00e3o, voc\u00ea pode utilizar a op\u00e7\u00e3o #SBATCH --mem-per-cpu , como demonstrado no exemplo a seguir: #!/bin/bash #SBATCH --mem-per-cpu=1000 #SBATCH --cpus-per-task=3 ./prog1 O script do exemplo utiliza 3 cpus e para cada cpu \u00e9 reservado aproximadamente 1GB de mem\u00f3ria. Sendo que a multiplica\u00e7\u00e3o entre a quantidade de mem\u00f3ria por cpu e o n\u00famero de cpus usada n\u00e3o pode ultrapassar de 4GB, pois esse \u00e9 o limite de cada n\u00f3. O limite de cada n\u00f3 varia de acordo com a sua parti\u00e7\u00e3o. Um N\u00f3 pertencente a parti\u00e7\u00e3o cluster ou service , s\u00f3 pode utilizar ao total 4GB. Um n\u00f3 na parti\u00e7\u00e3o intel-256 e gpu pode usar at\u00e9 8 GB e intel-512 pode usar at\u00e9 16GB. Lembrando que a parti\u00e7\u00e3o test \u00e9 cluster mais service . Tamb\u00e9m h\u00e1 a op\u00e7\u00e3o #SBATCH --mem que j\u00e1 especifica a quantidade de mem\u00f3ria para o job por completo. #!/bin/bash #SBATCH --mem=3000 #SBATCH --cpus-per-task=3 ./prog1","title":"Definir a quantidade de mem\u00f3ria a ser utilizada"},{"location":"intermediate/superpc_introduction_part_3/#carregando-softwares-disponiveis","text":"A grande maioria dos softwares cient\u00edficos instalados no supercomputador s\u00e3o executados por m\u00f3dulo e, portanto, ser\u00e1 preciso carregar seus m\u00f3dulos. Para verificar quais m\u00f3dulos est\u00e3o dispon\u00edveis no supercomputador, digite no terminal do Linux o comando a seguir: module avail Depois de verificar os m\u00f3dulos dispon\u00edveis, voc\u00ea poder\u00e1 carregar aqueles dos quais voc\u00ea precisa utilizando um comando semelhante ao mostrado a seguir: module load libraries/zlib/1.2.11-intel-16.0.1 # Substitua libraries/zlib/1.2.11-intel-16.0.1 pelo m\u00f3dulo desejado, inserindo seu caminho completo fornecido pelo comando module avail Para verificar quais m\u00f3dulos voc\u00ea tem atualmente carregados, utilize o comando a seguir: module list Para limpar todos os m\u00f3dulos que voc\u00ea tem atualmente carregados, utilize o comando a seguir: module clear Os m\u00f3dulos podem ser adicionados ao script criado usando o editor de textos dispon\u00edvel de sua prefer\u00eancia, caso queira editar via linha comando, pode utilizar o editor Nano , basta voc\u00ea efetuar o devido carregamento dos m\u00f3dulos necess\u00e1rios, atrav\u00e9s dos comandos supracitados. Tome o cuidado apenas de, no script, carregar os m\u00f3dulos antes de fazer a execu\u00e7\u00e3o do programa propriamente dito. #!/bin/bash #SBATCH --time=0-0:5 module load nome-do-software #nome do software que aparecer\u00e1 ap\u00f3s usar o comando module avail nome-do-software prog1","title":"Carregando softwares dispon\u00edveis"},{"location":"intermediate/superpc_introduction_part_3/#utilizando-a-sua-pasta-home","text":"Ao logar no supercomputador, voc\u00ea estar\u00e1 na pasta home. Essa pasta possui uma comunica\u00e7\u00e3o via rede com o supercomputador e j\u00e1 est\u00e1 configurada para, ao submeter os scripts para o supercomputador, ler os arquivos de entrada necess\u00e1rios para executar o job dessa pasta, assim como arquivo log de sa\u00edda que ser\u00e1 salvo nesse mesmo local. Ou seja, n\u00e3o \u00e9 necess\u00e1rio utilizar comandos extras ao utilizar essa pasta. Apenas as configura\u00e7\u00f5es b\u00e1sicas do script, como o tempo estimado de execu\u00e7\u00e3o do programa, quantidade de cpus, a fila em que deseja alocar o job e afins. Uma vez com o script pronto, \u00e9 s\u00f3 enviar o script com o comando sbatch . [ usuario@service0 ~ ] $ sbatch meu-script.sh","title":"Utilizando a sua pasta home"},{"location":"intermediate/superpc_introduction_part_3/#utilizando-o-scratch-global","text":"O NPAD utiliza o BeeGFS para scratch global, ou seja, armazenamento tempor\u00e1rio de arquivos, compartilhado com todos os n\u00f3s. Como usu\u00e1rio do sistema, tudo que voc\u00ea precisa fazer \u00e9 copiar os arquivos relevantes pra pasta ~/scratch. Voc\u00ea pode inclusive submeter jobs de l\u00e1. Essa pasta na verdade \u00e9 um symlink que aponta pra /scratch/global/usuario. Num job script, voc\u00ea tamb\u00e9m pode usar a vari\u00e1vel de ambiente $SCRATCH_GLOBAL para pegar a localiza\u00e7\u00e3o dessa pasta. Acessando a pasta /scratch/global destinada ao seu usu\u00e1rio: [ usuario@service0 ~ ] $ cd ~/scratch ou [ usuario@service0 ~ ] $ cd $SCRATCH_GLOBAL Exemplo de utiliza\u00e7\u00e3o: #!/bin/bash #SBATCH --time=0-0:5 #move os arquivos com os par\u00e2metros de entrada para a pasta /scratch/global mv entrada.in $SCRATCH_GLOBAL cd $SCRATCH_GLOBAL #execu\u00e7\u00e3o normal do seu programa ./prog #move os arquivos de sa\u00edda para o diret\u00f3rio /home/usuario mv saida.out /home/usuario/ O scratch global deve ser considerada como uma pasta tempor\u00e1ria, e os arquivos l\u00e1 podem ser exclu\u00eddos se a equipe do NPAD determinar que eles n\u00e3o est\u00e3o sendo usados h\u00e1 muito tempo. Portanto, n\u00e3o mantenha arquivos importantes nela. A boa pr\u00e1tica \u00e9 usar a scratch global para a gera\u00e7\u00e3o de arquivos tempor\u00e1rios e depois copiar os resultados importantes para fora dela.","title":"Utilizando o scratch global"},{"location":"intermediate/superpc_introduction_part_3/#backfill-e-escolha-do-tempo-de-execucao","text":"O Supercomputador por padr\u00e3o aloca os jobs de acordo com suas prioridades, ou seja, jobs com alta prioridade executam antes. Por\u00e9m a op\u00e7\u00e3o backfill \u00e9 utilizada para uma melhor utiliza\u00e7\u00e3o do sistema. Com isso \u00e9 poss\u00edvel que jobs com prioridades menores sejam alocados e executados antes de jobs com alta prioridade. Isso acontece apenas no caso em que o tempo de in\u00edcio esperado do jobs de alta prioridade for maior que o tempo limite de execu\u00e7\u00e3o do job de baixa prioridade. O tempo de in\u00edcio esperado depende da finaliza\u00e7\u00e3o dos jobs em execu\u00e7\u00e3o e o tempo limite de execu\u00e7\u00e3o depende da op\u00e7\u00e3o --time no script do job. Logo, uma escolha razo\u00e1vel do tempo estimado de execu\u00e7\u00e3o pode fazer com que o job comece a executar mais r\u00e1pido. Ex.: #!/bin/bash #SBATCH --time=0-0:5 #Formato padr\u00e3o: dias-horas:minutos Para executar programas em paralelo no supercomputador, leia os tutorias de OpenMP e MPI .","title":"Backfill e escolha do tempo de execu\u00e7\u00e3o"}]}