{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tutoriais do NPAD","text":"<p>Aqui os usu\u00e1rios do N\u00facleo de Processamento de Alto Desempenho (NPAD) encontram tutoriais para compreender o uso e funcionamento do supercomputador da UFRN.</p> <p>Encontrou algum problema nos tutoriais por favor crie uma issue em github.com/NPAD-UFRN/Tutorials/issues</p>"},{"location":"advanced/job_array/","title":"Tutorial de Job Array","text":"<ul> <li>Para que serve um array de jobs?</li> <li>Exemplo</li> <li>Deletando jobs</li> <li>Mais Exemplos</li> <li>Acessando vari\u00e1veis de ambiente</li> <li>Nome dos arquivos de sa\u00edda e erro</li> <li>A diretiva --array</li> </ul>"},{"location":"advanced/job_array/#para-que-serve-um-array-de-jobs","title":"Para que serve um array de jobs?","text":"<p>O array de jobs ou de tarefas (tasks) permite rodar um mesmo job v\u00e1rias vezes com varia\u00e7\u00f5es em cada rodada, por exemplo, na entrada de dados ou nas configura\u00e7\u00f5es.</p> <p>Para fazer isso, usa-se a diretiva <code>--array</code>  no script de job. Por exemplo,</p> <pre><code>#SBATCH --array=1-10\n</code></pre> <p>Isto far\u00e1 com que o mesmo script de job seja executado 10 vezes onde cada um dos 10 jobs \u00e9 independente um do outro. </p> <p>Em geral, a diretiva <code>--array</code> \u00e9 usada em conjunto com vari\u00e1vel de ambiente <code>SLURM_ARRAY_TASK_ID</code> que fornece o indice do array de jobs, ou seja, o \u00edndice do job em execu\u00e7\u00e3o. Este \u00edndice pode ser usado para selecionar dados de entrada, configura\u00e7\u00f5es, comandos etc.</p>"},{"location":"advanced/job_array/#exemplo","title":"Exemplo","text":"<p>Suponha a situa\u00e7\u00e3o onde um programa, digamos, <code>meuprograma</code>, deve ser executado para cada um dos dez arquivos de entrada: entrada1.txt,  entrada2.txt, entrada3.txt etc.  Ao inv\u00e9s de criar 10 scripts de jobs diferentes para cada arquivo, podemos usar um \u00fanico script de array de jobs, como este:</p> <pre><code>#!/bin/bash \n#SBATCH --partition=amd-512  # parti\u00e7\u00e3o para a qual o job \u00e9 enviado\n#SBATCH --time=0-0:10\n#SBATCH --array=1-10\n\n./meuprograma entrada${SLURM_ARRAY_TASK_ID}.txt\n</code></pre> <p>A vari\u00e1vel  <code>$SLURM_ARRAY_TASK_ID</code>  fornece o \u00edndice do array de jobs.  Ela \u00e9 usada na \u00faltima linha para selecionar o arquivo de entrada para o programa <code>meuprograma</code>. Digamos que o script tenha nome de <code>script_job_array.sh</code>. Ent\u00e3o submetemos este script de arrays de jobs:</p> <pre><code>$ sbatch script_job_array.sh \nSubmitted batch job 123456\n</code></pre> <p>E para visualizar como est\u00e3o os Jobs na fila de execu\u00e7\u00e3o, digite:</p> <pre><code>$ squeue -u $USER\n\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          123456_1   cluster job_arra  usuario  R       0:51      1 r1i2n10\n          123456_2   cluster job_arra  usuario  R       0:52      1 r1i2n10\n          123456_3   cluster job_arra  usuario  R       0:41      1 r1i2n10\n          123456_4   cluster job_arra  usuario  R       0:54      1 r1i2n10\n          123456_5   cluster job_arra  usuario  R       0:51      1 r1i2n10\n          123456_6   cluster job_arra  usuario  R       0:51      1 r1i2n10\n          123456_7   cluster job_arra  usuario  R       0:55      1 r1i2n10\n          123456_8   cluster job_arra  usuario  R       0:50      1 r1i2n10\n          123456_9   cluster job_arra  usuario  R       0:52      1 r1i2n10\n</code></pre> <p>Nota-se que foi criado um job array de 10 jobs independentes e como um jobID diferenciado apenas pelo \u00edndice do array de jobs.  </p> <p>Nota</p> <p>As especifica\u00e7\u00f5es no script do array de jobs tais como --nodes, --cpus-per-task, --mem, etc refere-se apenas a execu\u00e7\u00e3o de um \u00fanico job do array de jobs. Por isso, n\u00e3o precisamos alocar recursos para todos os jobs do array, mas somente para um \u00fanico job e o slurm repetir\u00e1 a mesma aloca\u00e7\u00e3o para todas os outros.</p>"},{"location":"advanced/job_array/#deletando-jobs","title":"Deletando jobs","text":"<p>Para deletar um array de jobs inteiro, digite <code>scancel</code> seguido do job ID do array de jobs. Por exemplo</p> <pre><code>$ scancel 123456\n</code></pre> <p>E para deletar um job espec\u00edfico, use <code>scancel</code> seguido do job ID do job. Por exemplo:</p> <pre><code>$ scancel 123456_5\n</code></pre>"},{"location":"advanced/job_array/#mais-exemplos","title":"Mais Exemplos","text":"<p>Nem sempre os arquivos de entrada est\u00e3o numerados sequencialmente como no exemplo acima. Supondo que eles est\u00e3o na pasta <code>/path/to/input</code>. Ent\u00e3o pode-se usar o seguinte script:</p> <pre><code>#!/bin/bash \n#SBATCH --partition=amd-512  # parti\u00e7\u00e3o para a qual o job \u00e9 enviado\n#SBATCH --time=0-0:10\n#SBATCH --array=1-5\n\nINPUT_PATH=/path/to/input\n$entrada=$(ls ${INPUT_PATH}/*.txt | sed -n ${SLURM_ARRAY_TASK_ID}p)\n./meuprograma $entrada\n</code></pre> <p>Se ao inv\u00e9s de arquivos de entrada, o programa recebe um argumento de linha de comando. Ent\u00e3o pode-se salvar os argumentos em um arquivo <code>lista.txt</code></p> <pre><code>$ cat lista.txt\n1 23 3.1 2 1.2\n</code></pre> <p>e usar o seguinte script de array de jobs</p> <pre><code>#!/bin/bash \n#SBATCH --partition=amd-512 \n#SBATCH --time=0-0:10\n#SBATCH --array=1-5\n\nARGS=($(&lt;lista.txt))\nARG=${ARGS[${SLURM_ARRAY_TASK_ID}]}\n./meuprograma -n $ARG\n</code></pre> <p>Se o programa recebe n argumentos, como neste exemplo:</p> <pre><code>./meuprograma --type 1 --name jupiter --factor 4.1\n</code></pre> <p>pode-se salvar os argumentos em um arquivo com n argumentos por linha. Neste caso com n=3 argumentos por linha, por exemplo</p> <pre><code>$ cat argumentos.txt \n\n1 jupiter 4.1 \n3 saturno 4.5 \n2 faraday 3.0 \n</code></pre> <p>E ent\u00e3o usar:</p> <pre><code>#!/bin/bash \n#SBATCH --partition=amd-512\n#SBATCH --time=0-0:10\n#SBATCH --array=1-5\n\nIFS=$'\\n' read -d '' -r -a ARGS &lt; argumentos.txt\nIFS=$' ' read -d '' -r -a ARG &lt;&lt;&lt; ${ARGS[${SLURM_ARRAY_TASK_ID}]}\n./meuprograma --type ${ARG[0]} --name ${ARG[1]} --factor ${ARG[2]}\n</code></pre>"},{"location":"advanced/job_array/#acessando-variaveis-de-ambiente","title":"Acessando vari\u00e1veis de ambiente","text":"<p>As linguagens de programa\u00e7\u00e3o, em geral, possuem comandos para ler a vari\u00e1veis de ambiente. Por exemplo, para ler vari\u00e1vel  <code>SLURM_ARRAY_TASK_ID</code> </p> <p>Em Python:</p> <pre><code>import os\ntask_id = os.getenv('SLURM_ARRAY_TASK_ID')\n</code></pre> <p>Em R:</p> <pre><code>task_id &lt;- Sys.getenv(\"SLURM_ARRAY_TASK_ID\")\n</code></pre> <p>Em C:</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char** argv) {\n    char* task_id = getenv(\"SLURM_ARRAY_TASK_ID\");\n    printf(\"SLURM_ARRAY_TASK_ID: %s\\n\", task_id ? task_id : \"null\");\n    return 0;\n}\n</code></pre> <p>Em C++:</p> <pre><code>#include &lt;cstdlib&gt;\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    char* task_id = getenv(\"SLURM_ARRAY_TASK_ID\");\n    if (task_id != NULL) {\n        cout &lt;&lt; \"SLURM_ARRAY_TASK_ID: \" &lt;&lt; task_id &lt;&lt; endl;\n    }\n    return 0;\n}\n</code></pre> <p>Em Fortran:</p> <pre><code>program exemplo\ncharacter task_id*4\ncall get_environment_variable ('SLURM_ARRAY_TASK_ID', task_id)\nwrite (*,*) 'task id = ', task_id\nend\n</code></pre>"},{"location":"advanced/job_array/#nome-dos-arquivos-de-saida-e-erro","title":"Nome dos arquivos de sa\u00edda e erro","text":"<p>Nos scripts de job array, usa-se os c\u00f3digos <code>%A</code> e <code>%a</code> para indicar o job ID do array de jobs e o \u00edndice do job, respectivamente. Por exemplo </p> <pre><code>#SBATCH --output=saida_%A_%a.out\n#SBATCH --error=erros_%A_%a.error\n</code></pre> <p>Se o c\u00f3digo <code>%a</code> for omitido, as sa\u00eddas de todas os jobs do array ser\u00e3o escritas no mesmo arquivo.</p>"},{"location":"advanced/job_array/#a-diretiva-array","title":"A diretiva --array","text":"<p>Pode-se especificar os \u00edndices de jobs usando a seguinte forma geral  <code>--array=x-y</code>. Por exemplo, para gerar \u00edndices entre 10 e 20, use</p> <pre><code>SBATCH --array=10-15\n</code></pre> <p>Neste caso ser\u00e1 gerado 6 jobs com \u00edndices 10, 11, 12, 13, 14 e 15.</p> <p>Para gerar \u00edndices com incrementos maiores que um use a forma <code>--array=x-y:passo</code> onde passo \u00e9 o tamanho do incremento. Por exemplo</p> <pre><code>SBATCH --array=10-20:2\n</code></pre> <p>Neste caso ser\u00e1 \u00edndices espa\u00e7ados com incremento igual a dois: 10, 12, 14, 16, 18 e 20.</p> <p>\u00c9 tamb\u00e9m poss\u00edvel especificar individualmente os \u00edndices. Por exemplo</p> <pre><code>SBATCH --array=3,5,8,10\n</code></pre> <p>Um exemplo mais complexo seria:</p> <pre><code>SBATCH --array=3,5,11-13,22-26:2\n</code></pre> <p>O operador <code>%</code> \u00e9 usado para limitar o n\u00fameros de jobs que s\u00e3o executados simultaneamente. Por exemplo:</p> <pre><code>SBATCH --array=1-10%3\n</code></pre> <p>Aqui ser\u00e3o submetidos 10 jobs, mas, no m\u00e1ximo tr\u00eas deles ser\u00e3o executados simultaneamente. Ao visualizarmos os jobs usando <code>squeue</code>, veremos que no m\u00e1ximo tr\u00eas em execu\u00e7\u00e3o e o restante da fila de espera. Por exemplo,</p> <pre><code>$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n   123456_[4-10%3]   cluster job_arra usuario  PD       0:00      1 (JobArrayTaskLimit)\n          123456_1   cluster job_arra usuario   R       0:20      1 r1i3n2\n          123456_2   cluster job_arra usuario   R       0:20      1 r1i3n2\n          123456_3   cluster job_arra usuario   R       0:20      1 r1i3n2\n</code></pre>"},{"location":"advanced/jupyter/","title":"Usando o Jupyter no supercomputador","text":"<p>O Jupyter \u00e9 uma aplica\u00e7\u00e3o web de c\u00f3digo-fonte aberto que permite voc\u00ea criar e compartilhar documentos (chamados de notebooks) que cont\u00e9m c\u00f3digo, equa\u00e7\u00f5es, visualiza\u00e7\u00f5es e texto.</p> <p>Para usar o jupyter no supercomputador, deve-se criar um script semelhante ao mostrado abaixo:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512 #parti\u00e7\u00e3o para a qual o job \u00e9 enviado\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=10      #Quantidade de n\u00facleos\n#SBATCH --time=1-00:00          #Tempo m\u00e1ximo do job no formato DIAS-HORAS:MINUTOS\n#SBATCH --hint=compute_bound\n\n# Execute conda activate forge\n# antes de submeter este script de job\n\n## Parametros iniciais\nXDG_RUNTIME_DIR=\"\"\nipnport=$(shuf -i8000-9999 -n1)\nipnip=$(hostname -i)\n\n## Imprime na sa\u00edda slurm-{jobid}.out\necho -e \"\n    Copy/Paste this in your local terminal to ssh tunnel with remote\n    Copie e cole no terminal local da sua m\u00e1quina o comando abaixo\n    -----------------------------------------------------------------\n\n    ssh -N -L $ipnport:$ipnip:$ipnport -p4422 $USER@sc2.npad.ufrn.br\n\n    -----------------------------------------------------------------\n    \"\n\n## start an ipcluster instance and launch jupyter server\n## Inicia servidor Jupyter\n\njupyter-lab --no-browser --port=$ipnport --ip=$ipnip  \n</code></pre> <p>No nosso exemplo, o script foi salvo com nome script_jupyter.sh.</p> <p>Com o script criado, siga as seguintes etapas para acessar o jupyter no supercomputador. </p> <p>Primeiro, ative o conda forge no terminal do supercomputador. Digite:</p> <pre><code>$ conda activate forge\n</code></pre> <p>Envie o job para fila de execu\u00e7\u00e3o com o comando sbatch:</p> <pre><code>$ sbatch script-jupyter.sh \n\nSubmitted batch job JOBID \n</code></pre> <p>JOBID \u00e9 n\u00famero do job. Agora, verifique se o seu job est\u00e1 rodando com o comando squeue:</p> <pre><code>$ squeue -lu $USER\n</code></pre> <p>Caso o job de c\u00f3digo JOBID esteja com estado R (RUNNING) na coluna STATE, o job j\u00e1 foi iniciado e podemos seguir em diante. </p> <p>Use o comando cat slurm-JOBID.out (substitua JOBID com o n\u00famero do job). <pre><code>$ cat slurm-JOBID.out\n</code></pre></p> <p>Aparecer\u00e1 uma mensagem contendo duas informa\u00e7\u00f5es que voc\u00ea deveria copiar. A primeira \u00e9 a seguinte:</p> <pre><code>ssh -N -L PORTA:IP:PORTA -p4422 NOMEDOUSUARIO@sc2.npad.ufrn.br\n</code></pre> <p>Trata-se de um comando para ser executado no terminal de seu computador. Nesse comando, PORTA \u00e9 um valor n\u00famerico, IP s\u00e3o valores num\u00e9ricos separados por ponto e NOMEDOUSUARIO \u00e9 o nome do seu usu\u00e1rio no supercomputador. Copie e cole e execute ele no terminal do seu computador. O terminal ficar\u00e1 travado.</p> <p>A segunda informa\u00e7\u00e3o \u00e9 a seguinte:  </p> <pre><code>http://127.0.0.1:PORTA/lab?token=TOKEN \n</code></pre> <p>Copie e cole no seu navegador e o jupyter ir\u00e1 aparecer. </p> <p>Quando terminar de usar o jupyter, destrave o terminal do seu computador com Ctrl+C. E tamb\u00e9m cancele o job do supercomputador:</p> <pre><code>$ scancel JOBID \n</code></pre>"},{"location":"advanced/mpi/","title":"Tutorial de MPI","text":"<p>Nesse tutorial iremos aprender a conduzir a execu\u00e7\u00e3o de aplica\u00e7\u00f5es utilizando o Intel MPI no supercomputador, uma das melhores implementa\u00e7\u00f5es MPI da ind\u00fastria, de maneira simples e pr\u00e1tica.</p> <p>O MPI \u00e9 um padr\u00e3o de troca de mensagens para uso em computa\u00e7\u00e3o paralela, oferecendo uma infraestrutura para a cria\u00e7\u00e3o de programas que utilizem recursos de CPU e mem\u00f3ria de m\u00faltiplos computadores em um cluster ou n\u00f3s em um supercomputador, al\u00e9m de permitir a passagem coordenada de mensagens entre processos.</p> <p>Definindo um padr\u00e3o industrial para a troca de mensagens, existem diversas implementa\u00e7\u00f5es do MPI. Iremos utilizar o Open MPI juntamente com o gerenciador de recursos SLURM de acordo com os passos a seguir:</p> <ol> <li>Conex\u00e3o ao supercomputador por SSH</li> <li>Cria\u00e7\u00e3o de um programa MPI</li> <li>Compila\u00e7\u00e3o do programa MPI</li> <li>Execu\u00e7\u00e3o do programa no supercomputador</li> </ol>"},{"location":"advanced/mpi/#passo-1-conexao-ao-supercomputador-por-ssh","title":"Passo 1: Conex\u00e3o ao supercomputador por SSH","text":"<p>Para se conectar ao supercomputador \u00e9 necess\u00e1rio utilizar o SSH j\u00e1 dispon\u00edvel por padr\u00e3o na maioria dos sistemas Unix como nas distribui\u00e7\u00f5es Linux e todas as vers\u00f5es do Mac OS X assim como dispon\u00edvel livremente na Internet como download para a plataforma Windows. O cliente SSH mais popular da plataforma Windows \u00e9 o Putty</p> <p>Iremos nesse tutorial utilizar como padr\u00e3o a plataforma Linux, mas sua execu\u00e7\u00e3o no Windows segue os mesmos princ\u00edpios.</p> <p>Para se conectar no n\u00f3 de login do supercomputador basta executar o seguinte comando:</p> <pre><code>ssh -p 4422 USUARIO@sc2.npad.ufrn.br\n</code></pre> <p>Troque USUARIO pelo seu nome de usu\u00e1rio j\u00e1 autorizado. Voc\u00ea deve visualizar uma tela com not\u00edcias sobre o supercomputador e a cria\u00e7\u00e3o dos seus diret\u00f3rios pessoais.</p> <p>Nesse momento voc\u00ea j\u00e1 est\u00e1 conectado ao supercomputador no ambiente Linux da m\u00e1quina em sua \u00e1rea pessoal de arquivos. Todos os comandos padr\u00e3o da distribui\u00e7\u00e3o CentOS assim como alguns editores de texto como o 'Emacs' e 'Vi' est\u00e3o dispon\u00edveis para que voc\u00ea possa manipular seus arquivos e pastas pessoais. Nesse momento voc\u00ea j\u00e1 est\u00e1 conectado ao supercomputador no ambiente Linux da m\u00e1quina em sua \u00e1rea pessoal de arquivos. Todos os comandos padr\u00e3o da distribui\u00e7\u00e3o CentOS assim como alguns editores de texto como o 'Emacs' e 'Vi' est\u00e3o dispon\u00edveis para que voc\u00ea possa manipular seus arquivos e pastas pessoais.</p>"},{"location":"advanced/mpi/#passo-2-criacao-de-um-programa-mpi","title":"Passo 2: Cria\u00e7\u00e3o de um programa MPI","text":"<p>O esqueleto de um programa MPI \u00e9 estruturado da seguinte forma:</p> <ul> <li>Inclus\u00e3o do cabe\u00e7alho mpi.h e dos demais cabe\u00e7alhos necess\u00e1rios ao programa</li> <li>In\u00edcio do programa indicado pela linha de c\u00f3digo \"int main (int argc, char *argv[])\"</li> <li>C\u00f3digo sequencial opcional, por exemplo: inicializa\u00e7\u00e3o de vari\u00e1veis</li> <li>Inicializa\u00e7\u00e3o do c\u00f3digo paralelo indicado pela linha de c\u00f3digo \"MPI_Init(&amp;argc, &amp;argv);\"</li> <li>Leitura da quantidade de processos criados indicado pela linha de c\u00f3digo \"MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);\"</li> <li>Leitura do rank do processo utilizado, indicado pela linha de c\u00f3digo \"MPI_Comm_rank(MPI_COMM_WORLD, &amp;comm_sz);\"</li> <li>C\u00f3digo paralelo</li> <li>Finaliza\u00e7\u00e3o do c\u00f3digo paralelo indicado pela linha de c\u00f3digo \"MPI_Finalize();\"</li> <li>C\u00f3digo sequencial opcional e finaliza\u00e7\u00e3o do programa</li> </ul> <p>Um exemplo de programa MPI \u00e9 ilustrado abaixo. Copie esse programa para um arquivo na sua pasta pessoal e salve com o nome \"mpi_hello.c\". A fun\u00e7\u00e3o desse programa \u00e9 imprimir uma mensagem de forma paralela a partir de processos diferentes. Esses processos podem estar no mesmo n\u00f3, utilizando diferentes n\u00facleos, ou podem estar em n\u00f3s diferentes. Essa escolha \u00e9 feita pelo usu\u00e1rio a partir dos par\u00e2metros utilizados no script de submiss\u00e3o que ser\u00e1 mostrado numa se\u00e7\u00e3o (ou passo) posterior.</p> <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n\nint main (int argc, char *argv[])\n{\n  int  rank, comm_sz, len;\n  char hostname[MPI_MAX_PROCESSOR_NAME];\n\n  MPI_Init(&amp;argc, &amp;argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n  MPI_Get_processor_name(hostname, &amp;len);\n  printf (\"Hello from process %d on node %s!\\n\", rank, hostname);\n  if (rank == 0)\n    printf(\"From process %d: Number of MPI processes is %d\\n\", rank, comm_sz);\n\n  MPI_Finalize();\n}\n</code></pre>"},{"location":"advanced/mpi/#passo-3-compilacao-do-programa-mpi","title":"Passo 3: Compila\u00e7\u00e3o do programa MPI","text":"<p>De posse do arquivo <code>mpi_hello.c</code>, voc\u00ea pode compilar o mesmo com o comando <code>mpicc</code> da seguinte forma:</p> <pre><code>$ mpicc mpi_hello.c -o mpi_hello \n$ ls \nmpi_hello.c   mpi_hello\n</code></pre> <p>Para programas em C++, use os comandos <code>mpic++</code> ou <code>mpiCC</code> em vez de <code>mpicc</code> para compilar.</p> <p>A compila\u00e7\u00e3o gerou com sucesso o arquivo bin\u00e1rio <code>mpi_hello</code> que ser\u00e1 executado em v\u00e1rios n\u00facleos de processadores do supercomputador no pr\u00f3ximo passo.</p>"},{"location":"advanced/mpi/#passo-4-execucao-do-programa-no-supercomputador","title":"Passo 4: Execu\u00e7\u00e3o do programa no supercomputador","text":"<p>No supercomputador \u00e9 utilizado o gerenciador de recursos SLURM para se executar tarefas em diversos processadores da m\u00e1quina. No SLURM, o termo tarefa (do ingl\u00eas task) \u00e9 equivalente ao conceito de processo como estudado nos cursos de sistemas operacionais.</p> <p>H\u00e1 duas maneiras de lan\u00e7ar tarefas (processos) no Slurm:</p> <p>1) Atrav\u00e9s do comando <code>mpirun</code>; 2) Atrav\u00e9s do comando do slurm <code>srun</code>.</p> <p>As duas maneiras s\u00e3o equivalentes para a maioria dos casos. Fica a crit\u00e9rio do usu\u00e1rio testar a melhor maneira de executar seu job.</p> <p>Neste tutorial, usaremos o comando <code>mpirun</code>.</p>"},{"location":"advanced/mpi/#exemplo-de-script-mais-simples","title":"Exemplo de script mais simples","text":"<p>Para executar 4 tarefas (processos) do programa <code>mpi_hello</code> devemos criar um script chamado, por exemplo, de <code>jobMPI.sh</code> com o conte\u00fado abaixo:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512 #parti\u00e7\u00e3o para a qual o job \u00e9 enviado\n#SBATCH --ntasks=4\n#SBATCH --time=0-0:5\n\nmpirun mpi_hello \n</code></pre> <p>Note que \u00e9 desnecess\u00e1rio usar a op\u00e7\u00e3o -np do <code>mpirun</code> porque o slurm j\u00e1 se encarrega de indicar o n\u00famero de processos, como definido em <code>\u2013ntasks</code>, para o <code>mpirun</code>.</p> <p>Para enviar esse \u201cjob\u201d para a fila de execu\u00e7\u00e3o usamos o comando:</p> <pre><code>$ sbatch jobMPI.sh \n\nSubmitted batch job 2230\n</code></pre> <p>Esse comando ir\u00e1 adicionar o \u201cjob\u201d na fila de execu\u00e7\u00e3o e retornar um id (nesse caso 2230) dessa nova tarefa adicionada. Para monitorar a tarefa voc\u00ea pode usar o comando <code>squeue</code> para visualizar todas as tarefas em andamento. Se desejar cancelar uma tarefa em execu\u00e7\u00e3o utilize o comando <code>scancel JOB_ID</code>.</p> <p>Ao finalizar sua execu\u00e7\u00e3o o SLURM cria automaticamente um arquivo chamado \u201cslurm-JOB_ID.out\u201d com a sa\u00edda padr\u00e3o de sua tarefa redirecionada para esse arquivo. O nome do arquivo cont\u00e9m o ID do job enviado (JOB_ID) para facilitar sua rela\u00e7\u00e3o com o mesmo. Isso \u00e9 importante se seu programa gera uma sa\u00edda padr\u00e3o que cont\u00e9m um resultado \u00fatil da sua execu\u00e7\u00e3o. Veja abaixo o resultado da execu\u00e7\u00e3o do simples script que criamos e sua sa\u00edda em arquivos:</p> <pre><code>$ sbatch jobMPI.sh \n\nSubmitted batch job 2230\n\n$ cat slurm-2230.out \n\nHello from process 3 on node r1i2n16!\n\nHello from process 2 on node r1i2n16!\n\nHello from process 1 on node r1i2n16!\n\nHello from process 0 on node r1i2n16!\n\nFrom process 0: Number of MPI processes is 4\n</code></pre> <p>Repare que todas as 4 tarefas foram executadas no mesmo n\u00f3 (neste caso o r1i2n16), pois o SLURM s\u00f3 foi instru\u00eddo para executar 4 tarefas e nada mais.</p>"},{"location":"advanced/mpi/#exemplo-de-script-mais-detalhado","title":"Exemplo de script mais detalhado","text":"<p>Um exemplo de script que solicita que 4 tarefas do programa \u201cmpi_hello\u201d executem em 2 n\u00f3s diferentes \u00e9 mostrado abaixo:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=MPI_hello  \n#SBATCH --output=saida%j.out\n#SBATCH --error=erro%j.err\n#SBATCH --partition=amd-512\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --time=0-0:5\n\nmpirun mpi_hello \n</code></pre> <p>Repare que agora o script cont\u00e9m v\u00e1rios par\u00e2metros que foram adicionados. A descri\u00e7\u00e3o desses par\u00e2metros \u00e9 informada abaixo:</p> <ul> <li><code>job-name</code>: Nome que aparecer\u00e1 na fila de jobs</li> <li><code>output</code>: Arquivo de sa\u00edda padr\u00e3o do programa</li> <li><code>error</code>: Arquivo de sa\u00edda de erro do programa</li> <li><code>nodes</code>: Quantidade de n\u00f3s alocados para o programa</li> <li><code>ntasks-per-node</code>: N\u00famero de processos em um mesmo n\u00f3 (normalmente 1 para jobs OpenMP) - \u00datil para jobs MPI</li> <li><code>time</code>: Tempo m\u00e1ximo para execu\u00e7\u00e3o do job (nesse exemplo = 5 min). Caso o job ainda n\u00e3o tenha terminado, ap\u00f3s esse tempo ele ser\u00e1 cancelado. Formato: dias-horas:minutos</li> </ul> <p>Observa-se, ent\u00e3o, que foram requisitados 2 (valor de --nodes) n\u00f3s para os jobs e que em cada n\u00f3 ser\u00e3o criados 2 (valor de --ntasks-per-node) processos. Veja sua execu\u00e7\u00e3o:</p> <pre><code>$ sbatch jobMPI.sh \n\nSubmitted batch job 2236\n\n$ cat slurm-2236.out \n\nHello from process 3 on node r1i1n2!\n\nHello from process 2 on node r1i1n2!\n\nHello from process 1 on node r1i3n5!\n\nHello from process 0 on node r1i3n5!\n\nFrom process 0: Number of MPI processes is 4\n</code></pre> <p>Repare que agora as 4 tarefas foram executadas em dois n\u00f3s distintos (r1i1n2 e r1i3n5), exatamente da forma que instru\u00edmos.</p> <p>Assim finalizamos nosso tutorial. Para mais informa\u00e7\u00f5es sobre o Open MPI e o SLURM acesse os links abaixo:</p> <ul> <li>http://slurm.schedmd.com</li> <li>https://slurm.schedmd.com/mpi_guide.html</li> <li>https://docs.open-mpi.org/en/v5.0.x/launching-apps/slurm.html</li> </ul>"},{"location":"advanced/openmp/","title":"Tutorial de OpenMP","text":"<p>Veremos neste tutorial como executar um job contendo c\u00f3digos escritos com OpenMP no supercomputador. OpenMP \u00e9 um modelo de programa\u00e7\u00e3o paralela para sistemas de mem\u00f3ria compartilhada. Nesse modelo, o programa cria diversas threads que s\u00e3o coordenadas por um thread mestre. O programador define algumas se\u00e7\u00f5es do c\u00f3digo como paralelas utilizando diretivas de preprocessamento espec\u00edficas.</p> <p>Os n\u00f3s do supercomputador n\u00e3o compartilham mem\u00f3ria. Por isso, n\u00e3o \u00e9 poss\u00edvel executar em mais de um n\u00f3 um job que utiliza somente OpenMP como ferramenta de programa\u00e7\u00e3o paralela. Contudo, um programa escrito com OpenMP pode ser executado em m\u00faltiplos cores em um mesmo n\u00f3. Para que um job seja executado em v\u00e1rios n\u00f3s, deve-se utilizar um modelo de programa\u00e7\u00e3o paralela para sistemas de mem\u00f3ria distribu\u00edda (e.g. MPI). Tamb\u00e9m \u00e9 poss\u00edvel utilizar OpenMP e MPI em um mesmo c\u00f3digo.</p>"},{"location":"advanced/openmp/#exemplo-hello-world","title":"Exemplo: Hello World","text":"<pre><code>//Arquivo hello_openmp.c \n#include &lt;omp.h&gt; \n#include &lt;stdio.h&gt;\nint main (int argc, char *argv[]) \n{ \n int nthreads, thread_id; \n #pragma omp parallel private(nthreads, thread_id) \n { \n  thread_id = omp_get_thread_num();\n  printf(\"Thread %d says: Hello World\\n\", thread_id); \n  if (thread_id == 0)\n  { \n   nthreads = omp_get_num_threads();\n   printf(\"Thread %d reports: the number of threads are %d\\n\", thread_id, nthreads); \n  } \n } \n return 0; \n}\n</code></pre> <p>Em seguida, \u00e9 necess\u00e1rio escrever um script de job para executar o programa. Por exemplo:</p> <pre><code>#!/bin/bash \n#SBATCH --job-name=OMP_hello \n#SBATCH --time=0-0:5\n#SBATCH --cpus-per-task=8 \n\n./hello_openmp\n</code></pre> <p>A descri\u00e7\u00e3o dos valores das diretivas do script \u00e9 informada abaixo:</p> <ul> <li><code>--job-name</code>: Nome que aparecer\u00e1 na fila de jobs</li> <li><code>--time</code>: Tempo m\u00e1ximo para execu\u00e7\u00e3o do job (neste exemplo = 5 min). Caso o job ainda n\u00e3o tenha terminado, ap\u00f3s esse tempo ele ser\u00e1 cancelado. Formato: dias-horas:minutos.</li> <li><code>--cpus-per-task</code>: Significa que foi solicitado oito processadores para o programa hello_openmp.  </li> </ul> <p>O Slurm considera que cada core tem dois processadores. Na verdade, esses dois processadores s\u00e3o threads de hardware. Para fazer com que o Slurm considere cada core como sendo um s\u00f3 processador use a diretiva <code>--hint=compute_bound</code>.</p> <p>Em programas OpenMP, usa-se a vari\u00e1vel de ambiente <code>OMP_NUM_THREADS</code> para especificar explicitamente o n\u00famero de threads do programa. Voc\u00ea pode usar a vari\u00e1vel de ambiente <code>SLURM_CPUS_PER_TASK</code> para definir <code>OMP_NUM_THREADS</code>. Por exemplo:</p> <pre><code>#!/bin/bash \n#SBATCH --job-name=OMP_hello \n#SBATCH --time=0-0:5\n#SBATCH --cpus-per-task=8 \n#SBATCH --hint=compute_bound\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n./hello_openmp\n</code></pre> <p>Note que configurar <code>OMP_NUM_THREADS</code> para um n\u00famero maior que <code>cpus-per-task</code> provavelmente n\u00e3o ir\u00e1 melhorar o desempenho do programa porque for\u00e7ar\u00e1 as threads do programa a compartilhar os recursos do mesmo processador. Em geral, para melhor desempenho, deve-se alocar um processador para cada thread do programa.</p>"},{"location":"advanced/openmp/#submissao-de-um-job","title":"Submiss\u00e3o de um job","text":"<p>Edite o arquivo <code>hello_openmp.c</code> com seu editor favorito e transfira para o supercomputador. Acesse o supercomputador e compile o programa:</p> <pre><code>gcc -g -Wall -fopenmp -o hello_openmp hello_openmp.c\n</code></pre> <p>Na mesma forma, crie o script de job <code>script_job.sh</code>. Ent\u00e3o submeta o job:</p> <pre><code>$ sbatch script_job.sh\n\nSubmitted batch job JOBID\n</code></pre> <p>JOBID \u00e9 n\u00famero do job. O job entrar\u00e1 em uma fila para ser executado. Quando isso acontecer, o sa\u00edda do programa ser\u00e1 gravada no arquivo slurm-JOBID.out. Ent\u00e3o, \u00e9 poss\u00edvel verificar a sa\u00edda com o seguinte comando:</p> <pre><code>$ cat slurm-JOBID.out\n\n  Thread 0 says: Hello Word!\n\n  Thread 0 reports: the number of threads are 8\n\n  Thread 7 says: Hello Word!\n\n  Thread 4 says: Hello Word!\n\n  Thread 6 says: Hello Word!\n\n  Thread 1 says: Hello Word!\n\n  Thread 3 says: Hello Word!\n\n  Thread 5 says: Hello Word!\n\n  Thread 2 says: Hello Word!\n</code></pre>"},{"location":"advanced/tch-rs/","title":"tch-rs-example MNIST","text":"<p>Nesse tutorial voc\u00ea ir\u00e1 aprender a usar a parti\u00e7\u00e3o gpu e rust  para treinar um modelo de deep learning para resolver o MNIST.</p> <ul> <li>Instalando depend\u00eancias</li> <li>Criando um aplica\u00e7\u00e3o em rust para treinar uma rede neural</li> <li>Carregamento do conjunto de dados MNIST<ul> <li>Configura\u00e7\u00e3o do dispositivo de processamento</li> <li>Criando um modelo deep learning</li> <li>Treinamento</li> </ul> </li> <li>Executando a aplica\u00e7\u00e3o com cuda no supercomputador</li> </ul>"},{"location":"advanced/tch-rs/#instalando-dependencias","title":"Instalando depend\u00eancias","text":"<p>Primeiro instale o rust toolchain na sua pasta HOME, no n\u00f3 de login.</p> <pre><code>$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre> <p>segundo crie um novo projeto rust com cargo chamado pytorch-example. Para esse projeto Adicione as depend\u00eancias:</p> <ul> <li>anyhow, uma biblioteca para facilita o tratamento de erro em rust</li> <li>tch O framework Pytorch para cria\u00e7\u00e3o de modelos deep learning escrito em C++, mas com bindings para rust</li> </ul> <pre><code>cargo new pytorch-example\ncd pytorch-example\ncargo add anyhow\ncargo add tch\n</code></pre>"},{"location":"advanced/tch-rs/#criando-um-aplicacao-em-rust-para-treinar-uma-rede-neural","title":"Criando um aplica\u00e7\u00e3o em rust para treinar uma rede neural","text":"<p>A aplica\u00e7\u00e3o escrita  em rust, dever\u00e1 selecionar qual dispositivo de processamento, CPU ou GPU, dever\u00e1 ser usado para a execu\u00e7\u00e3o dos c\u00e1lculos num\u00e9ricos. Carregar o conjunto de dados MNIST para ser computado em tal dispositivo. Criar um modelo deep learning e treinar o modelo. Portanto podemos entender a aplica\u00e7\u00e3o em  4 partes importantes:  carregamento do conjunto de dados MNIST, configura\u00e7\u00e3o do dispositivo, criando um modelo deep learning, treinamento.</p>"},{"location":"advanced/tch-rs/#carregamento-do-conjunto-de-dados-mnist","title":"Carregamento do conjunto de dados MNIST","text":"<p>Como o MNIST \u00e9 um conjunto de dados muito famoso, o pr\u00f3prio pytorch possui mecanismos de carreg\u00e1-lo, desde que voc\u00ea tenha ele baixado e descompactado. Para baixar o dataset, voc\u00ea pode criar um script similar ao get_inputs.sh. Onde basicamente ele cria o diret\u00f3rio chamado data e baixa os arquivos do dataset e os extrai com gunzip</p> <pre><code>#!/bin/bash \n# get_inputs.sh\nmkdir data -p\ncd data\nwget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz; gunzip train-images-idx3-ubyte.gz\nwget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz; gunzip train-labels-idx1-ubyte.gz\nwget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz; gunzip t10k-images-idx3-ubyte.gz\nwget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz; gunzip t10k-labels-idx1-ubyte.gz\n</code></pre> <p>Para carregar o dataset basta usar o a fun\u00e7\u00e3o tch::vision::mnist::load_dir</p> <pre><code>let dataset = match tch::vision::mnist::load_dir(\"data\") { Dataset\n          Ok(d) =&gt; d, \n          Err(_) =&gt; panic!(\"Dataset Not found, run the get_inputs.sh !!\"),\n};\n</code></pre>"},{"location":"advanced/tch-rs/#configuracao-do-dispositivo-de-processamento","title":"Configura\u00e7\u00e3o do dispositivo de processamento","text":"<p>Utilizando um framework pytorch podemos selecionar o dispositivo da seguinte forma, se uma GPU estiver dispon\u00edvel, ent\u00e3o utilize GPU. Caso o contr\u00e1rio utilize CPU. Com o dispositivo podemos criar um conjunto de tensores cujo seus valores variam e que esses valores dever\u00e3o ser capazes de serem armazenados, ou seja criamos uma VarStore.</p> <pre><code> let device = tch::Device::cuda_if_available();\n let vs = tch::nn::VarStore::new(device.clone());\n</code></pre> <p>Tamb\u00e9m precisamos transformar ou mover os dados do conjunto de dados para o dispositivo de processamento</p> <pre><code>fn dataset_to_device(dataset: Dataset, device: &amp;Device) -&gt; Dataset {\n    let train_labels = dataset.train_labels.to_device(device.clone());\n    let train_images = dataset.train_images.to_device(device.clone());\n    let test_labels = dataset.test_labels.to_device(device.clone());\n    let test_images = dataset.test_images.to_device(device.clone());\n\n    Dataset {\n        test_images,\n        test_labels,\n        train_images,\n        train_labels,\n        labels: dataset.labels,\n    }\n}\n\nlet dataset = dataset_to_device(dataset, &amp;device);\n</code></pre>"},{"location":"advanced/tch-rs/#criando-um-modelo-deep-learning","title":"Criando um modelo deep learning","text":"<p>O modelo deep learning utilizado nesse exemplo ser\u00e1 um modelo sequencial simples e com poucas camadas. Em rust o modelo ficou assim:</p> <pre><code>use tch::{nn, nn::Module};\nconst IMAGE_DIM: i64 = 784;\nconst HIDDEN_NODES: i64 = 128;\nconst LABELS: i64 = 10;\n\nfn net(vs: &amp;nn::Path) -&gt; impl Module {\n    nn::seq()\n        .add(nn::linear(\n            vs / \"layer1\",\n            IMAGE_DIM,\n            HIDDEN_NODES,\n            Default::default(),\n        ))\n        .add_fn(|xs| xs.relu())\n        .add(nn::linear(vs, HIDDEN_NODES, LABELS, Default::default()))\n}\n\nlet net = net(&amp;vs.root());\n</code></pre> <p>Observe que vs (VarStore) \u00e9 passada na fun\u00e7\u00e3o net, de modo que criar uma camada, ou m\u00f3dulo (layer) \u00e9 alocar novos de tensores, para a vari\u00e1vel.</p>"},{"location":"advanced/tch-rs/#treinamento","title":"Treinamento","text":"<p>Para treinar uma rede neural, pode-se utilizar v\u00e1rios algoritmos de otimiza\u00e7\u00e3o, por\u00e9m nesse exemplo foi utilizado o Adam, com uma taxa de aprendizado de 0.001. O modelo ser\u00e1 treinado durante 200 intera\u00e7\u00f5es. Em rust a implementa\u00e7\u00e3o do treinamento fica da seguinte forma</p> <pre><code>let mut opt = nn::Adam::default().build(&amp;vs, 1e-3)?;\n\nfor epoch in 1..200 {\n    let loss = net\n            .forward(&amp;dataset.train_images)\n            .cross_entropy_for_logits(&amp;dataset.train_labels);\n        opt.backward_step(&amp;loss);\n        let test_accuracy = net\n            .forward(&amp;dataset.test_images)\n            .accuracy_for_logits(&amp;dataset.test_labels);\n        println!(\n            \"epoch: {:4} train loss: {:8.5} test acc: {:5.2}% is cuda: {}\",\n            epoch,\n            f64::try_from(&amp;loss)?,\n            100. * f64::try_from(&amp;test_accuracy)?,\n            device.is_cuda(),\n        );\n    }\n</code></pre> <p>Voc\u00ea pode ver a implementa\u00e7\u00e3o completa em main.rs</p>"},{"location":"advanced/tch-rs/#executando-a-aplicacao-com-cuda-no-supercomputador","title":"Executando a aplica\u00e7\u00e3o com cuda no supercomputador","text":"<p>Atualmente o supercomputador no NPAD possui uma parti\u00e7\u00e3o chamada gpu, nessa parti\u00e7\u00e3o encontram-se os n\u00f3s com GPUs bem potentes capazes de executar a aplica\u00e7\u00e3o em 1 segundo.</p> <pre><code>#!/bin/bash \n#SBATCH --job-name=neural_train\n#SBATCH --time=0-0:15\n#SBATCH --partition=gpu-8-v100 \n#SBATCH --gpus-per-node=1\n\n# informando ao tch-rs que desejo compilar com cuda na vers\u00e3o 11.7\nexport TORCH_CUDA_VERSION=cu117\n\ncargo r --release\n</code></pre> <p>Cargo \u00e9 o gerenciador de pacotes official da linguagem Rust, perceba que ao executar o comando <code>cargo r --release</code>. A aplica\u00e7\u00e3o cargo ir\u00e1 compilar a aplica\u00e7\u00e3o utilizando flags de otimiza\u00e7\u00e3o e ir\u00e1 executar o programa. Caso tenha compilado a aplica\u00e7\u00e3o no n\u00f3 de login, ser\u00e1 necess\u00e1rio remover a pasta target, antes de submeter o script</p> <pre><code># rm -rf target # caso tenha compilado a aplica\u00e7\u00e3o no n\u00f3 de login.\n\nsbatch run_on_superpc.sh\n</code></pre> <p>o script run_on_superpc.sh pode ser encontrado aqui. Todo o projeto pode ser encontrado no github github.com/samuel-cavalcanti/tch-rs-example</p>"},{"location":"beginner/gnome_files/","title":"Copiando arquivos atrav\u00e9s de uma interface gr\u00e1fica Gnome Files (linux)","text":"<p>Gnome \u00e9 a interface padr\u00e3o do Ubuntu. Que \u00e9 distribui\u00e7\u00e3o padr\u00e3o para desenvolvimento do Instituto Metr\u00f3pole Digital (IMD). Portanto iremos mostrar como conectar o navegador de arquivos padr\u00e3o do Gnome, chamado Gnome files com os seus arquivos no supercomputador. A vers\u00e3o do Gnome files utilizada \u00e9 a 43.3</p> <p></p>"},{"location":"beginner/gnome_files/#clique-em-other-locations","title":"Clique em Other Locations","text":"<p>Abra o gestor de arquivos Gnome files e clique em Other Locations como apontado na imagem</p> <p></p>"},{"location":"beginner/gnome_files/#digite-o-endereco-do-super-pc","title":"Digite o endere\u00e7o do super pc","text":"<p>No canto inferior direito, digite o endere\u00e7o do supercomputador. No caso o endere\u00e7o varia de acordo com o seu nome de usu\u00e1rio:</p> <pre><code>ssh://nomeDoUsuario@sc2.npad.ufrn.br:4422\n</code></pre> <p>No entanto caso voc\u00ea tenha configurado o arquivo ~/.ssh/config o endere\u00e7o pode ser escrito da seguinte forma:</p> <pre><code>ssh://super-pc\n</code></pre> <p></p>"},{"location":"beginner/gnome_files/#dica-adicione-nos-favoritos","title":"Dica: Adicione nos favoritos","text":"<p>Para n\u00e3o precisar ficar refazendo este tutorial, voc\u00ea pode salvar o o endere\u00e7o do supercomputador nos favoritos, clicando com bot\u00e3o direito do mouse e depois em add to Bookmarks</p> <p></p>"},{"location":"beginner/introduction_part_1/","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte\u00a01","text":"<ul> <li>Instale os pr\u00e9-requisitos<ul> <li>Windows</li> <li>Linux</li> </ul> </li> <li>Gerando uma chave SSH p\u00fablica<ul> <li>Como gerar a chave p\u00fablica no Linux</li> <li>Como gerar a chave p\u00fablica no Windows 11</li> <li>Como gerar a chave p\u00fablica no Windows com MobaXterm</li> <li>Como gerar a chave p\u00fablica no Windows com PuTTy</li> </ul> </li> <li>Criando uma conta no NPAD</li> <li>Acessando o supercomputador</li> <li>Crie uma configura\u00e7\u00e3o para ssh</li> <li>Acessando arquivos do supercomputador<ul> <li>Atrav\u00e9s de uma interface gr\u00e1fica</li> <li>Atrav\u00e9s do terminal</li> </ul> </li> </ul> <p>O NPAD oferece como solu\u00e7\u00e3o um acesso a um supercomputador atrav\u00e9s de um terminal remoto. Para acessar o supercomputador \u00e9 necess\u00e1rio utilizar um programa chamado ssh. O ssh \u00e9 uma programa que permite fazer login em uma m\u00e1quina remotamente. Neste tutorial iremos aprender a gerar uma chave ssh, a acessar o supercomputador e a transferir arquivos para o supercomputador. Caso tenha alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato conosco atrav\u00e9s do e-mail atendimento\\npad.ufrn.br (substituindo \\ por @)."},{"location":"beginner/introduction_part_1/#instale-os-pre-requisitos","title":"Instale os pr\u00e9-requisitos","text":"<p>Como dito anteriormente, acessar o supercomputador requer ssh. O ssh \u00e9 apenas uma das ferramentas do OpenSSH. A seguir veremos como instalar o ssh em cada sistema operacional:</p>"},{"location":"beginner/introduction_part_1/#windows","title":"Windows","text":"<p>O Windows 11 j\u00e1 vem com o OpenSSH e, portanto, voc\u00ea pode usar o ssh ou gerar a chave no Windows Terminal. Nos demais Windows, voc\u00ea pode seguir por um desses caminhos:</p> <ul> <li>Instalar o MobaXterm Home Edition que j\u00e1 vem com OpenSSH</li> <li>Instalar o PuTTY que possui o seu pr\u00f3prio cliente ssh</li> <li>Instalar OpenSSH e utilizar o Windows PowerShell como terminal</li> </ul>"},{"location":"beginner/introduction_part_1/#linux","title":"Linux","text":"<p>Procure por openssh nos reposit\u00f3rios oficiais. No caso do ubuntu para instalar o openssh \u00e9:</p> <pre><code>sudo apt install openssh-client\n</code></pre> <p>Recomendamos utilizar o cliente ssh e terminal oficiais do sistema operacional que estiver utilizando.</p>"},{"location":"beginner/introduction_part_1/#gerando-uma-chave-ssh-publica","title":"Gerando uma chave SSH p\u00fablica","text":"<p>Para criar uma conta no supercomputador, ou caso tenha perdido a chave p\u00fablica, ser\u00e1 necess\u00e1rio inserir uma nova chave p\u00fablica do tipo rsa. Esta se\u00e7\u00e3o mostrar\u00e1 como gerar uma chave p\u00fablica.</p>"},{"location":"beginner/introduction_part_1/#como-gerar-a-chave-publica-no-linux","title":"Como gerar a chave p\u00fablica no Linux","text":"<p>Abra o terminal do linux ou aperte Ctrl + Alt + T</p> <p>Para gerar sua chave ssh do tipo rsa, digite o comando a seguir:</p> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>Ir\u00e1 ser realizado uma sequ\u00eancia de perguntas, apenas pressione enter em todas elas. Para visualizar sua chave p\u00fablica, digite o comando a seguir:</p> <pre><code>cat .ssh/id_rsa.pub\n</code></pre> <p>Voc\u00ea precisar\u00e1 copiar e colar essa chave p\u00fablica na hora de criar uma conta ou adicionar outra chave. Com uma chave p\u00fablica voc\u00ea est\u00e1 pronto para criar uma conta no NPAD. Perceba que voc\u00ea criou uma chave privada em .ssh/id_rsa e uma chave p\u00fablica .ssh/id_rsa.pub.</p>"},{"location":"beginner/introduction_part_1/#como-gerar-a-chave-publica-no-windows-11","title":"Como gerar a chave p\u00fablica no Windows 11","text":"<p>Procure pelo Windows Terminal e abra-o.</p> <p></p> <p>Para gerar sua chave ssh do tipo rsa, digite o comando a seguir que ir\u00e1 ser realizado uma sequ\u00eancia de perguntas, apenas pressione enter em todas elas:</p> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>Para visualizar sua chave p\u00fablica, digite o comando a seguir:</p> <pre><code>type .ssh\\id_rsa.pub\n</code></pre> <p>Voc\u00ea precisar\u00e1 copiar e colar essa chave p\u00fablica na hora de criar uma conta ou adicionar outra chave. Com uma chave p\u00fablica voc\u00ea est\u00e1 pronto para criar uma conta no NPAD. Perceba que voc\u00ea criou uma chave privada em .ssh\\id_rsa e uma chave p\u00fablica em .ssh\\id_rsa.pub. </p> <p>Vale observar que o Windows usa uma barra invertida para separar as pastas em um caminho (path) para arquivos ou pastas. Enquanto, no Linux usa-se uma barra normal. Por exemplo, se no Windows o caminho \u00e9 caminho\\para\\arquivo.txt, no Linux seria caminho/para/arquivo.txt.</p>"},{"location":"beginner/introduction_part_1/#como-gerar-a-chave-publica-no-windows-com-mobaxterm","title":"Como gerar a chave p\u00fablica no Windows com MobaXterm","text":"<p>Abra o MobaXterm e clique no bot\u00e3o start local terminal</p> <p>Para gerar sua chave ssh do tipo rsa , e digite o comando a seguir:</p> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>Ir\u00e1 ser realizado uma sequ\u00eancia de perguntas, apenas pressione enter em todas elas. Para visualizar sua chave p\u00fablica, digite o comando a seguir:</p> <pre><code>cat .ssh/id_rsa.pub\n</code></pre> <p>Voc\u00ea precisar\u00e1 copiar e colar essa chave p\u00fablica na hora de criar uma conta ou adicionar outra chave. Com uma chave p\u00fablica voc\u00ea est\u00e1 pronto para criar uma conta no NPAD. Perceba que voc\u00ea criou uma chave privada em .ssh/id_rsa e uma chave p\u00fablica .ssh/id_rsa.pub.</p>"},{"location":"beginner/introduction_part_1/#como-gerar-a-chave-publica-no-windows-com-putty","title":"Como gerar a chave p\u00fablica no Windows com PuTTy","text":"<p>Veja esse tutorial: PuTTy Tutoriais: Gerando um par de chaves publico privada tipo RSA</p>"},{"location":"beginner/introduction_part_1/#criando-uma-conta-no-npad","title":"Criando uma conta no NPAD","text":"<p>Para utilizar o supercomputador \u00e9 necess\u00e1rio criar uma conta na nossa P\u00e1gina de Cadastro. Para realizar o cadastro, verifique qual o seu enquadramento na nossa Pol\u00edtica de Acesso para saber qual o Tipo de Usu\u00e1rio da sua conta. Na P\u00e1gina de Primeiros Passos voc\u00ea obter\u00e1 informa\u00e7\u00f5es sobre o primeiro acesso. Depois de fazer o cadastro no site do NPAD, voc\u00ea receber\u00e1 um e-mail confirmando sua inscri\u00e7\u00e3o. Ap\u00f3s receber o e-mail, voc\u00ea poder\u00e1 acessar o supercomputador do computador que gerou o par de chaves ssh observando as orienta\u00e7\u00f5es presentes neste tutorial.</p>"},{"location":"beginner/introduction_part_1/#acessando-o-supercomputador","title":"Acessando o supercomputador","text":"<p>Uma vez que tenha cadastro no NPAD, voc\u00ea pode acessar o supercomputador de duas formas:</p> <ul> <li> <p>Usando a aplica\u00e7\u00e3o PuTTy. Caso deseja usar o PuTTy veja o tutorial do PuTTy. </p> </li> <li> <p>atrav\u00e9s de um terminal como: Windows PowerShell, MobaXterm, usando o comando ssh</p> </li> </ul> <p>Caso deseja usar o ssh, ent\u00e3o dentro do terminal digite o comando:</p> <pre><code>ssh -p4422 nomeDoUsuario@sc2.npad.ufrn.br\n</code></pre> <p>substituindo o termo nomeDoUsuario pelo nome de usu\u00e1rio criado. Caso tenha feito tudo corretamente ser\u00e1 apresentada a tela inicial do supercomputador:</p> <p></p> <p>Aviso: usu\u00e1rios n\u00e3o tem permiss\u00e3o para usar o comando sudo</p> <p>Sudo significa \u201csuper user do\u201d e \u00e9 um comando para elevar seus privil\u00e9gios ao poderoso usu\u00e1rio root que tem acesso total a todo o sistema. Por isso, voc\u00ea n\u00e3o tem permiss\u00e3o para usar o sudo. Se precisar fazer algo que necessite do poder de administrador do sistema, entre em contato com o atendimento do NPAD para obter assist\u00eancia.</p>"},{"location":"beginner/introduction_part_1/#crie-uma-configuracao-para-ssh","title":"Crie uma configura\u00e7\u00e3o para ssh","text":"<p>Se voc\u00ea criar ou adicionar a seguinte configura\u00e7\u00e3o no arquivo ~/.ssh/config:</p> <pre><code>Host super-pc\n  HostName sc2.npad.ufrn.br\n  Port 4422\n  User nomeDoUsuario\n</code></pre> <p>trocando o nomeDoUsuario pelo nome do seu usu\u00e1rio, voc\u00ea poder\u00e1 acessar o supercomputador usando o comando:</p> <pre><code>ssh super-pc\n</code></pre> <p>Caso voc\u00ea esteja usando MobaXterm, voc\u00ea pode criar uma nova sess\u00e3o para facilitar o acesso ao supercomputador: MobaXterm tutoriais: criando uma sess\u00e3o com NPAD</p>"},{"location":"beginner/introduction_part_1/#acessando-arquivos-do-supercomputador","title":"Acessando arquivos do supercomputador","text":"<p>O OpenSSH al\u00e9m de permitir fazer login em uma m\u00e1quina remotamente, tamb\u00e9m permite a transfer\u00eancia de arquivos por aplica\u00e7\u00f5es de linha de comando quanto por software de interface gr\u00e1fica de terceiros.</p>"},{"location":"beginner/introduction_part_1/#atraves-de-uma-interface-grafica","title":"Atrav\u00e9s de uma interface gr\u00e1fica","text":"<p>Em muitos casos \u00e9 simplesmente mais pr\u00e1tico acessar, copiar e mover arquivos do seu computador para o supercomputador atrav\u00e9s de uma interface gr\u00e1fica. Para isso voc\u00ea pode utilizar o WinSCP no caso do Windows ou configurar o pr\u00f3prio navegador de arquivos do ubuntu: Gnome Files para essa tarefa. Foi feito dois tutoriais:</p> <ul> <li> <p>Copiando Arquivos atrav\u00e9s de uma Interface gr\u00e1fica WinSCP (Windows)</p> </li> <li> <p>Copiando Arquivos atrav\u00e9s de uma Interface gr\u00e1fica Gnome Files (linux)</p> </li> </ul>"},{"location":"beginner/introduction_part_1/#atraves-do-terminal","title":"Atrav\u00e9s do terminal","text":"<p>\u00c9 poss\u00edvel transferir arquivos atrav\u00e9s das aplica\u00e7\u00f5es de linhas de comando como: scp e rsync. Sendo o rsync apenas para linux e scp funciona tamb\u00e9m no Windows 11 pelo Windows Terminal. Nos demais Windows,  apenas se voc\u00ea instalar o OpenSSH. Para aprender a usar o scp veja o tutorial: scp e para o rsync veja o tutorial: rsync.</p>"},{"location":"beginner/introduction_part_2/","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte\u00a02","text":"<p>Nesse tutorial iremos aprender a criar um script e executar um programa utilizando o supercomputador. Caso surja alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato conosco atrav\u00e9s do e-mail atendimento\\npad.ufrn.br (substituindo \\ por @). <ul> <li>Criando um script para um programa j\u00e1 existente no supercomputador</li> <li>Executando um programa j\u00e1 existente no supercomputador<ul> <li>Execu\u00e7\u00e3o de programas no n\u00f3 de login</li> <li>Execu\u00e7\u00e3o de programas no n\u00f3 de computa\u00e7\u00e3o</li> <li>Verifica\u00e7\u00e3o da execu\u00e7\u00e3o</li> </ul> </li> <li>Criando um script para executar um programa criado a partir do c\u00f3digo-fonte</li> <li>Parti\u00e7\u00f5es</li> <li>Executando um programa criado a partir do c\u00f3digo-fonte</li> </ul>"},{"location":"beginner/introduction_part_2/#criando-um-script-para-um-programa-ja-existente-no-supercomputador","title":"Criando um script para um programa j\u00e1 existente no supercomputador","text":"<p>Para criar um script para executar programas no supercomputador \u00e9 necess\u00e1rio informar o nome do programa que ir\u00e1 ser executado e o tempo m\u00e1ximo de sua execu\u00e7\u00e3o (saiba como escolher o tempo de execu\u00e7\u00e3o aqui). O tempo m\u00e1ximo depende do tipo de n\u00f3 escolhido. Acesse o supercomputador e siga as orienta\u00e7\u00f5es a seguir.</p> <p>Exemplo: cria\u00e7\u00e3o de um script para executar o programa factor.</p> <p>Neste exemplo, usaremos um programa simples como <code>factor</code> que mostra os fatores primos de um n\u00famero. No terminal, entre com o comando abaixo, seguido de <code>enter</code>.</p> <pre><code>nano nomeScript    # Substitua nomeScript pelo nome que voc\u00ea deseja dar ao seu script\n</code></pre> <p>Ap\u00f3s abrir o editor de textos nano, digite os comandos abaixo:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512  # parti\u00e7\u00e3o para a qual o job \u00e9 enviado\n#SBATCH --time=0-0:5    # Especifica o tempo m\u00e1ximo de execu\u00e7\u00e3o do job, dado no padr\u00e3o dias-horas:minutos\nfactor 120     # Especifica o programa a ser executado (no caso, factor) e o par\u00e2metro de entrada (120)\n</code></pre> <p>Em sequ\u00eancia, aperte <code>ctrl+o</code> para salvar, depois <code>enter</code> e, ent\u00e3o, aperte <code>ctrl+x</code> para sair do nano. Vale ressaltar que tamb\u00e9m \u00e9 poss\u00edvel abrir o arquivo correspondente ao script e fazer altera\u00e7\u00f5es diretamente nele, via interface gr\u00e1fica.</p>"},{"location":"beginner/introduction_part_2/#executando-um-programa-ja-existente-no-supercomputador","title":"Executando um programa j\u00e1 existente no supercomputador","text":"<p>Para executar um programa no supercomputador, voc\u00ea precisar\u00e1 criar um script, conforme descrito anteriormente, e escolher em qual tipo de n\u00f3 o programa vai ser executado:</p>"},{"location":"beginner/introduction_part_2/#execucao-de-programas-no-no-de-login","title":"Execu\u00e7\u00e3o de programas no n\u00f3 de login","text":"<p>O n\u00f3 de login \u00e9 mais indicado para execu\u00e7\u00e3o de softwares de teste, ou seja, que requerem um tempo de execu\u00e7\u00e3o menor. O tempo limite para execu\u00e7\u00e3o neste n\u00f3 \u00e9 de 30 minutos, considerando que o usu\u00e1rio est\u00e1 utilizando apenas um n\u00facleo e 100% de sua capacidade. Um usu\u00e1rio que decide utilizar esse n\u00f3 para execu\u00e7\u00e3o de um processo que ocupa 100% de 10 CPUs, por exemplo, ser\u00e1 terminado automaticamente ap\u00f3s 3 minutos de uso, como pode ser visto na tabela abaixo (Figura 1), junto com demais exemplos. Isso ocorre mesmo que o script tenha tempo maior configurado. Desse modo, recomenda-se utilizar o n\u00f3 de login apenas para fins de teste.</p> <p></p> <p>Figura 1 - Tabela com exemplos de tempo de execu\u00e7\u00e3o de programas no n\u00f3 de login, para v\u00e1rias configura\u00e7\u00f5es</p> <p>Para executar um programa no supercomputador no n\u00f3 de login, use o comando a seguir no terminal do Linux:</p> <pre><code>./meuPrimeiroScript\n</code></pre> <p>Onde meuPrimeiroScript \u00e9 o script com os dados de execu\u00e7\u00e3o.</p>"},{"location":"beginner/introduction_part_2/#execucao-de-programas-no-no-de-computacao","title":"Execu\u00e7\u00e3o de programas no n\u00f3 de computa\u00e7\u00e3o","text":"<p>O N\u00f3 de Computa\u00e7\u00e3o, por sua vez, \u00e9 o mais indicado para uso de softwares gerais, que necessitem de um tempo de execu\u00e7\u00e3o elevado. \u00c9 nele que voc\u00ea ir\u00e1 utilizar a maioria dos programas dispon\u00edveis no supercomputador, podendo deix\u00e1-los em execu\u00e7\u00e3o por longos per\u00edodos. A seguir, voc\u00ea ir\u00e1 aprender como criar scripts e executar programas no n\u00f3 de computa\u00e7\u00e3o.</p> <p>Para executar um programa no supercomputador no n\u00f3 de computa\u00e7\u00e3o, use o comando a seguir no terminal do Linux:</p> <pre><code>sbatch meuPrimeiroScript\n</code></pre> <p>Onde meuPrimeiroScript \u00e9 o script com os dados de execu\u00e7\u00e3o.</p>"},{"location":"beginner/introduction_part_2/#verificacao-da-execucao","title":"Verifica\u00e7\u00e3o da execu\u00e7\u00e3o","text":"<p>Caso o job tenha sido submetido corretamente, aparecer\u00e1 a mensagem:</p> <p><code>Submitted batch job JobID</code></p> <p>Onde JobID ser\u00e1 substitu\u00eddo pelo n\u00famero que identificar\u00e1 o job. A sa\u00edda do programa n\u00e3o aparecer\u00e1 na tela. Ela ser\u00e1 escrita em um arquivo de nome slurm-JobID.out, onde JobID ser\u00e1 o mesmo valor do id do job submetido.</p> <p>Para ver a sa\u00edda do programa, use o comando:</p> <pre><code>cat slurm-JobID.out\n</code></pre> <p>Substituindo JobID pelo id do job que voc\u00ea deseja ver a sa\u00edda.</p> <pre><code>[nomeDoUsuario@service0 ~]$ sbatch meuPrimeiroScript\nSubmitted batch job 14518\n[nomeDoUsuario@service0 ~]$ cat slurm-14518.out\n120: 2 2 2 3 5\n</code></pre> <p>A execu\u00e7\u00e3o de um programa no supercomputador est\u00e1 sujeita \u00e0 disponibilidade de n\u00f3s, desse modo, o resultado pode demorar. Para acompanhar o andamento da fila, bem como a prioridade do job submetido por voc\u00ea na mesma, a se\u00e7\u00e3o de Comandos pode ser bastante \u00fatil.</p>"},{"location":"beginner/introduction_part_2/#criando-um-script-para-executar-um-programa-criado-a-partir-do-codigo-fonte","title":"Criando um script para executar um programa criado a partir do c\u00f3digo-fonte","text":"<p>Caso o programa tenha sido criado a partir do c\u00f3dido-fonte deve-se acrescentar ao nome do programa a pasta onde este se encontra.</p> <p>Exemplo:</p> <pre><code>#!/bin/bash\n#SBATCH --time=0-0:5 #especifica o tempo m\u00e1ximo de execu\u00e7\u00e3o do job, dado no padr\u00e3o dias-horas:minutos\n#SBATCH --partition=amd-512  # parti\u00e7\u00e3o para a qual o job \u00e9 enviado\n\n./helloWorld #o ponto e a barra indicam o caminho at\u00e9 a pasta atual.\n</code></pre> <p>Saiba como escolher o tempo de execu\u00e7\u00e3o aqui.</p>"},{"location":"beginner/introduction_part_2/#particoes","title":"Parti\u00e7\u00f5es","text":"<p>Parti\u00e7\u00e3o \u00e9 a forma com que os administradores do NPAD organizam/agrupam os n\u00f3s do cluster.  </p> <p>Quando submeter um job, deve-se informar qual a parti\u00e7\u00e3o em que seu job ser\u00e1 executado usando a diretiva <code>#SBATCH --partition</code>. Por exemplo:</p> <pre><code>#SBATCH --partition=amd-512\n</code></pre> <p>Na d\u00favida de qual parti\u00e7\u00e3o usar, utilize as parti\u00e7\u00f5es amd-512 ou as intel-*. Para mais informa\u00e7\u00f5es sobre parti\u00e7\u00f5es, veja a p\u00e1gina de hardware do site do NPAD: http://npad.ufrn.br. </p>"},{"location":"beginner/introduction_part_2/#executando-um-programa-criado-a-partir-do-codigo-fonte","title":"Executando um programa criado a partir do c\u00f3digo-fonte","text":"<p>Para executar um programa a partir do c\u00f3digo-fonte o usu\u00e1rio dever\u00e1, em sua pasta no supercomputador, gerar o programa execut\u00e1vel.</p> <p>Para criar o execut\u00e1vel do programa digite:</p> <pre><code>gcc helloWorld.c -o helloWorld\n</code></pre> <p>Onde helloWorld.c \u00e9 o c\u00f3digo-fonte e helloWorld \u00e9 o c\u00f3digo execut\u00e1vel.</p> <p>Para executar o programa gerado, voc\u00ea dever\u00e1 executar o script gerado anteriormente utilizando o comando <code>sbatch jobHelloWorld</code> ou <code>sbatch -p test jobHelloWorld</code> de acordo com o tipo de n\u00f3 desejado.</p> <p>Caso o job tenha sido submetido corretamente, aparecer\u00e1 a mensagem:</p> <p><code>Submitted batch job JobID</code></p> <p>Onde JobID ser\u00e1 substitu\u00eddo pelo n\u00famero que identificar\u00e1 o job. A sa\u00edda do programa n\u00e3o aparecer\u00e1 na tela. Ela ser\u00e1 escrita em um arquivo de nome slurm-JobID.out, onde JobID ser\u00e1 o mesmo valor do id do job submetido.</p> <p>Para ver a sa\u00edda do programa, use o comando:</p> <pre><code>cat slurm-JobID.out\n</code></pre> <p>Substituindo JobID pelo id do job que voc\u00ea deseja ver a sa\u00edda.</p> <pre><code>[nomeDoUsuario@service0 ~]$ gcc helloWorld.c -o helloWorld\n[nomeDoUsuario@service0 ~]$ sbatch jobHelloWorld\nSubmitted batch job 14520\n[nomeDoUsuario@service0 ~]$ cat slurm-14520.out\nHello World!\n</code></pre> <p>Para executar programas em paralelo no supercomputador, leia os tutorias de OpenMP e MPI.</p>"},{"location":"beginner/mobaxterm/","title":"Tutorial do MobaXterm","text":"<p>A vers\u00e3o do MobaXterm utilizada neste tutorial \u00e9 a 23.1</p>"},{"location":"beginner/mobaxterm/#criando-uma-sessao-com-npad","title":"Criando uma sess\u00e3o com NPAD","text":"<p>Para deixar salvo uma sess\u00e3o com NPAD, voc\u00ea ir em: session -&gt; ssh e preencha o formul\u00e1rio da seguinte maneira:</p> <ul> <li>Remote host coloque: sc2.npad.ufrn.br</li> <li>Port: 4422</li> <li>Marque a caixa Specify username e coloque o seu usu\u00e1rio</li> </ul> <p>Clique em Advanced SSH settings e marque a caixa Use private key. Verifique se a chave privada est\u00e1 correta. Se voc\u00ea criou uma chave seguindo o nosso tutorial, o nome da chave \u00e9 id_rsa e est\u00e1 localizada em C:\\Users\\NomeDoSeuUsu\u00e1rio\\AppData\\Roaming\\MobaXterm\\home\\.ssh\\id_rsa.</p> <p></p>"},{"location":"beginner/putty/","title":"Tutorial do PuTTy","text":"<p>Nesses tutoriais foi utilizado o PuTTy na sua vers\u00e3o 0.78</p>"},{"location":"beginner/putty/#gerando-um-par-de-chaves-publicoprivada-tipo-rsa","title":"Gerando um par de chaves p\u00fablico/privada tipo RSA","text":"<p>Procure nos seus aplicativos e abra o programa puttygen. Em Type of Key to generate selecione RSA. Clique em Generate.</p> <p></p> <p>Fa\u00e7a movimentos aleat\u00f3rios com o mouse at\u00e9 preencher toda a barra verde. Quando isso acontecer ir\u00e1 aparecer a seguinte tela:</p> <p></p> <p>Debaixo de Public key for pasting into OpenSSH authorized keys file: Encontra-se a chave que voc\u00ea deve copiar colocar criar uma conta no NPAD. Lembre-se de salvar duas chaves, a publica e a privada. Voc\u00ea deve salvar as chaves clicando em Save public-key e Save private-key. Se voc\u00ea n\u00e3o colocar uma passphrase ele ir\u00e1 lhe alerta isso mas n\u00e3o \u00e9 necess\u00e1rio colocar uma. LEMBRE-SE que se voc\u00ea colocar uma passphrase e esquece-la ou perder o par de chaves voc\u00ea ter\u00e1 que criar um novo par de chaves e adicionar uma nova chave.</p>"},{"location":"beginner/putty/#acessando-o-supercomputador-atraves-do-putty","title":"Acessando o supercomputador atrav\u00e9s do PuTTy","text":"<p>Uma vez tendo feito o cadastro com a chave publica gerada anteriormente. \u00c9 necess\u00e1rio configurar o PuTTy para realizar o login no supercomputador. Tr\u00eas configura\u00e7\u00f5es precisam ser feitas.</p> <ul> <li>Adicionar a chave privada que voc\u00ea criou com PuTTYgen em Connection -&gt; SSH -&gt; Auth -&gt; Credentials</li> <li>Adicionar o seu nome de usu\u00e1rio cadastrado no NPAD em Connection -&gt; Data</li> <li>Adicionar o hostname: sc2.npad.ufrn.br e port: 4422 em Session</li> </ul> <p>Certifique-se que em Session, a caixa SSH em Connection type est\u00e1 marcada. Abaixo de Saved Sessions escreva um nome dessa sess\u00e3o, como por exemplo NPAD. Clique em Save para salvar todas as configura\u00e7\u00f5es feitas at\u00e9 o momento. Dessa forma sempre que quiser acessar o NPAD atrav\u00e9s do PuTTy \u00e9 s\u00f3 selecionar o nome da sess\u00e3o, clicar em Load e depois no bot\u00e3o Open.</p> <p> </p> <p></p> <p></p>"},{"location":"beginner/rsync/","title":"Tutorial do rsync","text":"<p>Outra aplica\u00e7\u00e3o que permite copiar arquivos \u00e9 o rsync. Em alguns sistemas eles n\u00e3o vem instalado por padr\u00e3o, certifique-se de que ele esteja instalado.</p>"},{"location":"beginner/rsync/#copiar-um-arquivo-do-seu-computador-para-o-supercomputador-usando-rsync","title":"Copiar um arquivo DO SEU COMPUTADOR para o supercomputador usando rsync","text":"<p>Para copiar o arquivo meuArquivo DO SEU COMPUTADOR para o super computador usando o programa rsync. Abra um terminal Linux, use o seguinte comando:</p> <pre><code># caso tenha configurado o ~/.ssh/config \nrsync  -aP  ~/Downloads/meuArquivo super-pc:~/\n# caso n\u00e3o tenha configurado \nrsync  -aP  ~/Downloads/meuArquivo --rsh='ssh -p4422' nomeDoUsuario@sc2.npad.ufrn.br:~/\n</code></pre> <p>LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo: meuArquivo a ser\u00e1 copiado na pasta home do supercomputador. Perceba que  o arquivo meuArquivo est\u00e1 localizado na pasta Downloads.</p>"},{"location":"beginner/rsync/#copiar-um-arquivo-do-supercomputador-para-o-seu-computador-usando-rsync","title":"Copiar um arquivo DO SUPERCOMPUTADOR para o seu computador usando rsync","text":"<p>Para copiar o arquivo: meuArquivo DO SUPERCOMPUTADOR para o seu computador na pasta Downloads usando o programa rsync. Abra um terminal Linux, use o seguinte comando:</p> <pre><code># caso tenha configurado o ~/.ssh/config \nrsync super-pc:~/meuArquivo  ~/Downloads\n# caso n\u00e3o tenha configurado \nrsync --rsh='ssh -p4422' -aP nomeDoUsuario@sc2.npad.ufrn.br:~/meuArquivo  ~/Downloads\n</code></pre> <p>LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador</p>"},{"location":"beginner/scp/","title":"Tutorial do scp","text":"<p>scp \u00e9 um aplica\u00e7\u00e3o  que copia arquivos entre dois computadores pela internet atrav\u00e9s do ssh. \u00c9 poss\u00edvel utilizar o scp tanto no Linux quando no Windows.</p>"},{"location":"beginner/scp/#copiar-um-arquivo-do-seu-computador-para-o-supercomputador-usando-scp","title":"Copiar um arquivo DO SEU COMPUTADOR para o supercomputador usando scp","text":"<p>Para copiar o arquivo meuArquivo DO SEU COMPUTADOR para o super computador usando o programa scp. Abra um terminal Linux, use o seguinte comando:</p> <pre><code># caso tenha configurado o ~/.ssh/config \nscp -r meuArquivo super-pc:~/\n# caso n\u00e3o tenha configurado \nscp -r -P4422 meuArquivo nomeDoUsuario@sc2.npad.ufrn.br:~/\n</code></pre> <p>LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador.</p>"},{"location":"beginner/scp/#copiar-um-arquivo-do-supercomputador-para-o-seu-computador-usando-scp","title":"Copiar um arquivo DO SUPERCOMPUTADOR para o seu computador usando scp","text":"<p>Para copiar o arquivo: meuArquivo DO SUPERCOMPUTADOR para o seu computador na pasta Downloads usando o programa scp. Abra um terminal Linux, use o seguinte comando:</p> <pre><code># caso tenha configurado o ~/.ssh/config \nscp -r super-pc:~/meuArquivo  ~/Downloads\n# caso n\u00e3o tenha configurado \nscp -r -P4422  nomeDoUsuario@sc2.npad.ufrn.br:~/meuArquivo  ~/Downloads\n</code></pre> <p>LEMBRE-SE de substituir o nomeDoUsuario para o seu usu\u00e1rio. Perceba que o arquivo a ser copiado est\u00e1 na pasta home do supercomputador.</p>"},{"location":"beginner/winscp/","title":"Copiando arquivos atrav\u00e9s de uma interface gr\u00e1fica WinSCP (Windows)","text":"<p>Ao contr\u00e1rio do linux, o windows explorer n\u00e3o possui suporte ao scp, tendo que fazer uso de um programa de terceiros chamado WinSCP. Para instalar o WinSCP v\u00e1 no site oficial e clique em Download. Ap\u00f3s a finaliza\u00e7\u00e3o do download, clique no execut\u00e1vel e instale a aplica\u00e7\u00e3o. Ao executar a aplica\u00e7\u00e3o ir\u00e1 aparecer a seguinte interface:</p> <p> </p> <ul> <li> <p>HostName coloque: sc2.npad.ufrn.br</p> </li> <li> <p>Port number coloque 4422 </p> </li> <li> <p>User name coloque o seu nome de usu\u00e1rio</p> </li> <li> <p>IMPORTANTE: deixe o campo Password vazio</p> </li> </ul> <p>Depois clique em Advanced...</p> <p></p> <p>Em Advanced, v\u00e1 em Authentication, um submenu de SSH. Procure por Private key file e clique nos tr\u00eas pontinhos (...). Agora voc\u00ea precisa procurar a chave privada que foi criada anteriormente, no meu caso o nome dela \u00e9 id_rsa.</p> <p></p>"},{"location":"beginner/winscp/#putty","title":"PuTTy","text":"<p>Caso voc\u00ea esteja utilizando o Putty v\u00e1 onde voc\u00ea salvou a chave privada pelo PuttyGen.</p>"},{"location":"beginner/winscp/#openssh-windows-powershell","title":"OpenSSH + Windows PowerShell","text":"<p>Caso voc\u00ea tenha esteja usando OpenSSH e Windows PowerShell e n\u00e3o tenha mudado o nome da chave, o caminho at\u00e9 a chave privada \u00e9 C:\\Users\\NomeDoSeuUsu\u00e1rio\\.ssh\\id_rsa. Para visualiza-l\u00e1 ter\u00e1 que selecionar a op\u00e7\u00e3o: todos os tipos de arquivo.</p>"},{"location":"beginner/winscp/#mobaxterm","title":"MobaXterm","text":"<p>No caso em que voc\u00ea esteja usando o MobaXterm, saiba que o local padr\u00e3o em que o MobaXterm salva as chaves \u00e9 C:\\Users\\NomeDoSeuUsu\u00e1rio\\AppData\\Roaming\\MobaXterm\\home\\.ssh.  Para visualizar a chave ter\u00e1 que selecionar a op\u00e7\u00e3o: todos os tipos de arquivo.</p> <p>Se tudo der certo o WinSCP ir\u00e1 pedir para converter a Chave em formato OpenSSH para um formado PuTTY caso precise. Concorde, converta, salve e sa\u00edda das configura\u00e7\u00f5es avan\u00e7adas atrav\u00e9s do bot\u00e3o OK. Quando voc\u00ea pressionar o bot\u00e3o Login. Voc\u00ea ter\u00e1 acesso ao seu sistema de arquivos do supercomputador. O WinSCP ir\u00e1 parecer como uma dessas duas telas dependendo do layout escolhido durante a instala\u00e7\u00e3o.</p> <p></p> <p></p>"},{"location":"beginner/winscp/#informacoes-extras-sobre-a-aplicacao-utilizada","title":"Informa\u00e7\u00f5es extras sobre a aplica\u00e7\u00e3o utilizada","text":"<ul> <li>WinSCP: winscp docs</li> </ul>"},{"location":"intermediate/conda/","title":"Tutorial do Conda (miniconda ou Anaconda)","text":"<ul> <li>O que \u00e9 conda?</li> <li>Como usar os ambientes virtuais</li> <li>Submiss\u00e3o de jobs usando os ambientes virtuais</li> <li>Cria\u00e7\u00e3o de ambientes virtuais</li> <li>Gerenciando os ambientes virtuais</li> <li>Criando um ambiente a partir de um arquivo</li> <li>Acelerando a instala\u00e7\u00e3o de pacotes com mamba</li> <li>Sobre o uso de pip e conda juntos</li> </ul>"},{"location":"intermediate/conda/#o-que-e-conda","title":"O que \u00e9 conda?","text":"<p>\u00c9 um gerenciador de pacotes e um sistema de gerenciamento de ambientes virtuais para v\u00e1rias linguagens como: Python, R, Ruby, Lua, Scala, Java, JavaScript, Go, C/ C++, FORTRAN. \u00c9 tamb\u00e9m um software de c\u00f3digo aberto e pode ser utilizado no Windows, Linux e MacOs. </p> <p>Atualmente, o conda \u00e9 muito popular como gerenciador de pacotes do Python and R.  Originalmente, o conda foi desenvolvido para superar as dificuldades enfrentadas pelos cientistas de dados no gerenciamento de pacotes do Python (Wikipedia: Conda).</p>"},{"location":"intermediate/conda/#como-usar-os-ambientes-virtuais","title":"Como usar os ambientes virtuais","text":"<p>No NPAD, existem diversos ambientes virtuais do conda com pacotes do Python pr\u00e9-instalados a fim de atender os usu\u00e1rios.</p> <p>Para visualizar os ambientes conda dispon\u00edveis no NPAD,  digite:</p> <pre><code>$ conda env list\n\n# conda environments:\n#\nbioinformatics           /opt/npad/shared/conda/bioinformatics\nbiology                  /opt/npad/shared/conda/biology\nforge                    /opt/npad/shared/conda/forge\nforge2023                /opt/npad/shared/conda/forge2023\ngeral                    /opt/npad/shared/conda/geral\ngpu                      /opt/npad/shared/conda/gpu\nbase                  *  /usr\n</code></pre> <p>A lista de ambientes mostrada pode ser diferente da lista acima por que os ambientes est\u00e3o sempre sendo atualizados.</p> <p>Para ativar, por exemplo, o ambiente virtual forge2023 da lista acima, digite:</p> <pre><code>$ conda activate forge2023\n</code></pre> <p>Uma vez ativado, o usu\u00e1rio j\u00e1 pode utilizar o python e os pacotes dispon\u00edveis naquele ambiente. A vers\u00e3o do python e dos pacotes variam de ambiente para ambiente.</p> <p>Para visualizar os pacotes dispon\u00edveis, digite:</p> <pre><code>$ conda list\n</code></pre> <p>Para desativar um ambiente, digite:</p> <pre><code>$ conda deactivate \n</code></pre>"},{"location":"intermediate/conda/#submissao-de-jobs-usando-os-ambientes-virtuais","title":"Submiss\u00e3o de jobs usando os ambientes virtuais","text":"<p>O ambiente virtual deve ser ativado antes da submiss\u00e3o do job ao supercomputador. Por exemplo, considere o seguinte script de job chamado de my-job.sh:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512  # parti\u00e7\u00e3o para a qual o job \u00e9 enviado\n#SBATCH --time=0-00:10\n#SBATCH --nodes=1\n\npython meuprograma.py \n</code></pre> <p>Para submeter my-job.sh ao supercomputador usando o ambiente virtual forge2023, digite:</p> <pre><code>$ conda activate forge2023\n$ sbatch my-job.sh\n</code></pre> <p>Caso seja necess\u00e1rio ativar o ambiente no pr\u00f3prio script de job,  voc\u00ea pode usar o seguinte exemplo: </p> <pre><code>#!/bin/bash\n#SBATCH --time=0-00:10\n#SBATCH --nodes=1\n\neval \"$(conda shell.bash hook)\" # inclua esta linha antes executar o conda\nconda activate forge2023\npython meuprograma.py \n</code></pre>"},{"location":"intermediate/conda/#criacao-de-ambientes-virtuais","title":"Cria\u00e7\u00e3o de ambientes virtuais","text":"<p>Se os ambientes dispon\u00edveis no sistema n\u00e3o forem satisfat\u00f3rios para o usu\u00e1rio, ele mesmo pode criar seu pr\u00f3prio ambiente virtual.</p> <p>Para criar um ambiente virtual b\u00e1sico digite:</p> <pre><code>$ conda create --name meu-amb python\n</code></pre> <p>A op\u00e7\u00e3o <code>--name</code> \u00e9 usada para definir o nome do ambiente virtual. Neste exemplo, foi utilizado o nome meu-amb. Em seguida, pode-se especificar uma lista de pacotes a serem instalados . Por exemplo:</p> <pre><code>$ conda create --name meu-amb python numpy pandas matplotlib\n</code></pre> <p>Usando o sinal de igual \"=\", pode-se definir a vers\u00e3o do Python ou do pacote a ser instalado:</p> <pre><code>$ conda create --name meu-amb38 python=3.8 numpy=1.23.4 pandas\n</code></pre> <p>Dica</p> <p>Apesar de poder adicionar pacotes ao ambiente ap\u00f3s a sua cria\u00e7\u00e3o, \u00e9 recomendado instalar todos os pacotes ao mesmo tempo para evitar conflitos com depend\u00eancia de pacotes. </p>"},{"location":"intermediate/conda/#gerenciando-os-ambientes-virtuais","title":"Gerenciando os ambientes virtuais","text":"<p>Para gerenciar um ambiente virtual, primeiro ative ele. Por exemplo:</p> <pre><code>$ conda activate meu-amb\n</code></pre> <p>Ent\u00e3o, pode-se adicionar mais pacotes ao ambiente ativado. Por exemplo:</p> <pre><code>$ conda install pacote1 pacote2 \n</code></pre> <p>O conda procura os pacotes para instalar nos reposit\u00f3rios padr\u00e3o (default). Para instalar de pacote de outros reposit\u00f3rios, basta especificar o nome do reposit\u00f3rio usando a op\u00e7\u00e3o <code>--channel</code>.  Por exemplo,</p> <pre><code>$ conda install --channel=conda-forge pacote1 pacote2 \n</code></pre> <p>Para remover um pacote espec\u00edfico, use o comando <code>conda remove</code> seguido do nome do pacote a ser removido. Exemplo:</p> <pre><code>$ conda remove pacote1 \n</code></pre> <p>Para remover um ambiente virtual, primeiro desative o ambiente e em seguida remova o ambiente como neste exemplo: </p> <pre><code>$ conda deactivate \n$ conda remove --name meu-amb --all \n</code></pre>"},{"location":"intermediate/conda/#criando-um-ambiente-a-partir-de-um-arquivo","title":"Criando um ambiente a partir de um arquivo","text":"<p>Pode-se criar um ambiente a partir de um arquivo <code>environment.yml</code> contendo a rela\u00e7\u00e3o de pacotes a serem instalados. Por exemplo</p> <pre><code>name: meu-amb\ndependencies:\n  - numpy\n  - pandas\n</code></pre> <p>Ent\u00e3o crie o ambiente de nome meu-amb, digitando:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>Pode-se especificar tamb\u00e9m canais de reposit\u00f3rios e vers\u00f5es do pacote. Por exemplo</p> <pre><code>name: meu-amb\nchannels:\n  - conda-forge\ndependencies:\n  - numpy=1.23.4\n  - pandas\n  - python=3.8 \n  - matplotlib\n</code></pre>"},{"location":"intermediate/conda/#acelerando-a-instalacao-de-pacotes-com-mamba","title":"Acelerando a instala\u00e7\u00e3o de pacotes com mamba","text":"<p>A instala\u00e7\u00e3o de pacotes pelo conda pode demorar bastante devido a resolu\u00e7\u00e3o de depend\u00eancias. Neste caso, voc\u00ea pode usar o mamba para acelerar a instala\u00e7\u00e3o. O mamba \u00e9 uma reimplementa\u00e7\u00e3o do conda em linguagem C++ para tornar mais eficiente e r\u00e1pido a instala\u00e7\u00e3o dos pacotes do conda.</p> <p>O Mamba pode ser instalado como se fosse um pacote:</p> <pre><code>$ conda install mamba\n</code></pre> <p>Mamba utiliza uma linha de comando compat\u00edvel com a do conda. Ou seja, usar  mamba \u00e9 similar a usar conda. Exemplos:</p> <p>Instala\u00e7\u00e3o de pacotes:</p> <pre><code>$ mamba install pacote1\n</code></pre> <p>Remo\u00e7\u00e3o de pacotes:</p> <pre><code>$ mamba remove pacote1\n</code></pre> <p>Lista de pacotes instalados:</p> <pre><code>$ mamba list\n</code></pre>"},{"location":"intermediate/conda/#sobre-o-uso-de-pip-e-conda-juntos","title":"Sobre o uso de pip e conda juntos","text":"<p>pip \u00e9 o instalador de pacotes do Python. \u00c9 possivel usar pip para instalar pacotes do Python em um ambiente virtual do conda. Geralmente, pip j\u00e1 est\u00e1 instalado quando for criado um ambiente virtual. Caso n\u00e3o esteja, \u00e9 s\u00f3 instalar:</p> <pre><code>conda install pip\n</code></pre> <p>De acordo com o site do conda, podem surgir problemas ao usar pip e conda juntos. Somente ap\u00f3s o conda ter sido usado para instalar tantos pacotes quanto poss\u00edvel, o pip deve ser usado para instalar qualquer pacote restante. Se forem necess\u00e1rias modifica\u00e7\u00f5es no ambiente, \u00e9 melhor criar um novo ambiente em vez de executar conda ap\u00f3s pip.</p>"},{"location":"intermediate/gpu/","title":"Uso da GPU","text":"<p>O NPAD disp\u00f5e de duas parti\u00e7\u00f5es para uso da GPU: A gpu-4-a100 com 4 GPUS e a gpu-8-v100 com 8 GPUS. Consulte a p\u00e1gina de Hardware para mais informa\u00e7\u00f5es sobre estas GPUS.</p> <p>Para requisitar uma ou mais gpus para um job, utilize a diretiva <code>gpus-per-node</code>.</p> <p>Um exemplo de script de job para usar duas GPUs da parti\u00e7\u00e3o gpu-8-v100:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=gpu-8-v100 \n#SBATCH --gpus-per-node=2   # N\u00famero GPUs por n\u00f3\n#SBATCH --time=3:00\n\n./meuprograma # use nvidia-smi para teste\n</code></pre> <p>Exemplo de script para um job com m\u00faltiplos threads usando uma GPU da parti\u00e7\u00e3o gpu-4-a100</p> <pre><code>#!/bin/bash\n#SBATCH --partition=gpu-4-a100 \n#SBATCH --gpus-per-node=1  # N\u00famero GPUs por n\u00f3\n#SBATCH --cpus-per-task=6        \n#SBATCH --time=0-03:00\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n./meuprograma\n</code></pre>"},{"location":"intermediate/gpu/#uso-do-pytorch","title":"Uso do Pytorch","text":"<p>O Pytorch para uso em GPUS est\u00e1 instalado no ambiente conda gpu. Voc\u00ea pode ativ\u00e1-lo com <pre><code>$ conda activate gpu\n</code></pre></p> <p>\u00c9 importante saber que o Pytorch usa somente uma GPU por default. Para usar m\u00faltiplas GPU veja a discuss\u00e3o na se\u00e7\u00e3o seguinte que tamb\u00e9m serve para o software TensorFlow.</p>"},{"location":"intermediate/gpu/#treinamento-da-rede-neural-usando-multiplas-gpus","title":"Treinamento da rede neural usando m\u00faltiplas GPUs","text":"<p>H\u00e1 duas maneiras b\u00e1sicas (que podem ser combinadas) de realizar um treinamento da rede neural com m\u00faltiplas GPUs: paralelismo de dados e paralelismo de modelo.</p> <p>No paralelismo de dados, o modelo da rede neural \u00e9 replicado em cada GPU e cada r\u00e9plica processa um diferente \"batch\" de dados. Em seguida, os resultados s\u00e3o reunidos. Pytorch disp\u00f5e de classes que facilitam a implementa\u00e7\u00e3o deste m\u00e9todo. Veja mais em:</p> <ul> <li> <p>Data Parallelism</p> </li> <li> <p>Multi-GPU Examples</p> </li> <li> <p>Getting started with distributed data parallel</p> </li> </ul> <p>No paralelismo de modelo, diferentes partes de um mesmo modelo de rede neurais s\u00e3o executadas em diferentes GPUS, mas processando o mesmo \"batch\" de dados. Veja mais em:</p> <ul> <li>Model Parallel</li> </ul>"},{"location":"intermediate/install_apps/","title":"Instala\u00e7\u00e3o de programas no supercomputador","text":"<p>No supercomputador, h\u00e1 diversos programas instalados para todos os usu\u00e1rios. Caso deseje instalar um novo software no supercomputador, voc\u00ea pode solicitar que a equipe do NPAD fa\u00e7a a instala\u00e7\u00e3o, atrav\u00e9s da nossa P\u00e1gina de Solicita\u00e7\u00e3o de Software. Entretanto, esse pedido de software ser\u00e1 analisado e apenas softwares a serem utilizados por diversos pesquisadores ser\u00e3o aprovados. Os software aprovados entrar\u00e3o na fila de tarefas da equipe do NPAD e ser\u00e3o instalados assim que poss\u00edvel. Caso sua necessidade de instala\u00e7\u00e3o seja urgente ou o software que voc\u00ea deseja instalar n\u00e3o seja largamente utilizado por pesquisadores, voc\u00ea poder\u00e1 instalar o programa desejado na sua pasta pessoal seguindo uma das duas formas de instala\u00e7\u00e3o. Instala\u00e7\u00e3o manual ou atrav\u00e9s do gerenciador de pacotes</p> <ul> <li>Instala\u00e7\u00e3o manual (recomendado)<ul> <li>Exemplo: instala\u00e7\u00e3o manual do programa htop</li> </ul> </li> <li>Instalando programas utilizando o yumdownloader<ul> <li>Exemplo: Instala\u00e7\u00e3o do htop atrav\u00e9s do yumdownloader</li> </ul> </li> </ul>"},{"location":"intermediate/install_apps/#instalacao-manual-recomendado","title":"Instala\u00e7\u00e3o manual (recomendado)","text":"<p>A instala\u00e7\u00e3o manual se resume em baixar, compilar o c\u00f3digo fonte e instalar na pasta home. A instala\u00e7\u00e3o manual \u00e9 a recomendada pois dessa forma voc\u00ea est\u00e1 livre para realizar as modifica\u00e7\u00f5es que quiser, escolher a vers\u00e3o que quiser sem a necessidade de interven\u00e7\u00e3o da equipe NPAD. No entanto, exige um conhecimento avan\u00e7ado sobre Linux que muitos clientes n\u00e3o possuem.</p>"},{"location":"intermediate/install_apps/#exemplo-instalacao-manual-do-programa-htop","title":"Exemplo: instala\u00e7\u00e3o manual do programa htop","text":"<p>Baixando o htop. No meu caso estou na minha pasta home, ao executar o comando.</p> <pre><code>wget https://github.com/htop-dev/htop/archive/refs/tags/3.2.2.tar.gz\n</code></pre> <p>ao finalizar o download terei baixado o arquivo: 3.2.2.tar.gz que representa o c\u00f3digo do htop na sua vers\u00e3o 3.2.2. Posso visualizar o arquivos com o comando ls.</p> <pre><code># comando ls na minha pasta home\nls\n3.2.2.tar.gz  etc  libvips  pascal-parsec  pkgs  scratch \n</code></pre> <p>O arquivo est\u00e1 compactado no formato .tar.gz. Ent\u00e3o vou descompactar o arquivo.</p> <pre><code>tar -xf 3.2.2.tar.gz \n</code></pre> <p>Listando o diret\u00f3rio novamente verei que agora tenho uma pasta chamada htop-3.2.2.</p> <pre><code>ls\n3.2.2.tar.gz  etc  htop-3.2.2  libvips  pascal-parsec  pkgs  scratch \n</code></pre> <p>Entro do diret\u00f3rio do htop-3.2.2 e inicio o processo de configura\u00e7\u00e3o e compila\u00e7\u00e3o do pacote seguindo a documenta\u00e7\u00e3o do projeto https://github.com/htop-dev/htop#compile-from-source</p> <pre><code>cd  htop-3.2.2;\n</code></pre> <p>Segundo a documenta\u00e7\u00e3o preciso executar 3 comandos autogen.sh, ./configure e make. Sendo o \u00faltimo comando a  compila\u00e7\u00e3o. Segundo a documenta\u00e7\u00e3o o comando ./configure, aceita o par\u00e2metro --prefix referente ao local de instala\u00e7\u00e3o do pacote. Como n\u00e3o se pode instalar nenhum programa globalmente no supercomputador, ent\u00e3o deve-se passar um outro diret\u00f3rio. Segundo a freedesktop.org \u00e9 recomendado instalar aplica\u00e7\u00f5es de usu\u00e1rio na pasta oculta: ~/.local ou  $HOME/.local, caso n\u00e3o exista essa pasta, crie.</p> <pre><code># configurando e compilando o htop\n./autogen.sh &amp;&amp; ./configure --prefix=$HOME/.local &amp;&amp; make\n# instalando o htop\nmake install\n</code></pre> <p>Agora posso executar htop da seguinte forma</p> <pre><code>~/.local/bin/htop\n</code></pre> <p>Como voc\u00ea instalou o htop na pasta padr\u00e3o do freedesktop, ent\u00e3o se voc\u00ea relogar ou executar o comando</p> <pre><code>source ~/.bashrc\n</code></pre> <p>ir\u00e1 perceber que o htop est\u00e1 dispon\u00edvel nas suas vari\u00e1veis de ambiente ou seja, pode executar o htop da seguinte forma:</p> <pre><code>htop --version\nhtop 3.2.2\n</code></pre> <p>perceba que a vers\u00e3o que vem instalada por padr\u00e3o \u00e9 a 3.0.5</p> <pre><code>/bin/htop --version\nhtop 3.0.5\n</code></pre>"},{"location":"intermediate/install_apps/#instalando-programas-utilizando-o-yumdownloader","title":"Instalando programas utilizando o yumdownloader","text":"<p>Ao acessar sua pasta no supercomputador, execute o seguinte comando para baixar o seu programa.</p> <pre><code>yumdownloader &lt;nome do programa&gt;\n</code></pre> <p>O yumdownloader ir\u00e1 procurar em diferentes reposit\u00f3rios  uma vers\u00e3o empacotada .rpm do programa que voc\u00ea pesquisou. Caso voc\u00ea encontre o pacote \u00e9 poss\u00edvel instalar o programa na sua pasta home com os programas: <code>rpm2cpio</code> e <code>cpio</code>, atrav\u00e9s do seguinte comando:</p> <pre><code>rpm2cpio &lt;nome do arquivo baixado&gt;.rpm | cpio -idv \n</code></pre> <p>Se tudo der certo o programa ser\u00e1 instalado no caminho relativo: <code>./usr</code>.</p>"},{"location":"intermediate/install_apps/#exemplo-instalacao-do-htop-atraves-do-yumdownloader","title":"Exemplo: Instala\u00e7\u00e3o do htop atrav\u00e9s do yumdownloader","text":"<p>Procure o htop nos reposit\u00f3rios com <code>yumdownloader</code></p> <pre><code>yumdownloader htop\n</code></pre> <p>Caso yumdownloader encontre o pacote ele ir\u00e1 baix\u00e1-lo para voc\u00ea</p> <pre><code>ls\netc  htop-3.2.1-1.el8.x86_64.rpm  libvips  pascal-parsec  pkgs  scratch \n</code></pre> <p>no caso do htop ele encontrou uma vers\u00e3o mais antiga da aplica\u00e7\u00e3o a vers\u00e3o 3.2.1. Para instalar o htop no diret\u00f3rio: <code>./usr</code>,  basta usar o comando:</p> <pre><code>rpm2cpio  htop-3.2.1-1.el8.x86_64.rpm | cpio -idv \n</code></pre> <p>Perceba que ele criou uma pasta usr no local onde voc\u00ea est\u00e1 e instalar o htop nessa pasta</p> <pre><code>ls \netc  htop-3.2.1-1.el8.x86_64.rpm  libvips  pascal-parsec  pkgs  scratch  usr\n</code></pre> <p>Como eu utilizei o comando <code>rpm2cpio</code>  na minha pasta home. Ent\u00e3o posso posso executar htop da seguinte forma</p> <pre><code>~/usr/bin/htop\n</code></pre>"},{"location":"intermediate/introduction_part_3/","title":"Introdu\u00e7\u00e3o ao supercomputador - Parte\u00a03","text":"<p>Nesse tutorial iremos aprender a fun\u00e7\u00e3o de algumas op\u00e7\u00f5es que podem ser inseridas nos scripts enviados para o supercomputador. Caso surja alguma d\u00favida durante o tutorial, sinta-se \u00e0 vontade para entrar em contato atrav\u00e9s do e-mail atendimento\\npad.ufrn.br (substituindo \\ por @). <ul> <li>Script para multithreading</li> <li>Script para utiliza\u00e7\u00e3o de v\u00e1rios n\u00f3s</li> <li>Compartilhamento dos n\u00f3s</li> <li>Escolha da qualidade de servi\u00e7o e Limite no uso do supercomputador<ul> <li>QOS 1</li> <li>QOS 2</li> <li>preempt</li> </ul> </li> <li>Receber e-mails sobre in\u00edcio e fim da execu\u00e7\u00e3o</li> <li>Definir a quantidade de mem\u00f3ria a ser utilizada</li> <li>Carregando softwares dispon\u00edveis</li> <li>Utilizando a sua pasta home</li> <li>Utilizando o scratch global</li> <li>Backfill e escolha do tempo de execu\u00e7\u00e3o</li> </ul>"},{"location":"intermediate/introduction_part_3/#script-para-multithreading","title":"Script para multithreading","text":"<p>Por padr\u00e3o, quando n\u00e3o \u00e9 definido o n\u00famero de n\u00facleos a ser utilizado, o supercomputador executar\u00e1 o job em apenas um um n\u00facleo do n\u00f3. Para que seu programa execute em mais de um n\u00facleo, \u00e9 necess\u00e1rio definir no script <code>#SBATCH --cpus-per-task</code>, da seguinte forma:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=multithreading_example\n#SBATCH --time=0-0:5\n#SBATCH --partition=amd-512\n#SBATCH --cpus-per-task=32\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n./hello_threads \n</code></pre> <p>A op\u00e7\u00e3o <code>#SBATCH --cpus-per-task=32</code> est\u00e1 definindo 32 cores ou n\u00facleos para esse job. A vari\u00e1vel de ambiente <code>SLURM_CPUS_PER_TASK</code> fornece a voc\u00ea o n\u00famero de cores que seu job ter\u00e1 durante a execu\u00e7\u00e3o do programa.</p>"},{"location":"intermediate/introduction_part_3/#script-para-utilizacao-de-varios-nos","title":"Script para utiliza\u00e7\u00e3o de v\u00e1rios n\u00f3s","text":"<p>Novamente, por padr\u00e3o, quando n\u00e3o \u00e9 definido o n\u00famero de n\u00f3s a ser utilizado, o supercomputador executar\u00e1 o job em apenas um n\u00f3 e em um n\u00facleo desse n\u00f3. Para que seu programa execute em mais de um n\u00f3, \u00e9 necess\u00e1rio definir no script da seguinte forma:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512  # parti\u00e7\u00e3o para a qual o job \u00e9 enviado\n#SBATCH --nodes=2 #n\u00famero de n\u00f3s\n#SBATCH --ntasks-per-node=2 #n\u00famero de tarefas por n\u00f3\n#SBATCH --hint=compute_bound\n\n#programa a ser executado.\n#srun: executa jobs em paralelo\nsrun prog1 \n\n#alternativa para casos em que srun n\u00e3o funciona:\nmpirun prog1\n</code></pre> <p>Onde <code>#SBATCH --nodes</code> indica a quantidade de n\u00f3s a ser utilizada, podendo tamb\u00e9m ser definido com <code>#SBATCH -N</code>. Tamb\u00e9m se pode definir o n\u00famero de tarefas por n\u00f3 e a quantidade de cpus por tarefas, essas configura\u00e7\u00f5es est\u00e3o relacionadas a paraleliza\u00e7\u00e3o com MPI. Para isso \u00e9 necess\u00e1rio primeiramente definir o n\u00famero de tarefas com a op\u00e7\u00e3o <code>#SBATCH --ntasks</code> ou <code>#SBATCH -n</code>, que define o total de tarefas, ou <code>#SBATCH --ntasks-per-node</code> que define o n\u00famero de tarefas por n\u00f3.  A op\u00e7\u00e3o <code>#SBATCH --hint=compute_bound</code> muda a configura\u00e7\u00e3o para ser um thread por core e pode trazer benef\u00edcios de performance quando se usa apenas paralelismo de mem\u00f3ria distribu\u00edda.</p> <p>Cada tarefa \u00e9 um processo, ou seja, um programa em execu\u00e7\u00e3o. Ent\u00e3o iniciar 10 tarefas em 2 n\u00f3s significa iniciar 10 processos em paralelo em cada um dos n\u00f3s, para um total de 20 processos. Se esses processos n\u00e3o usam MPI, ou alguma outra forma de comunica\u00e7\u00e3o entre si, h\u00e1 o risco de que eles apenas executem o mesmo trabalho 20 vezes.</p> <p>O comando srun \u00e9 quase sempre equivalente a mpirun: sua fun\u00e7\u00e3o \u00e9 efetivamente iniciar as tarefas com os recursos que foram alocados. No entanto, em casos em que se usam vers\u00f5es de MPI incompat\u00edveis com Slurm, o comando srun poder\u00e1 ter efeitos diferentes de mpirun. Em caso de d\u00favidas, recomenda-se testar cada caso.</p>"},{"location":"intermediate/introduction_part_3/#compartilhamento-dos-nos","title":"Compartilhamento dos n\u00f3s","text":"<p>Outro padr\u00e3o do supercomputador \u00e9, ao submeter um job o n\u00f3 n\u00e3o ser\u00e1 reservado exclusivamente para aquele job, podendo ser alocado mais jobs dependendo da disponibilidade dos recursos naquele n\u00f3. Caso seu programa necessite de um n\u00f3 por completo, utilize a op\u00e7\u00e3o <code>#SBATCH --exclusive</code>. Por exemplo, para um programa que ser\u00e1 executado em paralelo, o desempenho do programa ser\u00e1 melhor se o programa puder utilizar os recursos por completo.</p>"},{"location":"intermediate/introduction_part_3/#escolha-da-qualidade-de-servico-e-limite-no-uso-do-supercomputador","title":"Escolha da qualidade de servi\u00e7o e Limite no uso do supercomputador","text":"<p>O usu\u00e1rio poder\u00e1 enviar m\u00faltiplos jobs para o supercomputador. Por\u00e9m, para permitir que mais pesquisadores compartilhem esse recurso com menos tempo de espera foram impostos limites no uso do supercomputador a partir da qualidade do servi\u00e7o em ingl\u00eas Quality of Service (QOS) utilizado. Comando a ser acrescentado no script de execu\u00e7\u00e3o:</p> <pre><code>#SBATCH --qos=qosN #Subistitua N pelo tipo de QOS desejado\n</code></pre>"},{"location":"intermediate/introduction_part_3/#qos-1","title":"QOS 1","text":"<p>O QOS 1 \u00e9 o QOS padr\u00e3o, com ele o usu\u00e1rio poder\u00e1 enviar at\u00e9 100 jobs para o supercomputador, sendo somente 4 jobs em execu\u00e7\u00e3o ou 256 n\u00facleos f\u00edsicos (4 n\u00f3s completos) em utiliza\u00e7\u00e3o, o que ocorrer primeiro. O job tem limite de tempo de at\u00e9 2 dias Ex.:</p> <pre><code> #!/bin/bash\n #SBATCH --partition=amd-512\n #SBATCH --time=0-0:5\n\n ./prog1\n</code></pre>"},{"location":"intermediate/introduction_part_3/#qos-2","title":"QOS 2","text":"<p>O QOS 2 \u00e9 mais indicado para mais jobs que utilizam um n\u00f3 inteiro ou jobs que utilizam alguns poucos n\u00f3s. Com ele, o usu\u00e1rio poder\u00e1 colocar 100 jobs na fila, mas apenas 1 job em execu\u00e7\u00e3o  com at\u00e9 256 n\u00facleos f\u00edsicos (4 n\u00f3s completos) em utiliza\u00e7\u00e3o, o que ocorrer primeiro. Tendo o job o limite de tempo de at\u00e9 7 dias Ex.:</p> <pre><code> #!/bin/bash\n #SBATCH --partition=amd-512\n #SBATCH --time=0-0:5\n #SBATCH --qos=qos2\n\n ./prog1\n</code></pre>"},{"location":"intermediate/introduction_part_3/#preempt","title":"preempt","text":"<p>Para trabalhos que necessitem rodar v\u00e1rios jobs simultaneamente , ou job que precisa de mais de at\u00e9 20 dias para ser executado. Recomenda-se utilizar preempt, nele o usu\u00e1rio pode deixar rodando at\u00e9 100 jobs simult\u00e2neos. No entanto, por falta de recurso o seus jobs poder\u00e3o ser cancelados a qualquer momento. Ex:</p> <pre><code> #!/bin/bash\n #SBATCH --partition=amd-512\n #SBATCH --time=0-0:5\n #SBATCH --qos=preempt\n\n ./prog1\n</code></pre>"},{"location":"intermediate/introduction_part_3/#receber-e-mails-sobre-inicio-e-fim-da-execucao","title":"Receber e-mails sobre in\u00edcio e fim da execu\u00e7\u00e3o","text":"<p>Caso deseje receber notifica\u00e7\u00f5es por e-mail sobre in\u00edcio e fim de execu\u00e7\u00e3o, utiliza-se a op\u00e7\u00e3o --mail-type onde se pode definir que tipo de notifica\u00e7\u00e3o deseja receber. Se for definido <code>ALL</code>, as notifica\u00e7\u00f5es recebidas ser\u00e3o sobre <code>BEGIN</code>, <code>END</code>, <code>FAIL</code>, <code>REQUEUE</code> e <code>STAGE_OUT</code>. Caso deseje apenas um evento espec\u00edfico utilize uma das op\u00e7\u00f5es, sendo as op\u00e7\u00f5es dispon\u00edveis: <code>NONE</code>, <code>BEGIN</code>, <code>END</code>, <code>FAIL</code>, <code>REQUEUE</code>, <code>STAGE_OUT</code>, <code>TIME_LITMIT</code>, <code>TIME_LIMIT_90</code> (alcan\u00e7ou 90% do tempo limite), <code>TIME_LIMIT_50</code> e <code>ARRAY_TASKS</code> (enviar e-mail para cada array task).</p> <p>Tamb\u00e9m \u00e9 necess\u00e1rio definir o e-mail que ir\u00e1 receber as notifica\u00e7\u00f5es com <code>#SBATCH --mail-user</code>.</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512\n#SBATCH --mail-user=meuemail@mail.com\n#SBATCH --mail-type=ALL\n\n./prog1\n</code></pre>"},{"location":"intermediate/introduction_part_3/#definir-a-quantidade-de-memoria-a-ser-utilizada","title":"Definir a quantidade de mem\u00f3ria a ser utilizada","text":"<p>O supercomputador est\u00e1 configurado para atribuir, no m\u00ednimo, 2GB de mem\u00f3ria por thread (cpu). Caso queira modificar a quantidade de mem\u00f3ria padr\u00e3o, voc\u00ea pode utilizar a op\u00e7\u00e3o <code>#SBATCH --mem-per-cpu</code>, como demonstrado no exemplo a seguir:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512\n#SBATCH --mem-per-cpu=1000\n#SBATCH --cpus-per-task=3\n\n./prog1\n</code></pre> <p>O script do exemplo utiliza 3 cpus e para cada cpu \u00e9 reservado aproximadamente 1GB de mem\u00f3ria. Sendo que a multiplica\u00e7\u00e3o entre a quantidade de mem\u00f3ria por cpu e o n\u00famero de cpus usadas n\u00e3o pode ultrapassar o limite total do n\u00f3, que varia de acordo com a sua parti\u00e7\u00e3o. Um n\u00f3 pertencente a parti\u00e7\u00e3o intel-128 s\u00f3 pode utilizar ao total 4GB por core, ou 2GB por cpu. Um n\u00f3 na parti\u00e7\u00e3o intel-256 e  gpu  pode usar at\u00e9 8 GB e intel-512 pode usar at\u00e9 16GB.</p> <p>A tabela a seguir lista os limites de mem\u00f3ria por parti\u00e7\u00e3o:</p> Parti\u00e7\u00e3o Padr\u00e3o M\u00e1ximo amd-512 2000 4000 intel-128 2000 4000 intel-256 4000 8000 intel-512 8000 16000 gpu-8-v100 8000 8000 gpu-4-a100 2000 2000 <p>Tamb\u00e9m h\u00e1 a op\u00e7\u00e3o <code>#SBATCH --mem</code> que j\u00e1 especifica a quantidade de mem\u00f3ria para o job por completo.</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512\n#SBATCH --mem=3000\n#SBATCH --cpus-per-task=3\n\n./prog1\n</code></pre>"},{"location":"intermediate/introduction_part_3/#carregando-softwares-disponiveis","title":"Carregando softwares dispon\u00edveis","text":"<p>A grande maioria dos softwares cient\u00edficos instalados no supercomputador s\u00e3o executados por m\u00f3dulo e, portanto, ser\u00e1 preciso carregar seus m\u00f3dulos. Para verificar quais m\u00f3dulos est\u00e3o dispon\u00edveis no supercomputador, digite no terminal do Linux o comando a seguir:</p> <pre><code>module avail\n</code></pre> <p>Depois de verificar os m\u00f3dulos dispon\u00edveis, voc\u00ea poder\u00e1 carregar aqueles dos quais voc\u00ea precisa utilizando um comando semelhante ao mostrado a seguir:</p> <pre><code>module load libraries/zlib/1.2.11-intel-16.0.1     # Substitua libraries/zlib/1.2.11-intel-16.0.1 pelo m\u00f3dulo desejado, inserindo seu caminho completo fornecido pelo comando module avail\n</code></pre> <p>Para verificar quais m\u00f3dulos voc\u00ea tem atualmente carregados, utilize o comando a seguir:</p> <pre><code>module list\n</code></pre> <p>Para limpar todos os m\u00f3dulos que voc\u00ea tem atualmente carregados, utilize o comando a seguir:</p> <pre><code>module clear\n</code></pre> <p>Os m\u00f3dulos podem ser adicionados ao script criado usando o editor de textos dispon\u00edvel de sua prefer\u00eancia, caso queira editar via linha comando, pode utilizar o editor Nano, basta voc\u00ea efetuar o devido carregamento dos m\u00f3dulos necess\u00e1rios, atrav\u00e9s dos comandos supracitados. Tome o cuidado apenas de, no script, carregar os m\u00f3dulos antes de fazer a execu\u00e7\u00e3o do programa propriamente dito.</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512\n#SBATCH --time=0-0:5\n\nmodule load nome-do-software #nome do software que aparecer\u00e1 ap\u00f3s usar o comando module avail\nnome-do-software prog1\n</code></pre>"},{"location":"intermediate/introduction_part_3/#utilizando-a-sua-pasta-home","title":"Utilizando a sua pasta home","text":"<p>Ao logar no supercomputador, voc\u00ea estar\u00e1 na pasta home. Essa pasta possui uma comunica\u00e7\u00e3o via rede com o supercomputador e j\u00e1 est\u00e1 configurada para, ao submeter os scripts para o supercomputador, ler os arquivos de entrada necess\u00e1rios para executar o job dessa pasta, assim como arquivo log de sa\u00edda que ser\u00e1 salvo nesse mesmo local.</p> <p>Ou seja, n\u00e3o \u00e9 necess\u00e1rio utilizar comandos extras ao utilizar essa pasta. Apenas as configura\u00e7\u00f5es b\u00e1sicas do script, como o tempo estimado de execu\u00e7\u00e3o do programa, quantidade de cpus, a fila em que deseja alocar o job e afins.</p> <p>Uma vez com o script pronto, \u00e9 s\u00f3 enviar o script com o comando <code>sbatch</code>.</p> <pre><code>$ sbatch meu-script.sh\n</code></pre>"},{"location":"intermediate/introduction_part_3/#utilizando-o-scratch-global","title":"Utilizando o scratch global","text":"<p>O NPAD utiliza o BeeGFS para scratch global, ou seja, armazenamento tempor\u00e1rio de arquivos, compartilhado com todos os n\u00f3s. Como usu\u00e1rio do sistema, tudo que voc\u00ea precisa fazer \u00e9 copiar os arquivos relevantes pra pasta ~/scratch. Voc\u00ea pode inclusive submeter jobs de l\u00e1. Essa pasta na verdade \u00e9 um symlink que aponta pra /scratch/global/usuario. Num job script, voc\u00ea tamb\u00e9m pode usar a vari\u00e1vel de ambiente $SCRATCH_GLOBAL para pegar a localiza\u00e7\u00e3o dessa pasta. </p> <p>Acessando a pasta /scratch/global destinada ao seu usu\u00e1rio:</p> <pre><code>$ cd ~/scratch\n</code></pre> <p>ou</p> <pre><code>$ cd $SCRATCH_GLOBAL\n</code></pre> <p>Exemplo de utiliza\u00e7\u00e3o:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512\n#SBATCH --time=0-0:5\n\n#move os arquivos com os par\u00e2metros de entrada para a pasta /scratch/global\nmv entrada.in $SCRATCH_GLOBAL \ncd $SCRATCH_GLOBAL\n\n#execu\u00e7\u00e3o normal do seu programa\n./prog \n\n#move os arquivos de sa\u00edda para o diret\u00f3rio /home/usuario\nmv saida.out /home/usuario/ \n</code></pre> <p>O scratch global deve ser considerada como uma pasta tempor\u00e1ria, e os arquivos l\u00e1 podem ser exclu\u00eddos se a equipe do NPAD determinar que eles n\u00e3o est\u00e3o sendo usados h\u00e1 muito tempo. Portanto, n\u00e3o mantenha arquivos importantes nela. A boa pr\u00e1tica \u00e9 usar a scratch global para a gera\u00e7\u00e3o de arquivos tempor\u00e1rios e depois copiar os resultados importantes para fora dela.</p>"},{"location":"intermediate/introduction_part_3/#backfill-e-escolha-do-tempo-de-execucao","title":"Backfill e escolha do tempo de execu\u00e7\u00e3o","text":"<p>O Supercomputador por padr\u00e3o aloca os jobs de acordo com suas prioridades, ou seja, jobs com alta prioridade executam antes. Por\u00e9m a op\u00e7\u00e3o backfill \u00e9 utilizada para uma melhor utiliza\u00e7\u00e3o do sistema. Com isso \u00e9 poss\u00edvel que jobs com prioridades menores sejam alocados e executados antes de jobs com alta prioridade. Isso acontece apenas no caso em que o tempo de in\u00edcio esperado do jobs de alta prioridade for maior que o tempo limite de execu\u00e7\u00e3o do job de baixa prioridade.</p> <p>O tempo de in\u00edcio esperado depende da finaliza\u00e7\u00e3o dos jobs em execu\u00e7\u00e3o e o tempo limite de execu\u00e7\u00e3o depende da op\u00e7\u00e3o --time no script do job. Logo, uma escolha razo\u00e1vel do tempo estimado de execu\u00e7\u00e3o pode fazer com que o job comece a executar mais r\u00e1pido. Ex.:</p> <pre><code>#!/bin/bash\n#SBATCH --partition=amd-512\n#SBATCH --time=0-0:5 #Formato padr\u00e3o: dias-horas:minutos\n</code></pre> <p>Para executar programas em paralelo no supercomputador, leia os tutorias de OpenMP e MPI.</p>"},{"location":"intermediate/slurm_commands/","title":"Comandos do supercomputador","text":"<p>O Slurm \u00e9 o gerenciador de recursos usado no supercomputador. Ele que auxilia o usu\u00e1rio na utiliza\u00e7\u00e3o do supercomputador, disponibilizando ferramentas e comandos que ajudam na submiss\u00e3o, execu\u00e7\u00e3o e gerenciamento de jobs. Aqui ent\u00e3o vamos listar alguns desses comandos, muito \u00fateis para o usu\u00e1rio saber o que se passa com seu job rec\u00e9m-submetido.</p>"},{"location":"intermediate/slurm_commands/#comandos","title":"Comandos","text":"Comando Descri\u00e7\u00e3o sinfo Visualize as informa\u00e7\u00f5es das parti\u00e7\u00f5es e n\u00f3s do supercomputador. sprio Visualize as informa\u00e7\u00f5es dos fatores que comp\u00f5em a prioridade na fila de cada job. squeue Visualize as informa\u00e7\u00f5es gerais dos Job's que est\u00e3o na fila ou executando. sbatch Submeta um Job para o supercomputador. scancel Cancele um Job que est\u00e1 na fila ou em execu\u00e7\u00e3o."},{"location":"intermediate/slurm_commands/#sinfo","title":"sinfo","text":"<p>sinfo exibe as informa\u00e7\u00f5es das parti\u00e7\u00f5es e n\u00f3s do supercomputador.</p>"},{"location":"intermediate/slurm_commands/#exemplo-sinfo","title":"Exemplo: sinfo","text":"<pre><code>$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ncluster*     up 20-00:00:0      1  down* r1i2n14\ncluster*     up 20-00:00:0     18  alloc r1i0n[1-2,4-7,11-12,14-16],r1i1n[7,9,15],r1i2n[1-2,11],r1i3n1\ncluster*     up 20-00:00:0     13    mix r1i0n[0,3,9],r1i1n[4-6,10,14],r1i2n[3-4,10,13],r1i3n2\ncluster*     up 20-00:00:0     19   idle r1i0n[10,17],r1i1n[0-3,11-13,16],r1i2n[0,5-7,9,12,15-16],r1i3n0\nservice      up 20-00:00:0      3    mix service[1,3-4]\nservice      up 20-00:00:0      1  alloc service2\ntest         up      30:00      1  down* r1i2n14\ntest         up      30:00     19  alloc r1i0n[1-2,4-7,11-12,14-16],r1i1n[7,9,15],r1i2n[1-2,11],r1i3n1,service2\ntest         up      30:00     16    mix r1i0n[0,3,9],r1i1n[4-6,10,14],r1i2n[3-4,10,13],r1i3n2,service[1,3-4]\ntest         up      30:00     19   idle r1i0n[10,17],r1i1n[0-3,11-13,16],r1i2n[0,5-7,9,12,15-16],r1i3n0\nintel-512    up 20-00:00:0      6    mix r1i3n[11-16]\nintel-256    up 20-00:00:0      6    mix r1i3n[3-6,9-10]\nintel-256    up 20-00:00:0      1   idle r1i3n7\ngpu          up 2-00:00:00      2   idle gpunode[0-1]\n</code></pre> <p>A lista abaixo mostra o que representa cada campo de sa\u00edda do comando sinfo.</p> <ul> <li>PARTITION: Parti\u00e7\u00f5es do supercomputador</li> <li>cluster: Parti\u00e7\u00e3o padr\u00e3o indicada pelo asterisco, composta por 64 n\u00f3s     computacionais em l\u00e2mina.     \\     r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16] e r1i3n[0-7,     9-16]</li> <li>test: Parti\u00e7\u00e3o para testes r\u00e1pidos, composta por 72 (todos) n\u00f3s     computacionais. Jobs rodados na parti\u00e7\u00e3o teste tem o valor de prioridade     aumentado em 15000.     \\     r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16], r1i3n[0-7,     9-16], service[1-4, 8-11]</li> <li>service: Parti\u00e7\u00e3o composta por 4 n\u00f3s computacionais.     \\     service[1-4]</li> <li>knl: Parti\u00e7\u00e3o composta por 2 n\u00f3s computacionais com grande n\u00famero de     n\u00facleos.     \\     service[8,9]</li> <li>gpu: Parti\u00e7\u00e3o composta por 2 n\u00f3s computacionais com 8 GPUs cada.     \\     service[10,11]</li> <li>full: Parti\u00e7\u00e3o composta por todos os 72 n\u00f3s computacionais.     \\     r1i0n[0-7, 9-12, 14-17], r1i1n[0-7, 9-16], r1i2n[0-7, 9-16], r1i3n[0-7,     9-16], service[1-4, 8-11]</li> </ul> <p>Mais detalhes sobre o hardware podem ser vistos em   Hardware</p> <ul> <li> <p>AVAIL: Disponibilidade de cada parti\u00e7\u00e3o (Available)</p> </li> <li> <p>TIMELIMIT: Tempo limite de execu\u00e7\u00e3o do job na parti\u00e7\u00e3o correspondente.   Todas as parti\u00e7\u00f5es possuem o tempo limite de 20 dias para execu\u00e7\u00e3o do job.   Exceto a parti\u00e7\u00e3o teste, cujo o tempo limite \u00e9 de 30 minutos com ganho na   prioridade para execu\u00e7\u00e3o.\\   Formato do timelimit no comando sinfo: Dias - horas : minutos   : segundos, exemplo: 20-13:22:21 significa 20 dias, 13 horas, 22 minutos e   21 segundos</p> </li> <li> <p>NODES: N\u00famero de n\u00f3s de cada parti\u00e7\u00e3o</p> </li> <li> <p>STATE: Campo mais relevante dado como resposta do comando sinfo. Seu   resultado pode ter significados diferentes, dependendo da sa\u00edda fornecida,   conforme \u00e9 poss\u00edvel verificar abaixo.</p> </li> <li> <p>alloc: Indica que um conjunto de n\u00f3s est\u00e1 em uso</p> </li> <li> <p>idle: Indica que um conjunto de n\u00f3s est\u00e1 ocioso</p> </li> <li> <p>down/drain: Indica que um conjunto de n\u00f3s se encontra indispon\u00edvel</p> </li> <li> <p>maint/resv: Indica que um conjunto de n\u00f3s est\u00e1 reservados</p> </li> <li> <p>mix: Indica que um conjunto de n\u00f3s est\u00e1 sendo compartilhados por mais de     um job</p> </li> <li> <p>NODELIST: Representa as listas de n\u00f3s correspondentes a cada par   parti\u00e7\u00e3o/estado</p> </li> </ul>"},{"location":"intermediate/slurm_commands/#sprio","title":"sprio","text":"<p>Sprio \u00e9 usado para exibir componentes da prioridade da fila de jobs. Quanto maior for seu valor, maior ser\u00e1 a sua prioridade. S\u00e3o mostradas somente as informa\u00e7\u00f5es dos jobs que ainda n\u00e3o est\u00e3o em execu\u00e7\u00e3o. Caso seja de seu interesse entender como se d\u00e1 o c\u00e1lculo da prioridade da fila de jobs, clique aqui.</p>"},{"location":"intermediate/slurm_commands/#exemplo-sprio","title":"Exemplo: sprio","text":"<pre><code>$ sprio -l\n          JOBID PARTITION     USER   PRIORITY       SITE        AGE      ASSOC  FAIRSHARE    JOBSIZE  PARTITION        QOS        NICE\n         164854 intel-512 dnpinhei      25031          0       1000          0         31          0          0      24000           0\n         165887 intel-512 dnpinhei      25031          0       1000          0         31          0          0      24000           0\n         167884 cluster   dnpinhei      25031          0       1000          0         31          0          0      24000           0\n         168322 intel-512 bpesilva       1486          0       1000          0        487          0          0          0           0\n         168323 intel-512 bpesilva       1486          0       1000          0        487          0          0          0           0\n         168333 intel-512 bpesilva       1486          0       1000          0        487          0          0          0           0\n         168917 intel-512 bpesilva       1226          0        740          0        487          0          0          0           0\n         170024 cluster   jxdlneto       1028          0        510          0        518          0          0          0           0\n         170025 cluster   jxdlneto       1028          0        510          0        518          0          0          0           0\n         170026 cluster   jxdlneto       1027          0        510          0        518          0          0          0           0\n         170027 cluster   jxdlneto       1027          0        509          0        518          0          0          0           0\n         170028 cluster   jxdlneto       1027          0        509          0        518          0          0          0           0\n         170030 cluster   jxdlneto       1026          0        509          0        518          0          0          0           0\n         170031 cluster   jxdlneto       1024          0        506          0        518          0          0          0           0\n         170032 cluster   jxdlneto       1024          0        506          0        518          0          0          0           0\n         170033 cluster   jxdlneto       1023          0        506          0        518          0          0          0           0\n         170034 cluster    crcosta       1840          0        506          0       1334          0          0          0           0\n         170035 cluster    crcosta       1838          0        504          0       1334          0          0          0           0\n         170036 cluster    crcosta       1838          0        504          0       1334          0          0          0           0\n         170037 cluster   scavalca       2984          0        504          0       2480          0          0          0           0\n         170514 cluster   thsrodri       2423          0        446          0       1978          0          0          0           0\n         170515 cluster   thsrodri       2423          0        446          0       1978          0          0          0           0\n         170516 cluster   thsrodri       2423          0        446          0       1978          0          0          0           0\n         170517 cluster   thsrodri       2423          0        446          0       1978          0          0          0           0\n</code></pre> <p>A lista abaixo mostra o que representa cada campo de sa\u00edda do comando sprio.</p> <ul> <li> <p>JOBID: Identificador (ID) do job em espera</p> </li> <li> <p>USER: Usu\u00e1rio que submeteu o job</p> </li> <li> <p>PRIORITY: Representa a prioridade do job na fila. Quanto maior seu valor,   maior sua prioridade</p> </li> <li> <p>AGE: \u00c9 utilizado para o c\u00e1lculo da prioridade. Se o job acabou de ser   submetido, ent\u00e3o o AGE vale 0; caso o mesmo esteja em espera h\u00e1 24 horas,   ent\u00e3o o AGE tem valor 500; a partir de 48 horas, esse campo passa a   valer 1000. Quaisquer valores intermedi\u00e1rios representam fra\u00e7\u00f5es de tempo,   obedecendo o padr\u00e3o supracitado.</p> </li> <li> <p>FAIRSHARE \u00c9 utilizado para o c\u00e1lculo da prioridade, juntamente com o valor   do campo AGE. Representa uma esp\u00e9cie de peso, compensa\u00e7\u00e3o, no caso do usu\u00e1rio   consumir uma quantidade menor ou maior do que sua fatia esperada.</p> </li> <li> <p>JOBSIZE: Campo irrelevante: n\u00e3o \u00e9 utilizado no c\u00e1lculo da prioridade</p> </li> <li> <p>PARTITION: Somente ter\u00e1 um valor caso a fila utilizada seja a test</p> </li> <li> <p>QOS: Representa simplesmente um b\u00f4nus na prioridade, caso o usu\u00e1rio seja o   administrador</p> </li> <li> <p>NICE: Campo irrelevante: n\u00e3o \u00e9 utilizado no c\u00e1lculo da prioridade</p> </li> </ul>"},{"location":"intermediate/slurm_commands/#squeue","title":"squeue","text":"<p>Squeue exibe as informa\u00e7\u00f5es gerais dos Job's que est\u00e3o na fila ou executando. Este comando possu\u00ed varia\u00e7\u00f5es como:\\ squeue --start: Exibe, al\u00e9m da fila, o tempo esperado para execu\u00e7\u00e3o dos jobs.\\ watch squeue: Atualiza a exibi\u00e7\u00e3o da fila a cada 2 segundos.</p>"},{"location":"intermediate/slurm_commands/#exemplo-squeue","title":"Exemplo: squeue","text":"<pre><code>$ squeue\n\u2009            JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            167884   cluster   tupi10 dnpinhei PD       0:00     21 (Resources)\n            170642   cluster   MIN_06 rbdpasso PD       0:00      2 (Priority)\n            170639   cluster   MIN_03 rbdpasso PD       0:00      2 (Priority)\n            170640   cluster   MIN_04 rbdpasso PD       0:00      2 (Priority)\n            170641   cluster   MIN_05 rbdpasso PD       0:00      2 (Priority)\n            170643   cluster   MIN_07 rbdpasso PD       0:00      2 (Priority)\n            170037   cluster PascalAn scavalca PD       0:00      1 (Priority)\n            170518   cluster    cout1 thsrodri PD       0:00      1 (Priority)\n            170519   cluster    cout1 thsrodri PD       0:00      1 (Priority)\n            170517   cluster    cout1 thsrodri PD       0:00      1 (Priority)\n            170516   cluster    cout1 thsrodri PD       0:00      1 (Priority)\n            170515   cluster    cout1 thsrodri PD       0:00      1 (Priority)\n            170514   cluster    cout1 thsrodri PD       0:00      1 (Priority)\n            170034   cluster   SLP_20  crcosta PD       0:00      1 (Priority)\n            170035   cluster   SLP_21  crcosta PD       0:00      1 (Priority)\n            170036   cluster   SLP_22  crcosta PD       0:00      1 (Priority)\n            170659   cluster   SLP_31  crcosta PD       0:00      1 (Priority)\n            170660   cluster   SLP_32  crcosta PD       0:00      1 (Priority)\n            170661   cluster   SLP_33  crcosta PD       0:00      1 (Priority)\n            170662   cluster   SLP_41  crcosta PD       0:00      1 (Priority)\n            170663   cluster   SLP_42  crcosta PD       0:00      1 (Priority)\n            170664   cluster   SLP_43  crcosta PD       0:00      1 (Priority)\n            170025   cluster dinam_hw jxdlneto PD       0:00      1 (Priority)\n            170026   cluster dinam_iw jxdlneto PD       0:00      1 (Priority)\n            170024   cluster dinam_hw jxdlneto PD       0:00      1 (Priority)\n            170027   cluster dinam_iw jxdlneto PD       0:00      1 (Priority)\n            170028   cluster dinam_iw jxdlneto PD       0:00      1 (Priority)\n            170030   cluster PBEqe_ge jxdlneto PD       0:00      1 (Priority)\n            170032   cluster PBEd35qe jxdlneto PD       0:00      1 (Priority)\n            170033   cluster PBEd36qe jxdlneto PD       0:00      1 (Priority)\n            170031   cluster PBEd2qe_ jxdlneto PD       0:00      1 (Priority)\n            170657   cluster complex_ ldflacer PD       0:00      1 (Priority)\n            170658   cluster   Citopt bpesilva PD       0:00      1 (Priority)\n            168024   cluster fwi-deli cdssanta  R 2-02:21:08      4 r1i0n[14-16],r1i1n9\n</code></pre> <p>A lista abaixo mostra o que representa cada campo de sa\u00edda do comando squeue.</p> <ul> <li> <p>JOBID: Identificador (ID) do job</p> </li> <li> <p>PARTITION: Fila do supercomputador que est\u00e1 sendo utilizada</p> </li> <li> <p>NAME: Nome do job.</p> </li> <li> <p>USER: Usu\u00e1rio que est\u00e1 executando aquele job</p> </li> <li> <p>ST: O estado em que se encontra o job. Abaixo seguem os dois estados   poss\u00edveis em que um job pode se encontrar.</p> </li> <li>PD (Pending): Significa que o job est\u00e1 pendente, devido a uma ou     mais raz\u00f5es. Neste caso, mais detalhes s\u00e3o mostrados no campo     NODELIST(REASON)</li> <li> <p>R (Running): Significa que o job est\u00e1 em execu\u00e7\u00e3o normal</p> </li> <li> <p>TIME: O tempo total em que o job est\u00e1 em execu\u00e7\u00e3o</p> </li> <li> <p>NODES: O n\u00famero de n\u00f3s em que o job est\u00e1 sendo executado</p> </li> <li> <p>NODELIST(REASON): Se o job estiver em execu\u00e7\u00e3o, esse campo lista os nomes   dos n\u00f3s em que o job est\u00e1 sendo executado. Se o job estiver pendente, este   campo mostra o motivo pelo qual o trabalho est\u00e1 pendente. Neste caso, os   motivos podem ser:</p> </li> <li> <p>Resources: Significa que os recursos de computa\u00e7\u00e3o necess\u00e1rios para     aquele job n\u00e3o est\u00e3o dispon\u00edveis no momento</p> </li> <li> <p>Priority: Indica que o job est\u00e1 aguardando sua vez na fila,seguindo a     fila de prioridades</p> </li> <li> <p>Dependency: Indica que o job est\u00e1 aguardando a conclus\u00e3o de outro     trabalho antes de ser executado. Depend\u00eancias s\u00e3o solicitadas quando um     trabalho \u00e9 submetido</p> </li> <li> <p>PartitionTimeLimit: Significa que o job solicitou mais tempo de execu\u00e7\u00e3o     do que a fila permite</p> </li> <li> <p>AssocGrpCpuLimit: Indica que o grupo do usu\u00e1rio est\u00e1 executando pr\u00f3ximo     ao seu n\u00famero m\u00e1ximo de n\u00facleos de CPU permitidos</p> </li> <li> <p>AssocGrpCPURunMinsLimit: Indica que executar aquele job colocaria o     grupo do usu\u00e1rio al\u00e9m do n\u00famero m\u00e1ximo de minutos de CPU alocados para os     jobs atualmente em execu\u00e7\u00e3o.</p> </li> <li> <p>AssocGrpMemLimit: O grupo do usu\u00e1rio alocou sua quantidade m\u00e1xima de RAM</p> </li> <li> <p>JobArrayTaskLimit: Indica que aquele job est\u00e1 envolvido em um vetor     (grupo) de tarefas limitado a executar em um n\u00famero definido de n\u00facleos de     CPU de uma vez</p> </li> </ul>"},{"location":"intermediate/slurm_commands/#sbatch","title":"sbatch","text":"<p>Sbatch envia um script para o supercomputador, esse script \u00e9 passado atrav\u00e9s do nome do arquivo que for especificado, o sbach ir\u00e1 ler o script a partir da entrada padr\u00e3o. (Voc\u00ea pode encontrar como criar um script na Parte 2 do tutorial referente \u00e0 introdu\u00e7\u00e3o ao supercomputador).</p>"},{"location":"intermediate/slurm_commands/#exemplo","title":"Exemplo","text":"<pre><code>$ squeue -u scavalcanti\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\nsbatch run_pascalanalyzer.sh\nSubmitted batch job 170926\n\n$ squeue -u scavalcanti\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            170926   cluster PascalAn scavalca PD       0:00      1 (Priority)\n</code></pre> <p>no exemplo acima, foi utilizado o comando squeue como o par\u00e2metro -u para mostrar as informa\u00e7\u00f5es gerais dos jobs do usu\u00e1rio scavalcanti, tanto os que est\u00e3o executando. quanto os que est\u00e3o na fila. Sendo que n\u00e3o havia nenhum job a ser mostrado. Posteriormente, o comando sbatch foi usado para submeter um job. Novamente, o comando squeue foi usado, mostrando que o job foi devidamente submetido e aguarda na fila para ser executado.</p>"},{"location":"intermediate/slurm_commands/#scancel","title":"scancel","text":"<p>Scancel \u00e9 utilizado para cancelar um job depois que ele foi submetido, podendo ele estar na fila de espera ou em execu\u00e7\u00e3o. O job ser\u00e1 interrompido de imediato, ent\u00e3o seja sempre cuidadoso ao usar esse comando, lembrando sempre que ao executar outro job voc\u00ea estar\u00e1 no final da fila. Entretando, voc\u00ea n\u00e3o precisa se preocupar que nenhum usu\u00e1rio ter\u00e1 acesso ao job de outro, sendo assim, n\u00e3o h\u00e1 riscos de algu\u00e9m cancelar job's que n\u00e3o s\u00e3o seus.</p>"},{"location":"intermediate/slurm_commands/#exemplo-scancel","title":"Exemplo: scancel","text":"<pre><code>$ squeue -u scavalcanti\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            170926   cluster PascalAn scavalca PD       0:00      1 (Priority)\n\n$ scancel 170926\n\n$ squeue -u scavalcanti\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n</code></pre> <p>No exemplo acima, foi utilizado o comando squeue como o par\u00e2metro -u para mostrar as informa\u00e7\u00f5es gerais dos jobs do usu\u00e1rio scavalcanti, tanto os que est\u00e3o executando. quanto os que est\u00e3o na fila. Sendo mostrado um \u00fanico job em espera. Posteriormente, o comando scancel foi usado para cancelar o \u00fanico job existente. Novamente, o comando squeue foi usado, mostrando que o job foi devidamente deletado.</p>"}]}